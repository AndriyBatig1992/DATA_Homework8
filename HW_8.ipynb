{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Yf4NBJUSNM"
      },
      "source": [
        "# Створення нейронної мережі\n",
        "\n",
        "У цьому завданні ми створимо повнозв'язну нейронну мережу, використовуючи при цьому низькорівневі механізми tensorflow.\n",
        "\n",
        "Архітектура нейромережі представлена на наступному малюнку. Як бачиш, у ній є один вхідний шар, два приховані, а також вихідний шар. В якості активаційної функції у прихованих шарах буде використовуватись сигмоїда. На вихідному шарі ми використовуємо softmax.\n",
        "\n",
        "Частина коду зі створення мережі вже написана, тобі потрібно заповнити пропуски у вказаних місцях."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01rZWUu0USNQ"
      },
      "source": [
        "## Архітектура нейронної мережі\n",
        "\n",
        "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLvIZ705Qw_V"
      },
      "source": [
        "## Про датасет MNIST\n",
        "\n",
        "Дану нейромережу ми будемо вивчати на датасеті MNIST. Цей датасет являє собою велику кількість зображень рукописних цифр розміром $28 \\times 28$ пікселів. Кожен піксель приймає значення від 0 до 255.\n",
        "\n",
        "Як і раніше, датасет буде розділений на навчальну та тестову вибірки. При цьому ми виконаємо нормалізацію всіх зображень, щоб значення пікселів знаходилось у проміжку від 0 до 1, розділивши яскравість кожного пікселя на 255.\n",
        "\n",
        "Окрім того, архітектура нейронної мережі очікує на вхід вектор. У нашому ж випадку кожен об'єкт вибірки являє собою матрицю. Що ж робити? У цьому завданні ми \"розтягнемо\" матрицю $28 \\times 28$, отримавши при цьому вектор, що складається з 784 елементів.\n",
        "\n",
        "![MNIST Dataset](https://www.researchgate.net/profile/Steven-Young-5/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png)\n",
        "\n",
        "Більше інформації про датасет можна знайти [тут](http://yann.lecun.com/exdb/mnist/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "il_0_5OyUSNR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import keras as K\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cd-1_abTUSNS"
      },
      "outputs": [],
      "source": [
        "num_classes = 10 # загальна кількість класів, у нашому випадку це цифри від 0 до 9\n",
        "num_features = 784 # кількість атрибутів вхідного вектора 28 * 28 = 784\n",
        "\n",
        "learning_rate = 0.001 # швидкість навчання нейронної мережі\n",
        "training_steps = 3000 # максимальне число епох\n",
        "batch_size = 256 # перераховувати ваги мережі ми будемо не на всій вибірці, а на її випадковій підмножині з batch_size елементів\n",
        "display_step = 100 # кожні 100 ітерацій ми будемо показувати поточне значення функції втрат і точності\n",
        "\n",
        "n_hidden_1 = 128 # кількість нейронів 1-го шару\n",
        "n_hidden_2 = 256 # кількість нейронів 2-го шару"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGTXiRyTUSNT",
        "outputId": "6a4d8ac8-b900-4b81-a8a5-ac70475e6b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# from tensorflow.keras.datasets import mnist\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Завантажуємо датасет\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Перетворюємо цілочисельні пікселі на тип float32\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "\n",
        "# Перетворюємо матриці розміром 28x28 пікселів у вектор з 784 елементів\n",
        "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
        "\n",
        "# Нормалізуємо значення пікселів\n",
        "x_train, x_test = x_train / 255., x_test / 255.\n",
        "\n",
        "# Перемішаємо тренувальні дані\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FkRmCQjnUSNV"
      },
      "outputs": [],
      "source": [
        "# Створимо нейронну мережу\n",
        "\n",
        "class DenseLayer(tf.Module):\n",
        "    def __init__(self, in_features, out_features, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.w = tf.Variable(\n",
        "            tf.random.normal([in_features, out_features]), name=\"w\"\n",
        "        )\n",
        "        self.b = tf.Variable(tf.zeros([out_features]), name=\"b\")\n",
        "\n",
        "    def __call__(self, x, activation=0):\n",
        "        y = tf.matmul(x, self.w) + self.b\n",
        "        if activation != 0:\n",
        "            return tf.nn.softmax(y)\n",
        "        else:\n",
        "            return tf.nn.sigmoid(y)\n",
        "\n",
        "class NN(tf.Module):\n",
        "  def __init__(self, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.hidden_layer_1 = DenseLayer(num_features, n_hidden_1, name='hidden_layer_1')\n",
        "    self.hidden_layer_2 = DenseLayer(n_hidden_1, n_hidden_2, name='hidden_layer_2')\n",
        "    self.output_layer = DenseLayer(n_hidden_2, num_classes, name='output_layer')\n",
        "\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "     x = self.hidden_layer_1(x)\n",
        "     x = self.hidden_layer_2(x)\n",
        "     x=tf.nn.softmax(self.output_layer(x))\n",
        "     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LIf3o7VAUSNV"
      },
      "outputs": [],
      "source": [
        "# В якості функції помилки в даному випадку зручно взяти крос-ентропію\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    # Закодувати label в one hot vector\n",
        "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
        "\n",
        "    # Значення передбачення, щоб уникнути помилки log(0).\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "\n",
        "    # Обчислення крос-ентропії\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
        "\n",
        "# Як метрику якості використовуємо точність\n",
        "def accuracy(y_pred, y_true):\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MQeT1yatUSNW"
      },
      "outputs": [],
      "source": [
        "# Створимо екземпляр нейронної мережі\n",
        "neural_net = NN(name=\"mnist\")\n",
        "\n",
        "# Функція навчання нейромережі\n",
        "def train(neural_net, input_x, output_y):\n",
        "  # Для налаштування вагів мережі будемо використовувати стохастичний градієнтний спуск\n",
        "  optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "  # Активація автоматичного диференціювання\n",
        "  with tf.GradientTape() as g:\n",
        "    pred = neural_net(input_x)\n",
        "    loss = cross_entropy(pred, output_y)\n",
        "    trainable_variables = neural_net.trainable_variables\n",
        "    gradients= g.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fnyns9lBfpQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31dd679d-878f-4c7e-bbcd-e50a8a5340c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mПоказано результат, скорочений до останніх рядків (5000).\u001b[0m\n",
            "Step: 202100, Loss: 401.7809143066406, Accuracy: 0.87109375\n",
            "Step: 202200, Loss: 404.4111328125, Accuracy: 0.85546875\n",
            "Step: 202300, Loss: 396.8227233886719, Accuracy: 0.8984375\n",
            "Step: 202400, Loss: 403.1632080078125, Accuracy: 0.87890625\n",
            "Step: 202500, Loss: 403.84454345703125, Accuracy: 0.86328125\n",
            "Step: 202600, Loss: 402.0785217285156, Accuracy: 0.875\n",
            "Step: 202700, Loss: 398.26336669921875, Accuracy: 0.890625\n",
            "Step: 202800, Loss: 399.8960876464844, Accuracy: 0.88671875\n",
            "Step: 202900, Loss: 402.3232727050781, Accuracy: 0.859375\n",
            "Step: 203000, Loss: 395.77459716796875, Accuracy: 0.89453125\n",
            "Step: 203100, Loss: 404.49188232421875, Accuracy: 0.85546875\n",
            "Step: 203200, Loss: 403.6042785644531, Accuracy: 0.86328125\n",
            "Step: 203300, Loss: 403.8244323730469, Accuracy: 0.86328125\n",
            "Step: 203400, Loss: 401.0628662109375, Accuracy: 0.87890625\n",
            "Step: 203500, Loss: 401.87994384765625, Accuracy: 0.87109375\n",
            "Step: 203600, Loss: 399.46893310546875, Accuracy: 0.87890625\n",
            "Step: 203700, Loss: 403.0860290527344, Accuracy: 0.859375\n",
            "Step: 203800, Loss: 395.7101745605469, Accuracy: 0.90625\n",
            "Step: 203900, Loss: 403.18377685546875, Accuracy: 0.8671875\n",
            "Step: 204000, Loss: 408.9083557128906, Accuracy: 0.84375\n",
            "Step: 204100, Loss: 393.92144775390625, Accuracy: 0.91796875\n",
            "Step: 204200, Loss: 403.9106750488281, Accuracy: 0.8671875\n",
            "Step: 204300, Loss: 401.51953125, Accuracy: 0.87109375\n",
            "Step: 204400, Loss: 399.0208740234375, Accuracy: 0.88671875\n",
            "Step: 204500, Loss: 408.7137451171875, Accuracy: 0.828125\n",
            "Step: 204600, Loss: 402.3719482421875, Accuracy: 0.87109375\n",
            "Step: 204700, Loss: 402.9129333496094, Accuracy: 0.859375\n",
            "Step: 204800, Loss: 405.91094970703125, Accuracy: 0.83984375\n",
            "Step: 204900, Loss: 399.3885192871094, Accuracy: 0.8828125\n",
            "Step: 205000, Loss: 402.5585632324219, Accuracy: 0.875\n",
            "Step: 205100, Loss: 399.4078369140625, Accuracy: 0.875\n",
            "Step: 205200, Loss: 395.86444091796875, Accuracy: 0.90234375\n",
            "Step: 205300, Loss: 399.34136962890625, Accuracy: 0.890625\n",
            "Step: 205400, Loss: 399.1936340332031, Accuracy: 0.88671875\n",
            "Step: 205500, Loss: 398.1241455078125, Accuracy: 0.890625\n",
            "Step: 205600, Loss: 402.5350341796875, Accuracy: 0.87109375\n",
            "Step: 205700, Loss: 408.03125, Accuracy: 0.83984375\n",
            "Step: 205800, Loss: 400.7388000488281, Accuracy: 0.875\n",
            "Step: 205900, Loss: 404.780517578125, Accuracy: 0.85546875\n",
            "Step: 206000, Loss: 402.4530944824219, Accuracy: 0.87109375\n",
            "Step: 206100, Loss: 401.2803039550781, Accuracy: 0.87890625\n",
            "Step: 206200, Loss: 410.39697265625, Accuracy: 0.828125\n",
            "Step: 206300, Loss: 398.6328430175781, Accuracy: 0.88671875\n",
            "Step: 206400, Loss: 400.5814208984375, Accuracy: 0.87109375\n",
            "Step: 206500, Loss: 394.61083984375, Accuracy: 0.8984375\n",
            "Step: 206600, Loss: 396.2643737792969, Accuracy: 0.8984375\n",
            "Step: 206700, Loss: 397.1837158203125, Accuracy: 0.89453125\n",
            "Step: 206800, Loss: 401.31231689453125, Accuracy: 0.8828125\n",
            "Step: 206900, Loss: 403.52490234375, Accuracy: 0.86328125\n",
            "Step: 207000, Loss: 403.90911865234375, Accuracy: 0.8671875\n",
            "Step: 207100, Loss: 400.384033203125, Accuracy: 0.87890625\n",
            "Step: 207200, Loss: 400.5465087890625, Accuracy: 0.875\n",
            "Step: 207300, Loss: 402.3768005371094, Accuracy: 0.8671875\n",
            "Step: 207400, Loss: 403.48651123046875, Accuracy: 0.86328125\n",
            "Step: 207500, Loss: 398.9718322753906, Accuracy: 0.88671875\n",
            "Step: 207600, Loss: 405.76910400390625, Accuracy: 0.859375\n",
            "Step: 207700, Loss: 399.89202880859375, Accuracy: 0.88671875\n",
            "Step: 207800, Loss: 400.39453125, Accuracy: 0.875\n",
            "Step: 207900, Loss: 398.96240234375, Accuracy: 0.890625\n",
            "Step: 208000, Loss: 402.7803955078125, Accuracy: 0.86328125\n",
            "Step: 208100, Loss: 394.6749572753906, Accuracy: 0.9140625\n",
            "Step: 208200, Loss: 399.2835693359375, Accuracy: 0.88671875\n",
            "Step: 208300, Loss: 407.73333740234375, Accuracy: 0.83984375\n",
            "Step: 208400, Loss: 405.39630126953125, Accuracy: 0.85546875\n",
            "Step: 208500, Loss: 408.06231689453125, Accuracy: 0.8359375\n",
            "Step: 208600, Loss: 397.5351867675781, Accuracy: 0.89453125\n",
            "Step: 208700, Loss: 402.50213623046875, Accuracy: 0.8671875\n",
            "Step: 208800, Loss: 405.71429443359375, Accuracy: 0.859375\n",
            "Step: 208900, Loss: 406.9304504394531, Accuracy: 0.84765625\n",
            "Step: 209000, Loss: 410.3365173339844, Accuracy: 0.82421875\n",
            "Step: 209100, Loss: 402.3868408203125, Accuracy: 0.87890625\n",
            "Step: 209200, Loss: 404.38458251953125, Accuracy: 0.859375\n",
            "Step: 209300, Loss: 400.58746337890625, Accuracy: 0.87890625\n",
            "Step: 209400, Loss: 399.88226318359375, Accuracy: 0.875\n",
            "Step: 209500, Loss: 396.1431579589844, Accuracy: 0.88671875\n",
            "Step: 209600, Loss: 399.76885986328125, Accuracy: 0.87109375\n",
            "Step: 209700, Loss: 403.73895263671875, Accuracy: 0.8671875\n",
            "Step: 209800, Loss: 396.3868103027344, Accuracy: 0.88671875\n",
            "Step: 209900, Loss: 407.75445556640625, Accuracy: 0.83984375\n",
            "Step: 210000, Loss: 393.6999816894531, Accuracy: 0.8984375\n",
            "Step: 210100, Loss: 398.9271240234375, Accuracy: 0.87109375\n",
            "Step: 210200, Loss: 398.97052001953125, Accuracy: 0.890625\n",
            "Step: 210300, Loss: 399.1667785644531, Accuracy: 0.88671875\n",
            "Step: 210400, Loss: 391.89654541015625, Accuracy: 0.91796875\n",
            "Step: 210500, Loss: 397.30377197265625, Accuracy: 0.89453125\n",
            "Step: 210600, Loss: 404.0131530761719, Accuracy: 0.859375\n",
            "Step: 210700, Loss: 391.28271484375, Accuracy: 0.91796875\n",
            "Step: 210800, Loss: 395.1021728515625, Accuracy: 0.91015625\n",
            "Step: 210900, Loss: 395.9667663574219, Accuracy: 0.89453125\n",
            "Step: 211000, Loss: 403.4365539550781, Accuracy: 0.8671875\n",
            "Step: 211100, Loss: 406.590576171875, Accuracy: 0.8515625\n",
            "Step: 211200, Loss: 396.08251953125, Accuracy: 0.90234375\n",
            "Step: 211300, Loss: 396.3056945800781, Accuracy: 0.88671875\n",
            "Step: 211400, Loss: 396.26690673828125, Accuracy: 0.89453125\n",
            "Step: 211500, Loss: 401.08306884765625, Accuracy: 0.87109375\n",
            "Step: 211600, Loss: 402.1975402832031, Accuracy: 0.87109375\n",
            "Step: 211700, Loss: 408.23602294921875, Accuracy: 0.84375\n",
            "Step: 211800, Loss: 398.3372802734375, Accuracy: 0.890625\n",
            "Step: 211900, Loss: 401.1932678222656, Accuracy: 0.8671875\n",
            "Step: 212000, Loss: 403.4819641113281, Accuracy: 0.86328125\n",
            "Step: 212100, Loss: 403.5060119628906, Accuracy: 0.8671875\n",
            "Step: 212200, Loss: 399.3201904296875, Accuracy: 0.890625\n",
            "Step: 212300, Loss: 396.57720947265625, Accuracy: 0.89453125\n",
            "Step: 212400, Loss: 402.9593200683594, Accuracy: 0.87109375\n",
            "Step: 212500, Loss: 401.1871643066406, Accuracy: 0.87109375\n",
            "Step: 212600, Loss: 402.04071044921875, Accuracy: 0.87109375\n",
            "Step: 212700, Loss: 405.4216613769531, Accuracy: 0.86328125\n",
            "Step: 212800, Loss: 396.6481018066406, Accuracy: 0.890625\n",
            "Step: 212900, Loss: 397.972900390625, Accuracy: 0.88671875\n",
            "Step: 213000, Loss: 407.3548889160156, Accuracy: 0.83984375\n",
            "Step: 213100, Loss: 398.889404296875, Accuracy: 0.88671875\n",
            "Step: 213200, Loss: 407.7292785644531, Accuracy: 0.83203125\n",
            "Step: 213300, Loss: 403.1755065917969, Accuracy: 0.86328125\n",
            "Step: 213400, Loss: 404.79376220703125, Accuracy: 0.8515625\n",
            "Step: 213500, Loss: 394.57989501953125, Accuracy: 0.91015625\n",
            "Step: 213600, Loss: 397.2994384765625, Accuracy: 0.87109375\n",
            "Step: 213700, Loss: 401.1144714355469, Accuracy: 0.87109375\n",
            "Step: 213800, Loss: 399.059814453125, Accuracy: 0.87890625\n",
            "Step: 213900, Loss: 404.1683349609375, Accuracy: 0.85546875\n",
            "Step: 214000, Loss: 405.13067626953125, Accuracy: 0.8515625\n",
            "Step: 214100, Loss: 402.8328857421875, Accuracy: 0.875\n",
            "Step: 214200, Loss: 403.52679443359375, Accuracy: 0.86328125\n",
            "Step: 214300, Loss: 394.5986633300781, Accuracy: 0.91796875\n",
            "Step: 214400, Loss: 401.214111328125, Accuracy: 0.87109375\n",
            "Step: 214500, Loss: 401.6702880859375, Accuracy: 0.87109375\n",
            "Step: 214600, Loss: 396.38336181640625, Accuracy: 0.8984375\n",
            "Step: 214700, Loss: 396.53839111328125, Accuracy: 0.89453125\n",
            "Step: 214800, Loss: 396.4004821777344, Accuracy: 0.90234375\n",
            "Step: 214900, Loss: 394.10894775390625, Accuracy: 0.90234375\n",
            "Step: 215000, Loss: 404.51995849609375, Accuracy: 0.85546875\n",
            "Step: 215100, Loss: 400.270263671875, Accuracy: 0.875\n",
            "Step: 215200, Loss: 400.78509521484375, Accuracy: 0.8828125\n",
            "Step: 215300, Loss: 406.442626953125, Accuracy: 0.8515625\n",
            "Step: 215400, Loss: 396.5440673828125, Accuracy: 0.88671875\n",
            "Step: 215500, Loss: 397.72418212890625, Accuracy: 0.890625\n",
            "Step: 215600, Loss: 405.24957275390625, Accuracy: 0.8515625\n",
            "Step: 215700, Loss: 401.8105163574219, Accuracy: 0.8671875\n",
            "Step: 215800, Loss: 413.6739807128906, Accuracy: 0.82421875\n",
            "Step: 215900, Loss: 396.31982421875, Accuracy: 0.89453125\n",
            "Step: 216000, Loss: 398.39141845703125, Accuracy: 0.87109375\n",
            "Step: 216100, Loss: 390.5050048828125, Accuracy: 0.91796875\n",
            "Step: 216200, Loss: 394.66693115234375, Accuracy: 0.90625\n",
            "Step: 216300, Loss: 399.21063232421875, Accuracy: 0.88671875\n",
            "Step: 216400, Loss: 400.37396240234375, Accuracy: 0.87890625\n",
            "Step: 216500, Loss: 400.6363220214844, Accuracy: 0.87890625\n",
            "Step: 216600, Loss: 398.5154724121094, Accuracy: 0.8828125\n",
            "Step: 216700, Loss: 397.4000244140625, Accuracy: 0.890625\n",
            "Step: 216800, Loss: 406.4437255859375, Accuracy: 0.85546875\n",
            "Step: 216900, Loss: 405.798095703125, Accuracy: 0.8515625\n",
            "Step: 217000, Loss: 402.91802978515625, Accuracy: 0.87109375\n",
            "Step: 217100, Loss: 404.03607177734375, Accuracy: 0.8671875\n",
            "Step: 217200, Loss: 410.01336669921875, Accuracy: 0.82421875\n",
            "Step: 217300, Loss: 405.8298034667969, Accuracy: 0.8515625\n",
            "Step: 217400, Loss: 398.146240234375, Accuracy: 0.88671875\n",
            "Step: 217500, Loss: 407.27349853515625, Accuracy: 0.84375\n",
            "Step: 217600, Loss: 402.0833740234375, Accuracy: 0.8671875\n",
            "Step: 217700, Loss: 396.69281005859375, Accuracy: 0.8984375\n",
            "Step: 217800, Loss: 398.795166015625, Accuracy: 0.87890625\n",
            "Step: 217900, Loss: 395.9246826171875, Accuracy: 0.90234375\n",
            "Step: 218000, Loss: 399.63720703125, Accuracy: 0.87890625\n",
            "Step: 218100, Loss: 397.5196533203125, Accuracy: 0.88671875\n",
            "Step: 218200, Loss: 402.1712646484375, Accuracy: 0.87109375\n",
            "Step: 218300, Loss: 402.37335205078125, Accuracy: 0.86328125\n",
            "Step: 218400, Loss: 399.95147705078125, Accuracy: 0.875\n",
            "Step: 218500, Loss: 397.638671875, Accuracy: 0.89453125\n",
            "Step: 218600, Loss: 402.94708251953125, Accuracy: 0.859375\n",
            "Step: 218700, Loss: 400.7106628417969, Accuracy: 0.87890625\n",
            "Step: 218800, Loss: 394.0037841796875, Accuracy: 0.90625\n",
            "Step: 218900, Loss: 399.60614013671875, Accuracy: 0.8828125\n",
            "Step: 219000, Loss: 401.05462646484375, Accuracy: 0.875\n",
            "Step: 219100, Loss: 402.69842529296875, Accuracy: 0.85546875\n",
            "Step: 219200, Loss: 402.2768249511719, Accuracy: 0.859375\n",
            "Step: 219300, Loss: 401.25201416015625, Accuracy: 0.87890625\n",
            "Step: 219400, Loss: 403.65582275390625, Accuracy: 0.85546875\n",
            "Step: 219500, Loss: 395.800537109375, Accuracy: 0.90234375\n",
            "Step: 219600, Loss: 404.835205078125, Accuracy: 0.85546875\n",
            "Step: 219700, Loss: 403.96722412109375, Accuracy: 0.8515625\n",
            "Step: 219800, Loss: 394.94219970703125, Accuracy: 0.90234375\n",
            "Step: 219900, Loss: 399.1217041015625, Accuracy: 0.87109375\n",
            "Step: 220000, Loss: 405.27178955078125, Accuracy: 0.859375\n",
            "Step: 220100, Loss: 404.509033203125, Accuracy: 0.859375\n",
            "Step: 220200, Loss: 402.7852783203125, Accuracy: 0.8671875\n",
            "Step: 220300, Loss: 401.07733154296875, Accuracy: 0.87890625\n",
            "Step: 220400, Loss: 399.5035400390625, Accuracy: 0.87890625\n",
            "Step: 220500, Loss: 405.5069580078125, Accuracy: 0.83984375\n",
            "Step: 220600, Loss: 392.6678161621094, Accuracy: 0.91796875\n",
            "Step: 220700, Loss: 395.044677734375, Accuracy: 0.90234375\n",
            "Step: 220800, Loss: 407.628662109375, Accuracy: 0.84375\n",
            "Step: 220900, Loss: 402.60955810546875, Accuracy: 0.859375\n",
            "Step: 221000, Loss: 405.111083984375, Accuracy: 0.8515625\n",
            "Step: 221100, Loss: 400.77935791015625, Accuracy: 0.86328125\n",
            "Step: 221200, Loss: 403.89654541015625, Accuracy: 0.859375\n",
            "Step: 221300, Loss: 402.6342468261719, Accuracy: 0.8671875\n",
            "Step: 221400, Loss: 400.39849853515625, Accuracy: 0.87890625\n",
            "Step: 221500, Loss: 407.645751953125, Accuracy: 0.84765625\n",
            "Step: 221600, Loss: 406.37921142578125, Accuracy: 0.84765625\n",
            "Step: 221700, Loss: 398.74517822265625, Accuracy: 0.88671875\n",
            "Step: 221800, Loss: 402.93634033203125, Accuracy: 0.8671875\n",
            "Step: 221900, Loss: 405.5995178222656, Accuracy: 0.85546875\n",
            "Step: 222000, Loss: 396.8594055175781, Accuracy: 0.8984375\n",
            "Step: 222100, Loss: 399.00640869140625, Accuracy: 0.88671875\n",
            "Step: 222200, Loss: 398.6221008300781, Accuracy: 0.87890625\n",
            "Step: 222300, Loss: 402.493896484375, Accuracy: 0.875\n",
            "Step: 222400, Loss: 397.93499755859375, Accuracy: 0.88671875\n",
            "Step: 222500, Loss: 396.53582763671875, Accuracy: 0.90234375\n",
            "Step: 222600, Loss: 402.16680908203125, Accuracy: 0.875\n",
            "Step: 222700, Loss: 403.2914733886719, Accuracy: 0.85546875\n",
            "Step: 222800, Loss: 399.1304931640625, Accuracy: 0.88671875\n",
            "Step: 222900, Loss: 407.1526794433594, Accuracy: 0.84765625\n",
            "Step: 223000, Loss: 401.7292175292969, Accuracy: 0.8671875\n",
            "Step: 223100, Loss: 397.4151611328125, Accuracy: 0.890625\n",
            "Step: 223200, Loss: 394.8892517089844, Accuracy: 0.8984375\n",
            "Step: 223300, Loss: 399.073974609375, Accuracy: 0.88671875\n",
            "Step: 223400, Loss: 404.18609619140625, Accuracy: 0.86328125\n",
            "Step: 223500, Loss: 398.4737854003906, Accuracy: 0.890625\n",
            "Step: 223600, Loss: 397.0633544921875, Accuracy: 0.88671875\n",
            "Step: 223700, Loss: 394.42083740234375, Accuracy: 0.90625\n",
            "Step: 223800, Loss: 400.7791748046875, Accuracy: 0.87890625\n",
            "Step: 223900, Loss: 406.69781494140625, Accuracy: 0.84375\n",
            "Step: 224000, Loss: 407.7855224609375, Accuracy: 0.84765625\n",
            "Step: 224100, Loss: 398.89349365234375, Accuracy: 0.8828125\n",
            "Step: 224200, Loss: 403.15545654296875, Accuracy: 0.87109375\n",
            "Step: 224300, Loss: 390.5953063964844, Accuracy: 0.921875\n",
            "Step: 224400, Loss: 404.403564453125, Accuracy: 0.8671875\n",
            "Step: 224500, Loss: 408.1447448730469, Accuracy: 0.859375\n",
            "Step: 224600, Loss: 399.91363525390625, Accuracy: 0.87890625\n",
            "Step: 224700, Loss: 398.23919677734375, Accuracy: 0.8828125\n",
            "Step: 224800, Loss: 400.41558837890625, Accuracy: 0.8671875\n",
            "Step: 224900, Loss: 394.49725341796875, Accuracy: 0.90234375\n",
            "Step: 225000, Loss: 398.5155029296875, Accuracy: 0.8828125\n",
            "Step: 225100, Loss: 401.0787048339844, Accuracy: 0.875\n",
            "Step: 225200, Loss: 400.2215576171875, Accuracy: 0.8828125\n",
            "Step: 225300, Loss: 404.550537109375, Accuracy: 0.8515625\n",
            "Step: 225400, Loss: 396.33819580078125, Accuracy: 0.8984375\n",
            "Step: 225500, Loss: 395.9748229980469, Accuracy: 0.8984375\n",
            "Step: 225600, Loss: 405.9540710449219, Accuracy: 0.8515625\n",
            "Step: 225700, Loss: 398.58758544921875, Accuracy: 0.88671875\n",
            "Step: 225800, Loss: 398.225341796875, Accuracy: 0.890625\n",
            "Step: 225900, Loss: 407.18194580078125, Accuracy: 0.83984375\n",
            "Step: 226000, Loss: 398.2497863769531, Accuracy: 0.88671875\n",
            "Step: 226100, Loss: 399.5791320800781, Accuracy: 0.8828125\n",
            "Step: 226200, Loss: 402.4616394042969, Accuracy: 0.85546875\n",
            "Step: 226300, Loss: 407.1086730957031, Accuracy: 0.83984375\n",
            "Step: 226400, Loss: 404.32666015625, Accuracy: 0.86328125\n",
            "Step: 226500, Loss: 403.7094421386719, Accuracy: 0.86328125\n",
            "Step: 226600, Loss: 402.617431640625, Accuracy: 0.87890625\n",
            "Step: 226700, Loss: 405.4197998046875, Accuracy: 0.859375\n",
            "Step: 226800, Loss: 408.03631591796875, Accuracy: 0.83984375\n",
            "Step: 226900, Loss: 409.10687255859375, Accuracy: 0.84375\n",
            "Step: 227000, Loss: 402.73150634765625, Accuracy: 0.8671875\n",
            "Step: 227100, Loss: 405.1863708496094, Accuracy: 0.859375\n",
            "Step: 227200, Loss: 394.73828125, Accuracy: 0.9140625\n",
            "Step: 227300, Loss: 404.74310302734375, Accuracy: 0.8515625\n",
            "Step: 227400, Loss: 408.57025146484375, Accuracy: 0.84375\n",
            "Step: 227500, Loss: 400.774658203125, Accuracy: 0.8828125\n",
            "Step: 227600, Loss: 400.3376159667969, Accuracy: 0.88671875\n",
            "Step: 227700, Loss: 403.83367919921875, Accuracy: 0.86328125\n",
            "Step: 227800, Loss: 404.38897705078125, Accuracy: 0.8671875\n",
            "Step: 227900, Loss: 395.8168640136719, Accuracy: 0.89453125\n",
            "Step: 228000, Loss: 405.35003662109375, Accuracy: 0.86328125\n",
            "Step: 228100, Loss: 401.96514892578125, Accuracy: 0.875\n",
            "Step: 228200, Loss: 403.9480895996094, Accuracy: 0.859375\n",
            "Step: 228300, Loss: 403.185302734375, Accuracy: 0.86328125\n",
            "Step: 228400, Loss: 402.084228515625, Accuracy: 0.87109375\n",
            "Step: 228500, Loss: 406.63189697265625, Accuracy: 0.8515625\n",
            "Step: 228600, Loss: 401.147705078125, Accuracy: 0.8671875\n",
            "Step: 228700, Loss: 398.8166809082031, Accuracy: 0.89453125\n",
            "Step: 228800, Loss: 401.7129821777344, Accuracy: 0.8671875\n",
            "Step: 228900, Loss: 393.9075622558594, Accuracy: 0.90625\n",
            "Step: 229000, Loss: 402.13897705078125, Accuracy: 0.875\n",
            "Step: 229100, Loss: 401.637939453125, Accuracy: 0.87109375\n",
            "Step: 229200, Loss: 401.3540954589844, Accuracy: 0.87109375\n",
            "Step: 229300, Loss: 400.3404541015625, Accuracy: 0.875\n",
            "Step: 229400, Loss: 400.9105224609375, Accuracy: 0.875\n",
            "Step: 229500, Loss: 407.8072509765625, Accuracy: 0.84765625\n",
            "Step: 229600, Loss: 400.9586181640625, Accuracy: 0.87109375\n",
            "Step: 229700, Loss: 402.2053527832031, Accuracy: 0.8671875\n",
            "Step: 229800, Loss: 393.90606689453125, Accuracy: 0.91015625\n",
            "Step: 229900, Loss: 401.2764892578125, Accuracy: 0.875\n",
            "Step: 230000, Loss: 394.0770263671875, Accuracy: 0.90234375\n",
            "Step: 230100, Loss: 402.5135192871094, Accuracy: 0.859375\n",
            "Step: 230200, Loss: 405.63482666015625, Accuracy: 0.859375\n",
            "Step: 230300, Loss: 398.4329528808594, Accuracy: 0.890625\n",
            "Step: 230400, Loss: 398.64208984375, Accuracy: 0.890625\n",
            "Step: 230500, Loss: 399.34332275390625, Accuracy: 0.88671875\n",
            "Step: 230600, Loss: 395.91400146484375, Accuracy: 0.890625\n",
            "Step: 230700, Loss: 395.8852844238281, Accuracy: 0.8984375\n",
            "Step: 230800, Loss: 402.5986633300781, Accuracy: 0.875\n",
            "Step: 230900, Loss: 404.6145324707031, Accuracy: 0.8671875\n",
            "Step: 231000, Loss: 395.88348388671875, Accuracy: 0.90234375\n",
            "Step: 231100, Loss: 407.2310791015625, Accuracy: 0.83984375\n",
            "Step: 231200, Loss: 400.56903076171875, Accuracy: 0.8828125\n",
            "Step: 231300, Loss: 398.2392272949219, Accuracy: 0.89453125\n",
            "Step: 231400, Loss: 397.94866943359375, Accuracy: 0.890625\n",
            "Step: 231500, Loss: 395.9833984375, Accuracy: 0.890625\n",
            "Step: 231600, Loss: 400.54522705078125, Accuracy: 0.8828125\n",
            "Step: 231700, Loss: 401.78759765625, Accuracy: 0.86328125\n",
            "Step: 231800, Loss: 395.8529052734375, Accuracy: 0.8984375\n",
            "Step: 231900, Loss: 401.1689453125, Accuracy: 0.8671875\n",
            "Step: 232000, Loss: 400.2616271972656, Accuracy: 0.87890625\n",
            "Step: 232100, Loss: 402.40283203125, Accuracy: 0.86328125\n",
            "Step: 232200, Loss: 406.6033935546875, Accuracy: 0.84375\n",
            "Step: 232300, Loss: 397.81927490234375, Accuracy: 0.88671875\n",
            "Step: 232400, Loss: 403.99066162109375, Accuracy: 0.8671875\n",
            "Step: 232500, Loss: 400.854248046875, Accuracy: 0.87890625\n",
            "Step: 232600, Loss: 402.97454833984375, Accuracy: 0.86328125\n",
            "Step: 232700, Loss: 397.310791015625, Accuracy: 0.89453125\n",
            "Step: 232800, Loss: 404.5015563964844, Accuracy: 0.86328125\n",
            "Step: 232900, Loss: 405.7177734375, Accuracy: 0.85546875\n",
            "Step: 233000, Loss: 404.0456848144531, Accuracy: 0.859375\n",
            "Step: 233100, Loss: 400.3391418457031, Accuracy: 0.8828125\n",
            "Step: 233200, Loss: 393.85791015625, Accuracy: 0.90234375\n",
            "Step: 233300, Loss: 401.587158203125, Accuracy: 0.875\n",
            "Step: 233400, Loss: 407.33172607421875, Accuracy: 0.85546875\n",
            "Step: 233500, Loss: 402.63092041015625, Accuracy: 0.8671875\n",
            "Step: 233600, Loss: 403.13140869140625, Accuracy: 0.86328125\n",
            "Step: 233700, Loss: 400.5416259765625, Accuracy: 0.8671875\n",
            "Step: 233800, Loss: 396.3855285644531, Accuracy: 0.90234375\n",
            "Step: 233900, Loss: 398.51568603515625, Accuracy: 0.8828125\n",
            "Step: 234000, Loss: 409.63330078125, Accuracy: 0.83984375\n",
            "Step: 234100, Loss: 401.1053466796875, Accuracy: 0.87109375\n",
            "Step: 234200, Loss: 398.0265197753906, Accuracy: 0.890625\n",
            "Step: 234300, Loss: 400.96527099609375, Accuracy: 0.875\n",
            "Step: 234400, Loss: 403.62872314453125, Accuracy: 0.84375\n",
            "Step: 234500, Loss: 396.6776123046875, Accuracy: 0.89453125\n",
            "Step: 234600, Loss: 398.14617919921875, Accuracy: 0.8828125\n",
            "Step: 234700, Loss: 404.78472900390625, Accuracy: 0.8515625\n",
            "Step: 234800, Loss: 402.83685302734375, Accuracy: 0.8671875\n",
            "Step: 234900, Loss: 399.46954345703125, Accuracy: 0.88671875\n",
            "Step: 235000, Loss: 409.76287841796875, Accuracy: 0.83984375\n",
            "Step: 235100, Loss: 400.10174560546875, Accuracy: 0.875\n",
            "Step: 235200, Loss: 403.6151123046875, Accuracy: 0.859375\n",
            "Step: 235300, Loss: 400.3558349609375, Accuracy: 0.875\n",
            "Step: 235400, Loss: 403.03509521484375, Accuracy: 0.859375\n",
            "Step: 235500, Loss: 400.17181396484375, Accuracy: 0.8671875\n",
            "Step: 235600, Loss: 399.61553955078125, Accuracy: 0.8828125\n",
            "Step: 235700, Loss: 397.98138427734375, Accuracy: 0.88671875\n",
            "Step: 235800, Loss: 397.46807861328125, Accuracy: 0.8984375\n",
            "Step: 235900, Loss: 402.2478332519531, Accuracy: 0.875\n",
            "Step: 236000, Loss: 398.66619873046875, Accuracy: 0.88671875\n",
            "Step: 236100, Loss: 397.6214904785156, Accuracy: 0.890625\n",
            "Step: 236200, Loss: 407.57635498046875, Accuracy: 0.83984375\n",
            "Step: 236300, Loss: 407.72442626953125, Accuracy: 0.83984375\n",
            "Step: 236400, Loss: 403.7744140625, Accuracy: 0.87109375\n",
            "Step: 236500, Loss: 390.373779296875, Accuracy: 0.921875\n",
            "Step: 236600, Loss: 402.22113037109375, Accuracy: 0.875\n",
            "Step: 236700, Loss: 399.561279296875, Accuracy: 0.88671875\n",
            "Step: 236800, Loss: 394.28631591796875, Accuracy: 0.90625\n",
            "Step: 236900, Loss: 403.6314697265625, Accuracy: 0.87109375\n",
            "Step: 237000, Loss: 404.6968688964844, Accuracy: 0.85546875\n",
            "Step: 237100, Loss: 398.95733642578125, Accuracy: 0.8828125\n",
            "Step: 237200, Loss: 404.64630126953125, Accuracy: 0.859375\n",
            "Step: 237300, Loss: 405.41363525390625, Accuracy: 0.86328125\n",
            "Step: 237400, Loss: 397.321533203125, Accuracy: 0.89453125\n",
            "Step: 237500, Loss: 410.803466796875, Accuracy: 0.8125\n",
            "Step: 237600, Loss: 408.0005798339844, Accuracy: 0.8515625\n",
            "Step: 237700, Loss: 403.774169921875, Accuracy: 0.875\n",
            "Step: 237800, Loss: 413.3206787109375, Accuracy: 0.8203125\n",
            "Step: 237900, Loss: 401.37994384765625, Accuracy: 0.87109375\n",
            "Step: 238000, Loss: 405.04827880859375, Accuracy: 0.85546875\n",
            "Step: 238100, Loss: 412.66522216796875, Accuracy: 0.8203125\n",
            "Step: 238200, Loss: 398.1217956542969, Accuracy: 0.87890625\n",
            "Step: 238300, Loss: 403.6639404296875, Accuracy: 0.86328125\n",
            "Step: 238400, Loss: 407.4923400878906, Accuracy: 0.84765625\n",
            "Step: 238500, Loss: 406.00567626953125, Accuracy: 0.84765625\n",
            "Step: 238600, Loss: 397.3128662109375, Accuracy: 0.8984375\n",
            "Step: 238700, Loss: 390.7530517578125, Accuracy: 0.92578125\n",
            "Step: 238800, Loss: 411.6131286621094, Accuracy: 0.82421875\n",
            "Step: 238900, Loss: 397.3553466796875, Accuracy: 0.88671875\n",
            "Step: 239000, Loss: 399.97076416015625, Accuracy: 0.875\n",
            "Step: 239100, Loss: 401.72918701171875, Accuracy: 0.87109375\n",
            "Step: 239200, Loss: 399.0616455078125, Accuracy: 0.8828125\n",
            "Step: 239300, Loss: 405.943359375, Accuracy: 0.8515625\n",
            "Step: 239400, Loss: 396.8016357421875, Accuracy: 0.89453125\n",
            "Step: 239500, Loss: 400.86431884765625, Accuracy: 0.8828125\n",
            "Step: 239600, Loss: 400.16912841796875, Accuracy: 0.87890625\n",
            "Step: 239700, Loss: 402.84417724609375, Accuracy: 0.8671875\n",
            "Step: 239800, Loss: 399.34149169921875, Accuracy: 0.8828125\n",
            "Step: 239900, Loss: 403.84442138671875, Accuracy: 0.859375\n",
            "Step: 240000, Loss: 406.33538818359375, Accuracy: 0.85546875\n",
            "Step: 240100, Loss: 393.003662109375, Accuracy: 0.9140625\n",
            "Step: 240200, Loss: 393.0374755859375, Accuracy: 0.90234375\n",
            "Step: 240300, Loss: 402.10113525390625, Accuracy: 0.8671875\n",
            "Step: 240400, Loss: 404.0954895019531, Accuracy: 0.86328125\n",
            "Step: 240500, Loss: 406.896240234375, Accuracy: 0.85546875\n",
            "Step: 240600, Loss: 404.89813232421875, Accuracy: 0.85546875\n",
            "Step: 240700, Loss: 403.3258972167969, Accuracy: 0.8671875\n",
            "Step: 240800, Loss: 401.89349365234375, Accuracy: 0.859375\n",
            "Step: 240900, Loss: 396.6629638671875, Accuracy: 0.89453125\n",
            "Step: 241000, Loss: 406.1019592285156, Accuracy: 0.859375\n",
            "Step: 241100, Loss: 405.5087890625, Accuracy: 0.84765625\n",
            "Step: 241200, Loss: 397.2393493652344, Accuracy: 0.89453125\n",
            "Step: 241300, Loss: 402.202880859375, Accuracy: 0.875\n",
            "Step: 241400, Loss: 400.51763916015625, Accuracy: 0.87890625\n",
            "Step: 241500, Loss: 395.8601379394531, Accuracy: 0.8984375\n",
            "Step: 241600, Loss: 401.3200378417969, Accuracy: 0.875\n",
            "Step: 241700, Loss: 396.59344482421875, Accuracy: 0.90234375\n",
            "Step: 241800, Loss: 397.55975341796875, Accuracy: 0.88671875\n",
            "Step: 241900, Loss: 392.36309814453125, Accuracy: 0.91015625\n",
            "Step: 242000, Loss: 400.590087890625, Accuracy: 0.8828125\n",
            "Step: 242100, Loss: 403.69537353515625, Accuracy: 0.86328125\n",
            "Step: 242200, Loss: 405.6408996582031, Accuracy: 0.85546875\n",
            "Step: 242300, Loss: 400.05047607421875, Accuracy: 0.8671875\n",
            "Step: 242400, Loss: 395.7375793457031, Accuracy: 0.8984375\n",
            "Step: 242500, Loss: 399.07269287109375, Accuracy: 0.88671875\n",
            "Step: 242600, Loss: 396.5626220703125, Accuracy: 0.8984375\n",
            "Step: 242700, Loss: 402.88421630859375, Accuracy: 0.87109375\n",
            "Step: 242800, Loss: 402.7083740234375, Accuracy: 0.85546875\n",
            "Step: 242900, Loss: 401.7422180175781, Accuracy: 0.86328125\n",
            "Step: 243000, Loss: 400.0452880859375, Accuracy: 0.87890625\n",
            "Step: 243100, Loss: 400.8260498046875, Accuracy: 0.87890625\n",
            "Step: 243200, Loss: 401.5380859375, Accuracy: 0.8671875\n",
            "Step: 243300, Loss: 396.0806884765625, Accuracy: 0.890625\n",
            "Step: 243400, Loss: 407.6912841796875, Accuracy: 0.84765625\n",
            "Step: 243500, Loss: 401.2818603515625, Accuracy: 0.87890625\n",
            "Step: 243600, Loss: 400.11456298828125, Accuracy: 0.87890625\n",
            "Step: 243700, Loss: 406.9005126953125, Accuracy: 0.859375\n",
            "Step: 243800, Loss: 396.5672912597656, Accuracy: 0.88671875\n",
            "Step: 243900, Loss: 407.9803161621094, Accuracy: 0.828125\n",
            "Step: 244000, Loss: 400.47467041015625, Accuracy: 0.875\n",
            "Step: 244100, Loss: 399.08001708984375, Accuracy: 0.8828125\n",
            "Step: 244200, Loss: 405.1516418457031, Accuracy: 0.86328125\n",
            "Step: 244300, Loss: 394.5852966308594, Accuracy: 0.90234375\n",
            "Step: 244400, Loss: 398.8130187988281, Accuracy: 0.88671875\n",
            "Step: 244500, Loss: 404.9290771484375, Accuracy: 0.85546875\n",
            "Step: 244600, Loss: 401.72265625, Accuracy: 0.875\n",
            "Step: 244700, Loss: 393.2559814453125, Accuracy: 0.90234375\n",
            "Step: 244800, Loss: 396.92803955078125, Accuracy: 0.89453125\n",
            "Step: 244900, Loss: 402.8917236328125, Accuracy: 0.85546875\n",
            "Step: 245000, Loss: 402.09539794921875, Accuracy: 0.87109375\n",
            "Step: 245100, Loss: 401.1702880859375, Accuracy: 0.87890625\n",
            "Step: 245200, Loss: 393.146240234375, Accuracy: 0.90625\n",
            "Step: 245300, Loss: 408.3920593261719, Accuracy: 0.8359375\n",
            "Step: 245400, Loss: 395.72772216796875, Accuracy: 0.890625\n",
            "Step: 245500, Loss: 406.35455322265625, Accuracy: 0.85546875\n",
            "Step: 245600, Loss: 398.66656494140625, Accuracy: 0.8828125\n",
            "Step: 245700, Loss: 399.3572082519531, Accuracy: 0.890625\n",
            "Step: 245800, Loss: 395.3648986816406, Accuracy: 0.89453125\n",
            "Step: 245900, Loss: 406.57940673828125, Accuracy: 0.84765625\n",
            "Step: 246000, Loss: 394.45916748046875, Accuracy: 0.8984375\n",
            "Step: 246100, Loss: 407.02099609375, Accuracy: 0.83984375\n",
            "Step: 246200, Loss: 401.2435302734375, Accuracy: 0.87890625\n",
            "Step: 246300, Loss: 406.11883544921875, Accuracy: 0.8515625\n",
            "Step: 246400, Loss: 402.1772155761719, Accuracy: 0.87109375\n",
            "Step: 246500, Loss: 404.29803466796875, Accuracy: 0.859375\n",
            "Step: 246600, Loss: 398.84783935546875, Accuracy: 0.890625\n",
            "Step: 246700, Loss: 398.9087829589844, Accuracy: 0.890625\n",
            "Step: 246800, Loss: 405.51104736328125, Accuracy: 0.859375\n",
            "Step: 246900, Loss: 400.7308044433594, Accuracy: 0.875\n",
            "Step: 247000, Loss: 396.12713623046875, Accuracy: 0.8984375\n",
            "Step: 247100, Loss: 393.3980407714844, Accuracy: 0.9140625\n",
            "Step: 247200, Loss: 403.9965515136719, Accuracy: 0.86328125\n",
            "Step: 247300, Loss: 406.8577575683594, Accuracy: 0.8515625\n",
            "Step: 247400, Loss: 394.3441162109375, Accuracy: 0.91015625\n",
            "Step: 247500, Loss: 397.8370666503906, Accuracy: 0.89453125\n",
            "Step: 247600, Loss: 400.3450622558594, Accuracy: 0.8828125\n",
            "Step: 247700, Loss: 397.59112548828125, Accuracy: 0.890625\n",
            "Step: 247800, Loss: 398.6356506347656, Accuracy: 0.87890625\n",
            "Step: 247900, Loss: 410.5963439941406, Accuracy: 0.83203125\n",
            "Step: 248000, Loss: 395.74530029296875, Accuracy: 0.90234375\n",
            "Step: 248100, Loss: 400.6844787597656, Accuracy: 0.87890625\n",
            "Step: 248200, Loss: 400.5208740234375, Accuracy: 0.875\n",
            "Step: 248300, Loss: 403.9550476074219, Accuracy: 0.86328125\n",
            "Step: 248400, Loss: 398.0586853027344, Accuracy: 0.88671875\n",
            "Step: 248500, Loss: 399.5926513671875, Accuracy: 0.8828125\n",
            "Step: 248600, Loss: 399.8746032714844, Accuracy: 0.875\n",
            "Step: 248700, Loss: 394.2484436035156, Accuracy: 0.90625\n",
            "Step: 248800, Loss: 396.3475341796875, Accuracy: 0.90625\n",
            "Step: 248900, Loss: 403.5879821777344, Accuracy: 0.86328125\n",
            "Step: 249000, Loss: 400.3310241699219, Accuracy: 0.87890625\n",
            "Step: 249100, Loss: 395.0728759765625, Accuracy: 0.90234375\n",
            "Step: 249200, Loss: 398.6331787109375, Accuracy: 0.88671875\n",
            "Step: 249300, Loss: 403.0201416015625, Accuracy: 0.87109375\n",
            "Step: 249400, Loss: 412.10723876953125, Accuracy: 0.8203125\n",
            "Step: 249500, Loss: 403.19219970703125, Accuracy: 0.86328125\n",
            "Step: 249600, Loss: 403.65924072265625, Accuracy: 0.8671875\n",
            "Step: 249700, Loss: 399.9520263671875, Accuracy: 0.875\n",
            "Step: 249800, Loss: 401.96490478515625, Accuracy: 0.87109375\n",
            "Step: 249900, Loss: 407.8495178222656, Accuracy: 0.84765625\n",
            "Step: 250000, Loss: 405.1820068359375, Accuracy: 0.85546875\n",
            "Step: 250100, Loss: 400.2225341796875, Accuracy: 0.8828125\n",
            "Step: 250200, Loss: 403.145751953125, Accuracy: 0.85546875\n",
            "Step: 250300, Loss: 397.0968017578125, Accuracy: 0.89453125\n",
            "Step: 250400, Loss: 399.18292236328125, Accuracy: 0.87890625\n",
            "Step: 250500, Loss: 402.89385986328125, Accuracy: 0.85546875\n",
            "Step: 250600, Loss: 395.7828674316406, Accuracy: 0.90625\n",
            "Step: 250700, Loss: 393.41015625, Accuracy: 0.90625\n",
            "Step: 250800, Loss: 395.1025390625, Accuracy: 0.89453125\n",
            "Step: 250900, Loss: 395.69970703125, Accuracy: 0.89453125\n",
            "Step: 251000, Loss: 399.50469970703125, Accuracy: 0.87890625\n",
            "Step: 251100, Loss: 400.75262451171875, Accuracy: 0.875\n",
            "Step: 251200, Loss: 402.0068664550781, Accuracy: 0.87890625\n",
            "Step: 251300, Loss: 400.1302185058594, Accuracy: 0.875\n",
            "Step: 251400, Loss: 403.74554443359375, Accuracy: 0.86328125\n",
            "Step: 251500, Loss: 399.96453857421875, Accuracy: 0.87109375\n",
            "Step: 251600, Loss: 395.92449951171875, Accuracy: 0.89453125\n",
            "Step: 251700, Loss: 402.2857971191406, Accuracy: 0.875\n",
            "Step: 251800, Loss: 400.0458984375, Accuracy: 0.8828125\n",
            "Step: 251900, Loss: 404.12994384765625, Accuracy: 0.8515625\n",
            "Step: 252000, Loss: 400.22003173828125, Accuracy: 0.875\n",
            "Step: 252100, Loss: 398.50299072265625, Accuracy: 0.88671875\n",
            "Step: 252200, Loss: 411.0778503417969, Accuracy: 0.828125\n",
            "Step: 252300, Loss: 395.71148681640625, Accuracy: 0.90234375\n",
            "Step: 252400, Loss: 406.1918640136719, Accuracy: 0.8515625\n",
            "Step: 252500, Loss: 398.9324951171875, Accuracy: 0.89453125\n",
            "Step: 252600, Loss: 408.54595947265625, Accuracy: 0.84375\n",
            "Step: 252700, Loss: 400.67333984375, Accuracy: 0.87890625\n",
            "Step: 252800, Loss: 397.12335205078125, Accuracy: 0.88671875\n",
            "Step: 252900, Loss: 400.31011962890625, Accuracy: 0.875\n",
            "Step: 253000, Loss: 400.5464172363281, Accuracy: 0.8671875\n",
            "Step: 253100, Loss: 407.9737548828125, Accuracy: 0.84375\n",
            "Step: 253200, Loss: 396.349853515625, Accuracy: 0.8984375\n",
            "Step: 253300, Loss: 396.98114013671875, Accuracy: 0.890625\n",
            "Step: 253400, Loss: 403.1778564453125, Accuracy: 0.86328125\n",
            "Step: 253500, Loss: 399.8995056152344, Accuracy: 0.875\n",
            "Step: 253600, Loss: 392.6302490234375, Accuracy: 0.90625\n",
            "Step: 253700, Loss: 398.9813232421875, Accuracy: 0.87890625\n",
            "Step: 253800, Loss: 401.0177001953125, Accuracy: 0.875\n",
            "Step: 253900, Loss: 399.88665771484375, Accuracy: 0.8828125\n",
            "Step: 254000, Loss: 398.79119873046875, Accuracy: 0.890625\n",
            "Step: 254100, Loss: 402.2628173828125, Accuracy: 0.8671875\n",
            "Step: 254200, Loss: 399.50958251953125, Accuracy: 0.87109375\n",
            "Step: 254300, Loss: 402.7821350097656, Accuracy: 0.8671875\n",
            "Step: 254400, Loss: 401.8450927734375, Accuracy: 0.87109375\n",
            "Step: 254500, Loss: 403.00921630859375, Accuracy: 0.87109375\n",
            "Step: 254600, Loss: 396.97314453125, Accuracy: 0.88671875\n",
            "Step: 254700, Loss: 396.95953369140625, Accuracy: 0.890625\n",
            "Step: 254800, Loss: 403.24884033203125, Accuracy: 0.8671875\n",
            "Step: 254900, Loss: 400.3557434082031, Accuracy: 0.875\n",
            "Step: 255000, Loss: 408.4732360839844, Accuracy: 0.8515625\n",
            "Step: 255100, Loss: 403.0054931640625, Accuracy: 0.86328125\n",
            "Step: 255200, Loss: 403.189208984375, Accuracy: 0.8671875\n",
            "Step: 255300, Loss: 390.6966247558594, Accuracy: 0.92578125\n",
            "Step: 255400, Loss: 403.1646728515625, Accuracy: 0.875\n",
            "Step: 255500, Loss: 401.2691955566406, Accuracy: 0.87109375\n",
            "Step: 255600, Loss: 402.82623291015625, Accuracy: 0.87109375\n",
            "Step: 255700, Loss: 398.46014404296875, Accuracy: 0.88671875\n",
            "Step: 255800, Loss: 406.4620361328125, Accuracy: 0.859375\n",
            "Step: 255900, Loss: 405.64990234375, Accuracy: 0.8515625\n",
            "Step: 256000, Loss: 400.2347412109375, Accuracy: 0.87890625\n",
            "Step: 256100, Loss: 400.48211669921875, Accuracy: 0.8671875\n",
            "Step: 256200, Loss: 398.434814453125, Accuracy: 0.8828125\n",
            "Step: 256300, Loss: 394.20068359375, Accuracy: 0.8984375\n",
            "Step: 256400, Loss: 406.96734619140625, Accuracy: 0.84375\n",
            "Step: 256500, Loss: 401.82977294921875, Accuracy: 0.859375\n",
            "Step: 256600, Loss: 398.60638427734375, Accuracy: 0.8828125\n",
            "Step: 256700, Loss: 402.28314208984375, Accuracy: 0.8671875\n",
            "Step: 256800, Loss: 405.7357482910156, Accuracy: 0.84765625\n",
            "Step: 256900, Loss: 403.7943115234375, Accuracy: 0.859375\n",
            "Step: 257000, Loss: 400.64654541015625, Accuracy: 0.875\n",
            "Step: 257100, Loss: 400.3970031738281, Accuracy: 0.87890625\n",
            "Step: 257200, Loss: 400.460205078125, Accuracy: 0.87109375\n",
            "Step: 257300, Loss: 404.988037109375, Accuracy: 0.85546875\n",
            "Step: 257400, Loss: 398.6703796386719, Accuracy: 0.88671875\n",
            "Step: 257500, Loss: 409.32574462890625, Accuracy: 0.83984375\n",
            "Step: 257600, Loss: 404.765625, Accuracy: 0.84765625\n",
            "Step: 257700, Loss: 404.9214782714844, Accuracy: 0.85546875\n",
            "Step: 257800, Loss: 398.3373107910156, Accuracy: 0.8828125\n",
            "Step: 257900, Loss: 402.09893798828125, Accuracy: 0.8671875\n",
            "Step: 258000, Loss: 405.1492614746094, Accuracy: 0.85546875\n",
            "Step: 258100, Loss: 401.46759033203125, Accuracy: 0.8671875\n",
            "Step: 258200, Loss: 400.61669921875, Accuracy: 0.87109375\n",
            "Step: 258300, Loss: 397.35906982421875, Accuracy: 0.8984375\n",
            "Step: 258400, Loss: 405.91119384765625, Accuracy: 0.86328125\n",
            "Step: 258500, Loss: 397.25439453125, Accuracy: 0.88671875\n",
            "Step: 258600, Loss: 404.2332763671875, Accuracy: 0.85546875\n",
            "Step: 258700, Loss: 397.8338623046875, Accuracy: 0.890625\n",
            "Step: 258800, Loss: 404.03717041015625, Accuracy: 0.86328125\n",
            "Step: 258900, Loss: 402.24334716796875, Accuracy: 0.87109375\n",
            "Step: 259000, Loss: 402.27630615234375, Accuracy: 0.875\n",
            "Step: 259100, Loss: 399.50372314453125, Accuracy: 0.88671875\n",
            "Step: 259200, Loss: 408.18798828125, Accuracy: 0.8359375\n",
            "Step: 259300, Loss: 397.3502502441406, Accuracy: 0.890625\n",
            "Step: 259400, Loss: 403.51275634765625, Accuracy: 0.87109375\n",
            "Step: 259500, Loss: 408.4685363769531, Accuracy: 0.84375\n",
            "Step: 259600, Loss: 398.9884338378906, Accuracy: 0.87890625\n",
            "Step: 259700, Loss: 401.5310363769531, Accuracy: 0.87109375\n",
            "Step: 259800, Loss: 403.03717041015625, Accuracy: 0.86328125\n",
            "Step: 259900, Loss: 402.9667663574219, Accuracy: 0.8671875\n",
            "Step: 260000, Loss: 399.1052551269531, Accuracy: 0.87890625\n",
            "Step: 260100, Loss: 402.06011962890625, Accuracy: 0.8671875\n",
            "Step: 260200, Loss: 403.71624755859375, Accuracy: 0.86328125\n",
            "Step: 260300, Loss: 405.19830322265625, Accuracy: 0.85546875\n",
            "Step: 260400, Loss: 398.41204833984375, Accuracy: 0.8828125\n",
            "Step: 260500, Loss: 401.7502136230469, Accuracy: 0.875\n",
            "Step: 260600, Loss: 392.48095703125, Accuracy: 0.92578125\n",
            "Step: 260700, Loss: 405.63531494140625, Accuracy: 0.8515625\n",
            "Step: 260800, Loss: 397.8390808105469, Accuracy: 0.890625\n",
            "Step: 260900, Loss: 403.34674072265625, Accuracy: 0.87109375\n",
            "Step: 261000, Loss: 400.481201171875, Accuracy: 0.8828125\n",
            "Step: 261100, Loss: 397.0441589355469, Accuracy: 0.890625\n",
            "Step: 261200, Loss: 405.24591064453125, Accuracy: 0.8671875\n",
            "Step: 261300, Loss: 402.01739501953125, Accuracy: 0.8671875\n",
            "Step: 261400, Loss: 405.9468994140625, Accuracy: 0.8515625\n",
            "Step: 261500, Loss: 407.4405822753906, Accuracy: 0.83984375\n",
            "Step: 261600, Loss: 403.3057861328125, Accuracy: 0.86328125\n",
            "Step: 261700, Loss: 395.8573303222656, Accuracy: 0.90234375\n",
            "Step: 261800, Loss: 393.9729309082031, Accuracy: 0.90234375\n",
            "Step: 261900, Loss: 406.2841796875, Accuracy: 0.8515625\n",
            "Step: 262000, Loss: 399.3962707519531, Accuracy: 0.875\n",
            "Step: 262100, Loss: 401.18475341796875, Accuracy: 0.87890625\n",
            "Step: 262200, Loss: 402.0162658691406, Accuracy: 0.8671875\n",
            "Step: 262300, Loss: 402.64111328125, Accuracy: 0.86328125\n",
            "Step: 262400, Loss: 405.0462646484375, Accuracy: 0.8515625\n",
            "Step: 262500, Loss: 405.68212890625, Accuracy: 0.8515625\n",
            "Step: 262600, Loss: 400.892333984375, Accuracy: 0.87109375\n",
            "Step: 262700, Loss: 399.30596923828125, Accuracy: 0.875\n",
            "Step: 262800, Loss: 403.1428527832031, Accuracy: 0.86328125\n",
            "Step: 262900, Loss: 399.92608642578125, Accuracy: 0.88671875\n",
            "Step: 263000, Loss: 402.0382080078125, Accuracy: 0.86328125\n",
            "Step: 263100, Loss: 406.2488708496094, Accuracy: 0.85546875\n",
            "Step: 263200, Loss: 401.3439025878906, Accuracy: 0.875\n",
            "Step: 263300, Loss: 405.48468017578125, Accuracy: 0.8515625\n",
            "Step: 263400, Loss: 399.4251403808594, Accuracy: 0.88671875\n",
            "Step: 263500, Loss: 391.57440185546875, Accuracy: 0.921875\n",
            "Step: 263600, Loss: 401.5517272949219, Accuracy: 0.875\n",
            "Step: 263700, Loss: 401.36016845703125, Accuracy: 0.87890625\n",
            "Step: 263800, Loss: 401.62353515625, Accuracy: 0.87109375\n",
            "Step: 263900, Loss: 398.1034240722656, Accuracy: 0.8984375\n",
            "Step: 264000, Loss: 397.08074951171875, Accuracy: 0.875\n",
            "Step: 264100, Loss: 395.86920166015625, Accuracy: 0.90625\n",
            "Step: 264200, Loss: 394.36285400390625, Accuracy: 0.90625\n",
            "Step: 264300, Loss: 400.4939270019531, Accuracy: 0.88671875\n",
            "Step: 264400, Loss: 401.1741027832031, Accuracy: 0.8671875\n",
            "Step: 264500, Loss: 399.6894836425781, Accuracy: 0.88671875\n",
            "Step: 264600, Loss: 396.1835632324219, Accuracy: 0.890625\n",
            "Step: 264700, Loss: 403.8368835449219, Accuracy: 0.86328125\n",
            "Step: 264800, Loss: 396.2347412109375, Accuracy: 0.90234375\n",
            "Step: 264900, Loss: 403.72894287109375, Accuracy: 0.8671875\n",
            "Step: 265000, Loss: 399.03021240234375, Accuracy: 0.88671875\n",
            "Step: 265100, Loss: 403.64874267578125, Accuracy: 0.859375\n",
            "Step: 265200, Loss: 403.09991455078125, Accuracy: 0.875\n",
            "Step: 265300, Loss: 397.4990539550781, Accuracy: 0.88671875\n",
            "Step: 265400, Loss: 400.2935791015625, Accuracy: 0.88671875\n",
            "Step: 265500, Loss: 401.1097412109375, Accuracy: 0.87890625\n",
            "Step: 265600, Loss: 402.5369873046875, Accuracy: 0.86328125\n",
            "Step: 265700, Loss: 406.97271728515625, Accuracy: 0.85546875\n",
            "Step: 265800, Loss: 406.0588073730469, Accuracy: 0.859375\n",
            "Step: 265900, Loss: 399.58807373046875, Accuracy: 0.8828125\n",
            "Step: 266000, Loss: 398.80548095703125, Accuracy: 0.88671875\n",
            "Step: 266100, Loss: 398.7701416015625, Accuracy: 0.890625\n",
            "Step: 266200, Loss: 394.9718933105469, Accuracy: 0.90234375\n",
            "Step: 266300, Loss: 397.3621826171875, Accuracy: 0.88671875\n",
            "Step: 266400, Loss: 397.99737548828125, Accuracy: 0.890625\n",
            "Step: 266500, Loss: 398.3927307128906, Accuracy: 0.87890625\n",
            "Step: 266600, Loss: 392.79071044921875, Accuracy: 0.91796875\n",
            "Step: 266700, Loss: 401.35284423828125, Accuracy: 0.875\n",
            "Step: 266800, Loss: 398.8731689453125, Accuracy: 0.8828125\n",
            "Step: 266900, Loss: 399.0843505859375, Accuracy: 0.890625\n",
            "Step: 267000, Loss: 400.25457763671875, Accuracy: 0.87890625\n",
            "Step: 267100, Loss: 402.76104736328125, Accuracy: 0.875\n",
            "Step: 267200, Loss: 400.1007080078125, Accuracy: 0.8828125\n",
            "Step: 267300, Loss: 401.47900390625, Accuracy: 0.8671875\n",
            "Step: 267400, Loss: 398.8207702636719, Accuracy: 0.8828125\n",
            "Step: 267500, Loss: 392.39044189453125, Accuracy: 0.91015625\n",
            "Step: 267600, Loss: 400.90057373046875, Accuracy: 0.87890625\n",
            "Step: 267700, Loss: 408.7185974121094, Accuracy: 0.84375\n",
            "Step: 267800, Loss: 398.987060546875, Accuracy: 0.87890625\n",
            "Step: 267900, Loss: 407.45709228515625, Accuracy: 0.84765625\n",
            "Step: 268000, Loss: 396.52935791015625, Accuracy: 0.89453125\n",
            "Step: 268100, Loss: 400.85638427734375, Accuracy: 0.875\n",
            "Step: 268200, Loss: 394.1478576660156, Accuracy: 0.90625\n",
            "Step: 268300, Loss: 400.06689453125, Accuracy: 0.88671875\n",
            "Step: 268400, Loss: 400.708740234375, Accuracy: 0.875\n",
            "Step: 268500, Loss: 404.7188720703125, Accuracy: 0.85546875\n",
            "Step: 268600, Loss: 404.00238037109375, Accuracy: 0.859375\n",
            "Step: 268700, Loss: 398.2879333496094, Accuracy: 0.88671875\n",
            "Step: 268800, Loss: 393.9572448730469, Accuracy: 0.90234375\n",
            "Step: 268900, Loss: 400.66058349609375, Accuracy: 0.88671875\n",
            "Step: 269000, Loss: 401.36956787109375, Accuracy: 0.8828125\n",
            "Step: 269100, Loss: 407.6537780761719, Accuracy: 0.84375\n",
            "Step: 269200, Loss: 398.8153076171875, Accuracy: 0.87890625\n",
            "Step: 269300, Loss: 393.99774169921875, Accuracy: 0.90234375\n",
            "Step: 269400, Loss: 406.596435546875, Accuracy: 0.8515625\n",
            "Step: 269500, Loss: 393.4372253417969, Accuracy: 0.91015625\n",
            "Step: 269600, Loss: 397.9884033203125, Accuracy: 0.890625\n",
            "Step: 269700, Loss: 414.67999267578125, Accuracy: 0.828125\n",
            "Step: 269800, Loss: 397.802978515625, Accuracy: 0.890625\n",
            "Step: 269900, Loss: 397.6047058105469, Accuracy: 0.88671875\n",
            "Step: 270000, Loss: 393.75335693359375, Accuracy: 0.90234375\n",
            "Step: 270100, Loss: 398.7775573730469, Accuracy: 0.88671875\n",
            "Step: 270200, Loss: 403.6978759765625, Accuracy: 0.8671875\n",
            "Step: 270300, Loss: 401.7147521972656, Accuracy: 0.87890625\n",
            "Step: 270400, Loss: 401.6824951171875, Accuracy: 0.875\n",
            "Step: 270500, Loss: 400.09033203125, Accuracy: 0.8828125\n",
            "Step: 270600, Loss: 399.86553955078125, Accuracy: 0.8828125\n",
            "Step: 270700, Loss: 397.3305969238281, Accuracy: 0.8984375\n",
            "Step: 270800, Loss: 405.1275329589844, Accuracy: 0.859375\n",
            "Step: 270900, Loss: 394.1372375488281, Accuracy: 0.89453125\n",
            "Step: 271000, Loss: 397.599853515625, Accuracy: 0.890625\n",
            "Step: 271100, Loss: 404.89501953125, Accuracy: 0.85546875\n",
            "Step: 271200, Loss: 397.0712585449219, Accuracy: 0.8984375\n",
            "Step: 271300, Loss: 405.4077453613281, Accuracy: 0.84765625\n",
            "Step: 271400, Loss: 395.98583984375, Accuracy: 0.8984375\n",
            "Step: 271500, Loss: 402.28265380859375, Accuracy: 0.8671875\n",
            "Step: 271600, Loss: 397.31671142578125, Accuracy: 0.8828125\n",
            "Step: 271700, Loss: 401.5621337890625, Accuracy: 0.87890625\n",
            "Step: 271800, Loss: 399.2291564941406, Accuracy: 0.890625\n",
            "Step: 271900, Loss: 396.80322265625, Accuracy: 0.890625\n",
            "Step: 272000, Loss: 392.4306640625, Accuracy: 0.92578125\n",
            "Step: 272100, Loss: 395.14056396484375, Accuracy: 0.89453125\n",
            "Step: 272200, Loss: 397.8149108886719, Accuracy: 0.88671875\n",
            "Step: 272300, Loss: 404.125244140625, Accuracy: 0.86328125\n",
            "Step: 272400, Loss: 399.08782958984375, Accuracy: 0.8828125\n",
            "Step: 272500, Loss: 399.72418212890625, Accuracy: 0.8828125\n",
            "Step: 272600, Loss: 406.74786376953125, Accuracy: 0.84765625\n",
            "Step: 272700, Loss: 399.9100341796875, Accuracy: 0.87890625\n",
            "Step: 272800, Loss: 403.197265625, Accuracy: 0.859375\n",
            "Step: 272900, Loss: 402.60382080078125, Accuracy: 0.87890625\n",
            "Step: 273000, Loss: 409.9056701660156, Accuracy: 0.8359375\n",
            "Step: 273100, Loss: 400.3509521484375, Accuracy: 0.875\n",
            "Step: 273200, Loss: 393.53167724609375, Accuracy: 0.91015625\n",
            "Step: 273300, Loss: 405.4321594238281, Accuracy: 0.85546875\n",
            "Step: 273400, Loss: 396.461669921875, Accuracy: 0.90234375\n",
            "Step: 273500, Loss: 401.3467102050781, Accuracy: 0.87890625\n",
            "Step: 273600, Loss: 390.3452453613281, Accuracy: 0.91796875\n",
            "Step: 273700, Loss: 400.4923095703125, Accuracy: 0.87890625\n",
            "Step: 273800, Loss: 406.7340393066406, Accuracy: 0.84375\n",
            "Step: 273900, Loss: 396.09100341796875, Accuracy: 0.90234375\n",
            "Step: 274000, Loss: 394.3658447265625, Accuracy: 0.890625\n",
            "Step: 274100, Loss: 402.50543212890625, Accuracy: 0.8671875\n",
            "Step: 274200, Loss: 401.6337890625, Accuracy: 0.875\n",
            "Step: 274300, Loss: 402.8550720214844, Accuracy: 0.8671875\n",
            "Step: 274400, Loss: 411.8883056640625, Accuracy: 0.8125\n",
            "Step: 274500, Loss: 403.29827880859375, Accuracy: 0.8671875\n",
            "Step: 274600, Loss: 402.2843933105469, Accuracy: 0.8671875\n",
            "Step: 274700, Loss: 399.35150146484375, Accuracy: 0.8828125\n",
            "Step: 274800, Loss: 408.36566162109375, Accuracy: 0.84375\n",
            "Step: 274900, Loss: 397.0347900390625, Accuracy: 0.890625\n",
            "Step: 275000, Loss: 398.2982177734375, Accuracy: 0.88671875\n",
            "Step: 275100, Loss: 397.5323791503906, Accuracy: 0.8828125\n",
            "Step: 275200, Loss: 397.84918212890625, Accuracy: 0.89453125\n",
            "Step: 275300, Loss: 396.7676086425781, Accuracy: 0.89453125\n",
            "Step: 275400, Loss: 399.3351745605469, Accuracy: 0.8828125\n",
            "Step: 275500, Loss: 398.0306396484375, Accuracy: 0.88671875\n",
            "Step: 275600, Loss: 399.3076171875, Accuracy: 0.8828125\n",
            "Step: 275700, Loss: 399.07391357421875, Accuracy: 0.8828125\n",
            "Step: 275800, Loss: 405.3271484375, Accuracy: 0.859375\n",
            "Step: 275900, Loss: 407.64471435546875, Accuracy: 0.84765625\n",
            "Step: 276000, Loss: 397.4822998046875, Accuracy: 0.8828125\n",
            "Step: 276100, Loss: 396.01959228515625, Accuracy: 0.890625\n",
            "Step: 276200, Loss: 399.28564453125, Accuracy: 0.88671875\n",
            "Step: 276300, Loss: 400.2813720703125, Accuracy: 0.87890625\n",
            "Step: 276400, Loss: 400.3066711425781, Accuracy: 0.87890625\n",
            "Step: 276500, Loss: 403.161376953125, Accuracy: 0.8828125\n",
            "Step: 276600, Loss: 403.808349609375, Accuracy: 0.8671875\n",
            "Step: 276700, Loss: 396.39862060546875, Accuracy: 0.90234375\n",
            "Step: 276800, Loss: 398.1527404785156, Accuracy: 0.8828125\n",
            "Step: 276900, Loss: 405.84173583984375, Accuracy: 0.8515625\n",
            "Step: 277000, Loss: 395.83001708984375, Accuracy: 0.8984375\n",
            "Step: 277100, Loss: 397.06787109375, Accuracy: 0.8828125\n",
            "Step: 277200, Loss: 403.2870178222656, Accuracy: 0.875\n",
            "Step: 277300, Loss: 406.63031005859375, Accuracy: 0.83984375\n",
            "Step: 277400, Loss: 396.15057373046875, Accuracy: 0.90234375\n",
            "Step: 277500, Loss: 402.5595703125, Accuracy: 0.859375\n",
            "Step: 277600, Loss: 398.12274169921875, Accuracy: 0.8984375\n",
            "Step: 277700, Loss: 398.91693115234375, Accuracy: 0.875\n",
            "Step: 277800, Loss: 399.1527099609375, Accuracy: 0.88671875\n",
            "Step: 277900, Loss: 400.5991516113281, Accuracy: 0.875\n",
            "Step: 278000, Loss: 401.9617614746094, Accuracy: 0.87890625\n",
            "Step: 278100, Loss: 398.5390625, Accuracy: 0.88671875\n",
            "Step: 278200, Loss: 398.8747863769531, Accuracy: 0.88671875\n",
            "Step: 278300, Loss: 404.4467468261719, Accuracy: 0.859375\n",
            "Step: 278400, Loss: 402.956787109375, Accuracy: 0.87890625\n",
            "Step: 278500, Loss: 401.4402770996094, Accuracy: 0.8671875\n",
            "Step: 278600, Loss: 400.7205810546875, Accuracy: 0.8828125\n",
            "Step: 278700, Loss: 401.0523681640625, Accuracy: 0.8828125\n",
            "Step: 278800, Loss: 396.7566833496094, Accuracy: 0.8984375\n",
            "Step: 278900, Loss: 399.2509460449219, Accuracy: 0.88671875\n",
            "Step: 279000, Loss: 401.9944763183594, Accuracy: 0.87890625\n",
            "Step: 279100, Loss: 393.96966552734375, Accuracy: 0.8984375\n",
            "Step: 279200, Loss: 405.36224365234375, Accuracy: 0.85546875\n",
            "Step: 279300, Loss: 401.7398681640625, Accuracy: 0.86328125\n",
            "Step: 279400, Loss: 398.71826171875, Accuracy: 0.890625\n",
            "Step: 279500, Loss: 396.7229919433594, Accuracy: 0.89453125\n",
            "Step: 279600, Loss: 397.10479736328125, Accuracy: 0.89453125\n",
            "Step: 279700, Loss: 399.8756408691406, Accuracy: 0.87890625\n",
            "Step: 279800, Loss: 401.6347961425781, Accuracy: 0.875\n",
            "Step: 279900, Loss: 396.998291015625, Accuracy: 0.89453125\n",
            "Step: 280000, Loss: 404.52423095703125, Accuracy: 0.8671875\n",
            "Step: 280100, Loss: 407.7446594238281, Accuracy: 0.84765625\n",
            "Step: 280200, Loss: 404.22991943359375, Accuracy: 0.859375\n",
            "Step: 280300, Loss: 395.4411926269531, Accuracy: 0.90625\n",
            "Step: 280400, Loss: 393.8906555175781, Accuracy: 0.90625\n",
            "Step: 280500, Loss: 401.27978515625, Accuracy: 0.875\n",
            "Step: 280600, Loss: 398.0118408203125, Accuracy: 0.8984375\n",
            "Step: 280700, Loss: 399.0233459472656, Accuracy: 0.88671875\n",
            "Step: 280800, Loss: 394.7458190917969, Accuracy: 0.90234375\n",
            "Step: 280900, Loss: 396.06781005859375, Accuracy: 0.90234375\n",
            "Step: 281000, Loss: 395.4920349121094, Accuracy: 0.90234375\n",
            "Step: 281100, Loss: 396.54833984375, Accuracy: 0.90234375\n",
            "Step: 281200, Loss: 402.36279296875, Accuracy: 0.87109375\n",
            "Step: 281300, Loss: 400.25457763671875, Accuracy: 0.875\n",
            "Step: 281400, Loss: 404.63214111328125, Accuracy: 0.859375\n",
            "Step: 281500, Loss: 395.26544189453125, Accuracy: 0.90234375\n",
            "Step: 281600, Loss: 396.22955322265625, Accuracy: 0.8984375\n",
            "Step: 281700, Loss: 395.71710205078125, Accuracy: 0.8984375\n",
            "Step: 281800, Loss: 399.6307373046875, Accuracy: 0.87109375\n",
            "Step: 281900, Loss: 405.6561279296875, Accuracy: 0.86328125\n",
            "Step: 282000, Loss: 394.3370361328125, Accuracy: 0.90625\n",
            "Step: 282100, Loss: 397.57257080078125, Accuracy: 0.89453125\n",
            "Step: 282200, Loss: 399.62811279296875, Accuracy: 0.87890625\n",
            "Step: 282300, Loss: 396.2193603515625, Accuracy: 0.90234375\n",
            "Step: 282400, Loss: 402.1901550292969, Accuracy: 0.87109375\n",
            "Step: 282500, Loss: 401.14154052734375, Accuracy: 0.8671875\n",
            "Step: 282600, Loss: 394.27313232421875, Accuracy: 0.91015625\n",
            "Step: 282700, Loss: 399.666015625, Accuracy: 0.87890625\n",
            "Step: 282800, Loss: 389.99932861328125, Accuracy: 0.9296875\n",
            "Step: 282900, Loss: 402.8191833496094, Accuracy: 0.87109375\n",
            "Step: 283000, Loss: 393.20977783203125, Accuracy: 0.91015625\n",
            "Step: 283100, Loss: 404.5479431152344, Accuracy: 0.86328125\n",
            "Step: 283200, Loss: 399.58856201171875, Accuracy: 0.87109375\n",
            "Step: 283300, Loss: 394.283203125, Accuracy: 0.90234375\n",
            "Step: 283400, Loss: 399.0557861328125, Accuracy: 0.88671875\n",
            "Step: 283500, Loss: 397.3028259277344, Accuracy: 0.89453125\n",
            "Step: 283600, Loss: 398.939208984375, Accuracy: 0.88671875\n",
            "Step: 283700, Loss: 406.8193664550781, Accuracy: 0.85546875\n",
            "Step: 283800, Loss: 399.4641418457031, Accuracy: 0.88671875\n",
            "Step: 283900, Loss: 398.56036376953125, Accuracy: 0.88671875\n",
            "Step: 284000, Loss: 402.2298583984375, Accuracy: 0.87109375\n",
            "Step: 284100, Loss: 407.3587341308594, Accuracy: 0.83984375\n",
            "Step: 284200, Loss: 403.70404052734375, Accuracy: 0.8671875\n",
            "Step: 284300, Loss: 395.4821472167969, Accuracy: 0.90234375\n",
            "Step: 284400, Loss: 404.494873046875, Accuracy: 0.8515625\n",
            "Step: 284500, Loss: 402.48944091796875, Accuracy: 0.8671875\n",
            "Step: 284600, Loss: 400.9386901855469, Accuracy: 0.8828125\n",
            "Step: 284700, Loss: 400.05389404296875, Accuracy: 0.87890625\n",
            "Step: 284800, Loss: 397.5774230957031, Accuracy: 0.890625\n",
            "Step: 284900, Loss: 400.013671875, Accuracy: 0.87890625\n",
            "Step: 285000, Loss: 401.90057373046875, Accuracy: 0.87109375\n",
            "Step: 285100, Loss: 405.7669982910156, Accuracy: 0.8515625\n",
            "Step: 285200, Loss: 409.7468566894531, Accuracy: 0.83984375\n",
            "Step: 285300, Loss: 400.5404052734375, Accuracy: 0.87109375\n",
            "Step: 285400, Loss: 401.49688720703125, Accuracy: 0.87109375\n",
            "Step: 285500, Loss: 396.858642578125, Accuracy: 0.89453125\n",
            "Step: 285600, Loss: 390.9437255859375, Accuracy: 0.9140625\n",
            "Step: 285700, Loss: 393.71026611328125, Accuracy: 0.90234375\n",
            "Step: 285800, Loss: 407.2861022949219, Accuracy: 0.84375\n",
            "Step: 285900, Loss: 402.8001708984375, Accuracy: 0.86328125\n",
            "Step: 286000, Loss: 396.6927795410156, Accuracy: 0.8984375\n",
            "Step: 286100, Loss: 403.8730163574219, Accuracy: 0.85546875\n",
            "Step: 286200, Loss: 405.03192138671875, Accuracy: 0.85546875\n",
            "Step: 286300, Loss: 402.4841613769531, Accuracy: 0.875\n",
            "Step: 286400, Loss: 400.87078857421875, Accuracy: 0.875\n",
            "Step: 286500, Loss: 400.7923583984375, Accuracy: 0.875\n",
            "Step: 286600, Loss: 399.5065612792969, Accuracy: 0.890625\n",
            "Step: 286700, Loss: 396.384033203125, Accuracy: 0.8984375\n",
            "Step: 286800, Loss: 404.0230712890625, Accuracy: 0.86328125\n",
            "Step: 286900, Loss: 408.9494934082031, Accuracy: 0.83984375\n",
            "Step: 287000, Loss: 404.190673828125, Accuracy: 0.86328125\n",
            "Step: 287100, Loss: 395.91046142578125, Accuracy: 0.90234375\n",
            "Step: 287200, Loss: 399.435302734375, Accuracy: 0.87890625\n",
            "Step: 287300, Loss: 408.2710876464844, Accuracy: 0.84765625\n",
            "Step: 287400, Loss: 407.33453369140625, Accuracy: 0.84765625\n",
            "Step: 287500, Loss: 402.73980712890625, Accuracy: 0.8671875\n",
            "Step: 287600, Loss: 402.4688720703125, Accuracy: 0.87109375\n",
            "Step: 287700, Loss: 393.4324951171875, Accuracy: 0.9140625\n",
            "Step: 287800, Loss: 401.4395446777344, Accuracy: 0.87890625\n",
            "Step: 287900, Loss: 394.9319152832031, Accuracy: 0.90234375\n",
            "Step: 288000, Loss: 400.9268798828125, Accuracy: 0.87890625\n",
            "Step: 288100, Loss: 404.15380859375, Accuracy: 0.86328125\n",
            "Step: 288200, Loss: 398.56683349609375, Accuracy: 0.87890625\n",
            "Step: 288300, Loss: 397.199462890625, Accuracy: 0.890625\n",
            "Step: 288400, Loss: 400.17279052734375, Accuracy: 0.8828125\n",
            "Step: 288500, Loss: 397.6253662109375, Accuracy: 0.89453125\n",
            "Step: 288600, Loss: 397.4512939453125, Accuracy: 0.89453125\n",
            "Step: 288700, Loss: 408.8423767089844, Accuracy: 0.8359375\n",
            "Step: 288800, Loss: 407.31536865234375, Accuracy: 0.84765625\n",
            "Step: 288900, Loss: 407.0867919921875, Accuracy: 0.85546875\n",
            "Step: 289000, Loss: 404.65582275390625, Accuracy: 0.8515625\n",
            "Step: 289100, Loss: 400.5059509277344, Accuracy: 0.8828125\n",
            "Step: 289200, Loss: 401.5129089355469, Accuracy: 0.875\n",
            "Step: 289300, Loss: 392.89813232421875, Accuracy: 0.921875\n",
            "Step: 289400, Loss: 404.42523193359375, Accuracy: 0.859375\n",
            "Step: 289500, Loss: 397.51922607421875, Accuracy: 0.88671875\n",
            "Step: 289600, Loss: 406.99078369140625, Accuracy: 0.8359375\n",
            "Step: 289700, Loss: 393.61322021484375, Accuracy: 0.921875\n",
            "Step: 289800, Loss: 399.61328125, Accuracy: 0.890625\n",
            "Step: 289900, Loss: 397.9700012207031, Accuracy: 0.88671875\n",
            "Step: 290000, Loss: 399.04718017578125, Accuracy: 0.88671875\n",
            "Step: 290100, Loss: 401.99822998046875, Accuracy: 0.875\n",
            "Step: 290200, Loss: 396.68853759765625, Accuracy: 0.8984375\n",
            "Step: 290300, Loss: 405.83001708984375, Accuracy: 0.859375\n",
            "Step: 290400, Loss: 400.2008056640625, Accuracy: 0.87890625\n",
            "Step: 290500, Loss: 405.3968505859375, Accuracy: 0.85546875\n",
            "Step: 290600, Loss: 396.909912109375, Accuracy: 0.89453125\n",
            "Step: 290700, Loss: 396.4631042480469, Accuracy: 0.8984375\n",
            "Step: 290800, Loss: 398.49530029296875, Accuracy: 0.87890625\n",
            "Step: 290900, Loss: 399.0496826171875, Accuracy: 0.8828125\n",
            "Step: 291000, Loss: 399.4293518066406, Accuracy: 0.87890625\n",
            "Step: 291100, Loss: 398.1982116699219, Accuracy: 0.8828125\n",
            "Step: 291200, Loss: 404.8441162109375, Accuracy: 0.859375\n",
            "Step: 291300, Loss: 405.0196228027344, Accuracy: 0.859375\n",
            "Step: 291400, Loss: 404.4115905761719, Accuracy: 0.86328125\n",
            "Step: 291500, Loss: 404.0834655761719, Accuracy: 0.859375\n",
            "Step: 291600, Loss: 400.48736572265625, Accuracy: 0.8828125\n",
            "Step: 291700, Loss: 404.84552001953125, Accuracy: 0.85546875\n",
            "Step: 291800, Loss: 406.8528747558594, Accuracy: 0.84375\n",
            "Step: 291900, Loss: 401.3570251464844, Accuracy: 0.875\n",
            "Step: 292000, Loss: 404.84503173828125, Accuracy: 0.85546875\n",
            "Step: 292100, Loss: 395.38690185546875, Accuracy: 0.8984375\n",
            "Step: 292200, Loss: 404.6804504394531, Accuracy: 0.84765625\n",
            "Step: 292300, Loss: 406.57720947265625, Accuracy: 0.85546875\n",
            "Step: 292400, Loss: 407.1643371582031, Accuracy: 0.8515625\n",
            "Step: 292500, Loss: 401.657470703125, Accuracy: 0.87890625\n",
            "Step: 292600, Loss: 404.6923828125, Accuracy: 0.859375\n",
            "Step: 292700, Loss: 395.82379150390625, Accuracy: 0.90234375\n",
            "Step: 292800, Loss: 399.604736328125, Accuracy: 0.8828125\n",
            "Step: 292900, Loss: 398.2416076660156, Accuracy: 0.890625\n",
            "Step: 293000, Loss: 404.8885498046875, Accuracy: 0.84765625\n",
            "Step: 293100, Loss: 403.5391845703125, Accuracy: 0.86328125\n",
            "Step: 293200, Loss: 404.0793762207031, Accuracy: 0.85546875\n",
            "Step: 293300, Loss: 405.2179260253906, Accuracy: 0.85546875\n",
            "Step: 293400, Loss: 404.05914306640625, Accuracy: 0.86328125\n",
            "Step: 293500, Loss: 408.1732177734375, Accuracy: 0.84375\n",
            "Step: 293600, Loss: 391.601806640625, Accuracy: 0.921875\n",
            "Step: 293700, Loss: 403.37713623046875, Accuracy: 0.86328125\n",
            "Step: 293800, Loss: 396.6183166503906, Accuracy: 0.890625\n",
            "Step: 293900, Loss: 392.2740478515625, Accuracy: 0.9140625\n",
            "Step: 294000, Loss: 405.10906982421875, Accuracy: 0.86328125\n",
            "Step: 294100, Loss: 404.87701416015625, Accuracy: 0.8515625\n",
            "Step: 294200, Loss: 398.657470703125, Accuracy: 0.8828125\n",
            "Step: 294300, Loss: 396.2572021484375, Accuracy: 0.90625\n",
            "Step: 294400, Loss: 401.5843505859375, Accuracy: 0.875\n",
            "Step: 294500, Loss: 393.5909423828125, Accuracy: 0.90625\n",
            "Step: 294600, Loss: 395.15728759765625, Accuracy: 0.90234375\n",
            "Step: 294700, Loss: 397.4858093261719, Accuracy: 0.88671875\n",
            "Step: 294800, Loss: 402.3428039550781, Accuracy: 0.86328125\n",
            "Step: 294900, Loss: 395.4298400878906, Accuracy: 0.8984375\n",
            "Step: 295000, Loss: 400.2267761230469, Accuracy: 0.8828125\n",
            "Step: 295100, Loss: 400.3565673828125, Accuracy: 0.8828125\n",
            "Step: 295200, Loss: 398.2272033691406, Accuracy: 0.89453125\n",
            "Step: 295300, Loss: 399.60003662109375, Accuracy: 0.87890625\n",
            "Step: 295400, Loss: 389.56390380859375, Accuracy: 0.92578125\n",
            "Step: 295500, Loss: 399.3599548339844, Accuracy: 0.875\n",
            "Step: 295600, Loss: 397.9092712402344, Accuracy: 0.88671875\n",
            "Step: 295700, Loss: 392.0928955078125, Accuracy: 0.91796875\n",
            "Step: 295800, Loss: 403.63330078125, Accuracy: 0.8515625\n",
            "Step: 295900, Loss: 402.3690185546875, Accuracy: 0.86328125\n",
            "Step: 296000, Loss: 396.25701904296875, Accuracy: 0.890625\n",
            "Step: 296100, Loss: 395.96337890625, Accuracy: 0.89453125\n",
            "Step: 296200, Loss: 393.86468505859375, Accuracy: 0.9140625\n",
            "Step: 296300, Loss: 391.928955078125, Accuracy: 0.9140625\n",
            "Step: 296400, Loss: 402.4642028808594, Accuracy: 0.87109375\n",
            "Step: 296500, Loss: 411.4888610839844, Accuracy: 0.83984375\n",
            "Step: 296600, Loss: 399.2414855957031, Accuracy: 0.890625\n",
            "Step: 296700, Loss: 398.3919677734375, Accuracy: 0.87890625\n",
            "Step: 296800, Loss: 397.4299621582031, Accuracy: 0.8984375\n",
            "Step: 296900, Loss: 395.77154541015625, Accuracy: 0.8984375\n",
            "Step: 297000, Loss: 406.5396728515625, Accuracy: 0.8515625\n",
            "Step: 297100, Loss: 403.77740478515625, Accuracy: 0.859375\n",
            "Step: 297200, Loss: 402.715087890625, Accuracy: 0.859375\n",
            "Step: 297300, Loss: 396.80279541015625, Accuracy: 0.89453125\n",
            "Step: 297400, Loss: 401.3450927734375, Accuracy: 0.875\n",
            "Step: 297500, Loss: 402.1240234375, Accuracy: 0.8671875\n",
            "Step: 297600, Loss: 411.5330810546875, Accuracy: 0.8359375\n",
            "Step: 297700, Loss: 397.7837219238281, Accuracy: 0.87890625\n",
            "Step: 297800, Loss: 399.510986328125, Accuracy: 0.875\n",
            "Step: 297900, Loss: 406.8846740722656, Accuracy: 0.8515625\n",
            "Step: 298000, Loss: 398.32537841796875, Accuracy: 0.88671875\n",
            "Step: 298100, Loss: 402.2872314453125, Accuracy: 0.8671875\n",
            "Step: 298200, Loss: 393.9887390136719, Accuracy: 0.90625\n",
            "Step: 298300, Loss: 407.11651611328125, Accuracy: 0.84375\n",
            "Step: 298400, Loss: 397.9854431152344, Accuracy: 0.890625\n",
            "Step: 298500, Loss: 393.1377868652344, Accuracy: 0.91015625\n",
            "Step: 298600, Loss: 397.0386962890625, Accuracy: 0.89453125\n",
            "Step: 298700, Loss: 393.768798828125, Accuracy: 0.90234375\n",
            "Step: 298800, Loss: 399.60888671875, Accuracy: 0.88671875\n",
            "Step: 298900, Loss: 400.2829284667969, Accuracy: 0.8671875\n",
            "Step: 299000, Loss: 401.8631286621094, Accuracy: 0.8671875\n",
            "Step: 299100, Loss: 403.0430908203125, Accuracy: 0.8671875\n",
            "Step: 299200, Loss: 402.17242431640625, Accuracy: 0.87109375\n",
            "Step: 299300, Loss: 399.70947265625, Accuracy: 0.8828125\n",
            "Step: 299400, Loss: 402.4956970214844, Accuracy: 0.875\n",
            "Step: 299500, Loss: 408.12213134765625, Accuracy: 0.84765625\n",
            "Step: 299600, Loss: 398.12750244140625, Accuracy: 0.87890625\n",
            "Step: 299700, Loss: 402.048095703125, Accuracy: 0.87109375\n",
            "Step: 299800, Loss: 406.39703369140625, Accuracy: 0.85546875\n",
            "Step: 299900, Loss: 399.2353210449219, Accuracy: 0.8828125\n",
            "Step: 300000, Loss: 400.249267578125, Accuracy: 0.88671875\n",
            "Step: 300100, Loss: 401.07208251953125, Accuracy: 0.88671875\n",
            "Step: 300200, Loss: 406.8662109375, Accuracy: 0.84375\n",
            "Step: 300300, Loss: 395.588134765625, Accuracy: 0.89453125\n",
            "Step: 300400, Loss: 402.10369873046875, Accuracy: 0.87890625\n",
            "Step: 300500, Loss: 403.82421875, Accuracy: 0.86328125\n",
            "Step: 300600, Loss: 401.07208251953125, Accuracy: 0.8828125\n",
            "Step: 300700, Loss: 404.54754638671875, Accuracy: 0.85546875\n",
            "Step: 300800, Loss: 401.27197265625, Accuracy: 0.87109375\n",
            "Step: 300900, Loss: 400.2374267578125, Accuracy: 0.875\n",
            "Step: 301000, Loss: 403.81640625, Accuracy: 0.8671875\n",
            "Step: 301100, Loss: 394.21600341796875, Accuracy: 0.9140625\n",
            "Step: 301200, Loss: 402.06732177734375, Accuracy: 0.86328125\n",
            "Step: 301300, Loss: 401.57177734375, Accuracy: 0.87109375\n",
            "Step: 301400, Loss: 400.3868713378906, Accuracy: 0.87890625\n",
            "Step: 301500, Loss: 390.15997314453125, Accuracy: 0.921875\n",
            "Step: 301600, Loss: 396.4786376953125, Accuracy: 0.8984375\n",
            "Step: 301700, Loss: 401.18353271484375, Accuracy: 0.8671875\n",
            "Step: 301800, Loss: 410.4417419433594, Accuracy: 0.8203125\n",
            "Step: 301900, Loss: 402.2287902832031, Accuracy: 0.87109375\n",
            "Step: 302000, Loss: 412.084716796875, Accuracy: 0.83203125\n",
            "Step: 302100, Loss: 413.8856201171875, Accuracy: 0.8125\n",
            "Step: 302200, Loss: 405.12744140625, Accuracy: 0.85546875\n",
            "Step: 302300, Loss: 403.2876892089844, Accuracy: 0.86328125\n",
            "Step: 302400, Loss: 400.782470703125, Accuracy: 0.8828125\n",
            "Step: 302500, Loss: 407.5850830078125, Accuracy: 0.84375\n",
            "Step: 302600, Loss: 405.8797607421875, Accuracy: 0.8515625\n",
            "Step: 302700, Loss: 397.3463134765625, Accuracy: 0.8984375\n",
            "Step: 302800, Loss: 396.8369445800781, Accuracy: 0.89453125\n",
            "Step: 302900, Loss: 401.5433349609375, Accuracy: 0.86328125\n",
            "Step: 303000, Loss: 401.1377868652344, Accuracy: 0.8671875\n",
            "Step: 303100, Loss: 397.93609619140625, Accuracy: 0.89453125\n",
            "Step: 303200, Loss: 400.40032958984375, Accuracy: 0.890625\n",
            "Step: 303300, Loss: 393.692138671875, Accuracy: 0.90625\n",
            "Step: 303400, Loss: 401.7827453613281, Accuracy: 0.8671875\n",
            "Step: 303500, Loss: 396.79595947265625, Accuracy: 0.89453125\n",
            "Step: 303600, Loss: 397.35870361328125, Accuracy: 0.8984375\n",
            "Step: 303700, Loss: 406.75103759765625, Accuracy: 0.84375\n",
            "Step: 303800, Loss: 403.3666687011719, Accuracy: 0.86328125\n",
            "Step: 303900, Loss: 406.92193603515625, Accuracy: 0.84765625\n",
            "Step: 304000, Loss: 400.64501953125, Accuracy: 0.8671875\n",
            "Step: 304100, Loss: 400.4914855957031, Accuracy: 0.87109375\n",
            "Step: 304200, Loss: 400.6871337890625, Accuracy: 0.8828125\n",
            "Step: 304300, Loss: 400.27862548828125, Accuracy: 0.87890625\n",
            "Step: 304400, Loss: 398.2740173339844, Accuracy: 0.90234375\n",
            "Step: 304500, Loss: 398.6971130371094, Accuracy: 0.8828125\n",
            "Step: 304600, Loss: 401.6559143066406, Accuracy: 0.86328125\n",
            "Step: 304700, Loss: 398.5391540527344, Accuracy: 0.8828125\n",
            "Step: 304800, Loss: 397.4093322753906, Accuracy: 0.89453125\n",
            "Step: 304900, Loss: 395.053466796875, Accuracy: 0.8984375\n",
            "Step: 305000, Loss: 393.591552734375, Accuracy: 0.91796875\n",
            "Step: 305100, Loss: 405.46728515625, Accuracy: 0.87109375\n",
            "Step: 305200, Loss: 407.1897888183594, Accuracy: 0.8515625\n",
            "Step: 305300, Loss: 402.16253662109375, Accuracy: 0.875\n",
            "Step: 305400, Loss: 400.5980224609375, Accuracy: 0.87890625\n",
            "Step: 305500, Loss: 401.4752502441406, Accuracy: 0.86328125\n",
            "Step: 305600, Loss: 396.4064636230469, Accuracy: 0.90234375\n",
            "Step: 305700, Loss: 398.20355224609375, Accuracy: 0.890625\n",
            "Step: 305800, Loss: 394.0122985839844, Accuracy: 0.90625\n",
            "Step: 305900, Loss: 404.9547119140625, Accuracy: 0.85546875\n",
            "Step: 306000, Loss: 400.3751220703125, Accuracy: 0.8828125\n",
            "Step: 306100, Loss: 402.1859130859375, Accuracy: 0.86328125\n",
            "Step: 306200, Loss: 396.5981140136719, Accuracy: 0.89453125\n",
            "Step: 306300, Loss: 411.5120849609375, Accuracy: 0.83203125\n",
            "Step: 306400, Loss: 403.3869934082031, Accuracy: 0.8671875\n",
            "Step: 306500, Loss: 396.09979248046875, Accuracy: 0.90234375\n",
            "Step: 306600, Loss: 398.1463623046875, Accuracy: 0.8828125\n",
            "Step: 306700, Loss: 401.82647705078125, Accuracy: 0.87109375\n",
            "Step: 306800, Loss: 401.4521484375, Accuracy: 0.87890625\n",
            "Step: 306900, Loss: 410.0068054199219, Accuracy: 0.83203125\n",
            "Step: 307000, Loss: 405.4710693359375, Accuracy: 0.84375\n",
            "Step: 307100, Loss: 405.6634521484375, Accuracy: 0.84375\n",
            "Step: 307200, Loss: 396.78021240234375, Accuracy: 0.89453125\n",
            "Step: 307300, Loss: 399.994384765625, Accuracy: 0.87890625\n",
            "Step: 307400, Loss: 398.72332763671875, Accuracy: 0.88671875\n",
            "Step: 307500, Loss: 400.39447021484375, Accuracy: 0.87890625\n",
            "Step: 307600, Loss: 409.44970703125, Accuracy: 0.83203125\n",
            "Step: 307700, Loss: 396.7020568847656, Accuracy: 0.89453125\n",
            "Step: 307800, Loss: 390.2587890625, Accuracy: 0.92578125\n",
            "Step: 307900, Loss: 400.9854736328125, Accuracy: 0.8828125\n",
            "Step: 308000, Loss: 411.19140625, Accuracy: 0.83203125\n",
            "Step: 308100, Loss: 403.24664306640625, Accuracy: 0.859375\n",
            "Step: 308200, Loss: 398.2706604003906, Accuracy: 0.88671875\n",
            "Step: 308300, Loss: 403.34649658203125, Accuracy: 0.86328125\n",
            "Step: 308400, Loss: 399.83648681640625, Accuracy: 0.87109375\n",
            "Step: 308500, Loss: 404.00616455078125, Accuracy: 0.859375\n",
            "Step: 308600, Loss: 404.26751708984375, Accuracy: 0.859375\n",
            "Step: 308700, Loss: 396.55487060546875, Accuracy: 0.89453125\n",
            "Step: 308800, Loss: 400.7108154296875, Accuracy: 0.87109375\n",
            "Step: 308900, Loss: 396.77923583984375, Accuracy: 0.8828125\n",
            "Step: 309000, Loss: 397.8261413574219, Accuracy: 0.8984375\n",
            "Step: 309100, Loss: 395.71551513671875, Accuracy: 0.8984375\n",
            "Step: 309200, Loss: 395.66192626953125, Accuracy: 0.90234375\n",
            "Step: 309300, Loss: 410.79718017578125, Accuracy: 0.828125\n",
            "Step: 309400, Loss: 404.8091125488281, Accuracy: 0.875\n",
            "Step: 309500, Loss: 396.6295166015625, Accuracy: 0.89453125\n",
            "Step: 309600, Loss: 401.9229736328125, Accuracy: 0.86328125\n",
            "Step: 309700, Loss: 399.71063232421875, Accuracy: 0.8828125\n",
            "Step: 309800, Loss: 405.4678955078125, Accuracy: 0.85546875\n",
            "Step: 309900, Loss: 397.3736877441406, Accuracy: 0.8828125\n",
            "Step: 310000, Loss: 398.5894470214844, Accuracy: 0.890625\n",
            "Step: 310100, Loss: 407.1097412109375, Accuracy: 0.8359375\n",
            "Step: 310200, Loss: 406.2357177734375, Accuracy: 0.85546875\n",
            "Step: 310300, Loss: 401.9979248046875, Accuracy: 0.875\n",
            "Step: 310400, Loss: 397.8865661621094, Accuracy: 0.89453125\n",
            "Step: 310500, Loss: 393.0936279296875, Accuracy: 0.90625\n",
            "Step: 310600, Loss: 398.0613708496094, Accuracy: 0.890625\n",
            "Step: 310700, Loss: 396.482666015625, Accuracy: 0.8984375\n",
            "Step: 310800, Loss: 403.74481201171875, Accuracy: 0.87109375\n",
            "Step: 310900, Loss: 400.8406982421875, Accuracy: 0.8671875\n",
            "Step: 311000, Loss: 403.71875, Accuracy: 0.8671875\n",
            "Step: 311100, Loss: 401.9454345703125, Accuracy: 0.86328125\n",
            "Step: 311200, Loss: 402.50848388671875, Accuracy: 0.8671875\n",
            "Step: 311300, Loss: 399.21307373046875, Accuracy: 0.8828125\n",
            "Step: 311400, Loss: 401.63616943359375, Accuracy: 0.87109375\n",
            "Step: 311500, Loss: 405.65576171875, Accuracy: 0.8515625\n",
            "Step: 311600, Loss: 401.2661437988281, Accuracy: 0.8828125\n",
            "Step: 311700, Loss: 392.4163513183594, Accuracy: 0.921875\n",
            "Step: 311800, Loss: 393.07550048828125, Accuracy: 0.9140625\n",
            "Step: 311900, Loss: 404.74932861328125, Accuracy: 0.86328125\n",
            "Step: 312000, Loss: 398.42083740234375, Accuracy: 0.8828125\n",
            "Step: 312100, Loss: 402.2083740234375, Accuracy: 0.86328125\n",
            "Step: 312200, Loss: 400.50372314453125, Accuracy: 0.8828125\n",
            "Step: 312300, Loss: 399.8644714355469, Accuracy: 0.8828125\n",
            "Step: 312400, Loss: 398.018798828125, Accuracy: 0.88671875\n",
            "Step: 312500, Loss: 392.7016296386719, Accuracy: 0.91796875\n",
            "Step: 312600, Loss: 413.64178466796875, Accuracy: 0.81640625\n",
            "Step: 312700, Loss: 402.2112731933594, Accuracy: 0.8671875\n",
            "Step: 312800, Loss: 401.6157531738281, Accuracy: 0.875\n",
            "Step: 312900, Loss: 403.6488037109375, Accuracy: 0.86328125\n",
            "Step: 313000, Loss: 401.1432189941406, Accuracy: 0.875\n",
            "Step: 313100, Loss: 406.11395263671875, Accuracy: 0.8515625\n",
            "Step: 313200, Loss: 403.53363037109375, Accuracy: 0.8671875\n",
            "Step: 313300, Loss: 402.6488037109375, Accuracy: 0.8671875\n",
            "Step: 313400, Loss: 394.1820068359375, Accuracy: 0.9140625\n",
            "Step: 313500, Loss: 401.14288330078125, Accuracy: 0.875\n",
            "Step: 313600, Loss: 405.5050964355469, Accuracy: 0.859375\n",
            "Step: 313700, Loss: 402.19482421875, Accuracy: 0.87109375\n",
            "Step: 313800, Loss: 401.91827392578125, Accuracy: 0.87890625\n",
            "Step: 313900, Loss: 397.84796142578125, Accuracy: 0.88671875\n",
            "Step: 314000, Loss: 404.6669921875, Accuracy: 0.859375\n",
            "Step: 314100, Loss: 405.6938171386719, Accuracy: 0.84765625\n",
            "Step: 314200, Loss: 393.03863525390625, Accuracy: 0.91015625\n",
            "Step: 314300, Loss: 399.59942626953125, Accuracy: 0.890625\n",
            "Step: 314400, Loss: 402.31988525390625, Accuracy: 0.859375\n",
            "Step: 314500, Loss: 394.1294250488281, Accuracy: 0.8984375\n",
            "Step: 314600, Loss: 404.9550476074219, Accuracy: 0.85546875\n",
            "Step: 314700, Loss: 401.0679931640625, Accuracy: 0.875\n",
            "Step: 314800, Loss: 405.63299560546875, Accuracy: 0.859375\n",
            "Step: 314900, Loss: 402.38543701171875, Accuracy: 0.8671875\n",
            "Step: 315000, Loss: 397.11553955078125, Accuracy: 0.89453125\n",
            "Step: 315100, Loss: 400.91278076171875, Accuracy: 0.86328125\n",
            "Step: 315200, Loss: 399.056884765625, Accuracy: 0.8828125\n",
            "Step: 315300, Loss: 400.03240966796875, Accuracy: 0.87890625\n",
            "Step: 315400, Loss: 401.212646484375, Accuracy: 0.87109375\n",
            "Step: 315500, Loss: 402.4485778808594, Accuracy: 0.875\n",
            "Step: 315600, Loss: 404.38897705078125, Accuracy: 0.86328125\n",
            "Step: 315700, Loss: 397.48858642578125, Accuracy: 0.8984375\n",
            "Step: 315800, Loss: 403.4771728515625, Accuracy: 0.8671875\n",
            "Step: 315900, Loss: 403.658935546875, Accuracy: 0.8671875\n",
            "Step: 316000, Loss: 396.06304931640625, Accuracy: 0.8984375\n",
            "Step: 316100, Loss: 405.77337646484375, Accuracy: 0.8515625\n",
            "Step: 316200, Loss: 402.4833068847656, Accuracy: 0.87109375\n",
            "Step: 316300, Loss: 396.7419738769531, Accuracy: 0.8984375\n",
            "Step: 316400, Loss: 402.1968994140625, Accuracy: 0.87109375\n",
            "Step: 316500, Loss: 397.79534912109375, Accuracy: 0.8984375\n",
            "Step: 316600, Loss: 396.79644775390625, Accuracy: 0.89453125\n",
            "Step: 316700, Loss: 392.7550964355469, Accuracy: 0.9140625\n",
            "Step: 316800, Loss: 400.12872314453125, Accuracy: 0.875\n",
            "Step: 316900, Loss: 404.7373046875, Accuracy: 0.859375\n",
            "Step: 317000, Loss: 397.580322265625, Accuracy: 0.89453125\n",
            "Step: 317100, Loss: 399.72607421875, Accuracy: 0.875\n",
            "Step: 317200, Loss: 403.4870300292969, Accuracy: 0.86328125\n",
            "Step: 317300, Loss: 403.5179138183594, Accuracy: 0.859375\n",
            "Step: 317400, Loss: 395.5985412597656, Accuracy: 0.90625\n",
            "Step: 317500, Loss: 400.09844970703125, Accuracy: 0.8828125\n",
            "Step: 317600, Loss: 401.87890625, Accuracy: 0.87109375\n",
            "Step: 317700, Loss: 401.27471923828125, Accuracy: 0.8828125\n",
            "Step: 317800, Loss: 404.91796875, Accuracy: 0.859375\n",
            "Step: 317900, Loss: 403.4376220703125, Accuracy: 0.8671875\n",
            "Step: 318000, Loss: 400.486083984375, Accuracy: 0.8828125\n",
            "Step: 318100, Loss: 395.76116943359375, Accuracy: 0.8984375\n",
            "Step: 318200, Loss: 391.4974365234375, Accuracy: 0.9140625\n",
            "Step: 318300, Loss: 405.51849365234375, Accuracy: 0.8515625\n",
            "Step: 318400, Loss: 407.166015625, Accuracy: 0.85546875\n",
            "Step: 318500, Loss: 402.5183410644531, Accuracy: 0.87109375\n",
            "Step: 318600, Loss: 401.7961120605469, Accuracy: 0.875\n",
            "Step: 318700, Loss: 393.71441650390625, Accuracy: 0.91015625\n",
            "Step: 318800, Loss: 396.7843933105469, Accuracy: 0.8984375\n",
            "Step: 318900, Loss: 401.88958740234375, Accuracy: 0.875\n",
            "Step: 319000, Loss: 397.45172119140625, Accuracy: 0.89453125\n",
            "Step: 319100, Loss: 396.6184387207031, Accuracy: 0.89453125\n",
            "Step: 319200, Loss: 404.1845397949219, Accuracy: 0.859375\n",
            "Step: 319300, Loss: 405.189208984375, Accuracy: 0.85546875\n",
            "Step: 319400, Loss: 403.404052734375, Accuracy: 0.859375\n",
            "Step: 319500, Loss: 400.9030456542969, Accuracy: 0.875\n",
            "Step: 319600, Loss: 400.48138427734375, Accuracy: 0.87890625\n",
            "Step: 319700, Loss: 397.18646240234375, Accuracy: 0.89453125\n",
            "Step: 319800, Loss: 408.80230712890625, Accuracy: 0.84765625\n",
            "Step: 319900, Loss: 405.8868103027344, Accuracy: 0.86328125\n",
            "Step: 320000, Loss: 405.51385498046875, Accuracy: 0.85546875\n",
            "Step: 320100, Loss: 406.14111328125, Accuracy: 0.86328125\n",
            "Step: 320200, Loss: 397.544677734375, Accuracy: 0.89453125\n",
            "Step: 320300, Loss: 412.837646484375, Accuracy: 0.83203125\n",
            "Step: 320400, Loss: 399.0870056152344, Accuracy: 0.8828125\n",
            "Step: 320500, Loss: 396.76068115234375, Accuracy: 0.89453125\n",
            "Step: 320600, Loss: 401.68511962890625, Accuracy: 0.8828125\n",
            "Step: 320700, Loss: 407.3778076171875, Accuracy: 0.83203125\n",
            "Step: 320800, Loss: 402.693359375, Accuracy: 0.8671875\n",
            "Step: 320900, Loss: 400.43963623046875, Accuracy: 0.88671875\n",
            "Step: 321000, Loss: 401.2165832519531, Accuracy: 0.86328125\n",
            "Step: 321100, Loss: 397.626953125, Accuracy: 0.8828125\n",
            "Step: 321200, Loss: 406.2840576171875, Accuracy: 0.85546875\n",
            "Step: 321300, Loss: 399.715576171875, Accuracy: 0.88671875\n",
            "Step: 321400, Loss: 408.861083984375, Accuracy: 0.83984375\n",
            "Step: 321500, Loss: 402.86077880859375, Accuracy: 0.8671875\n",
            "Step: 321600, Loss: 407.1638488769531, Accuracy: 0.8515625\n",
            "Step: 321700, Loss: 402.02642822265625, Accuracy: 0.87890625\n",
            "Step: 321800, Loss: 398.8941650390625, Accuracy: 0.88671875\n",
            "Step: 321900, Loss: 401.7817687988281, Accuracy: 0.8671875\n",
            "Step: 322000, Loss: 404.62213134765625, Accuracy: 0.8515625\n",
            "Step: 322100, Loss: 408.07232666015625, Accuracy: 0.8515625\n",
            "Step: 322200, Loss: 399.44561767578125, Accuracy: 0.88671875\n",
            "Step: 322300, Loss: 405.2095642089844, Accuracy: 0.84375\n",
            "Step: 322400, Loss: 400.1834716796875, Accuracy: 0.87890625\n",
            "Step: 322500, Loss: 394.22601318359375, Accuracy: 0.90625\n",
            "Step: 322600, Loss: 397.03070068359375, Accuracy: 0.89453125\n",
            "Step: 322700, Loss: 397.34381103515625, Accuracy: 0.890625\n",
            "Step: 322800, Loss: 397.1561279296875, Accuracy: 0.890625\n",
            "Step: 322900, Loss: 399.0777587890625, Accuracy: 0.88671875\n",
            "Step: 323000, Loss: 401.87445068359375, Accuracy: 0.86328125\n",
            "Step: 323100, Loss: 398.0975341796875, Accuracy: 0.8984375\n",
            "Step: 323200, Loss: 396.8116455078125, Accuracy: 0.89453125\n",
            "Step: 323300, Loss: 402.3309326171875, Accuracy: 0.8671875\n",
            "Step: 323400, Loss: 397.06915283203125, Accuracy: 0.8984375\n",
            "Step: 323500, Loss: 395.71429443359375, Accuracy: 0.90625\n",
            "Step: 323600, Loss: 398.1416015625, Accuracy: 0.87890625\n",
            "Step: 323700, Loss: 397.5728454589844, Accuracy: 0.88671875\n",
            "Step: 323800, Loss: 398.55706787109375, Accuracy: 0.8828125\n",
            "Step: 323900, Loss: 399.6426696777344, Accuracy: 0.88671875\n",
            "Step: 324000, Loss: 397.89984130859375, Accuracy: 0.8828125\n",
            "Step: 324100, Loss: 402.3664245605469, Accuracy: 0.8828125\n",
            "Step: 324200, Loss: 396.08343505859375, Accuracy: 0.89453125\n",
            "Step: 324300, Loss: 409.1194763183594, Accuracy: 0.8359375\n",
            "Step: 324400, Loss: 397.6650085449219, Accuracy: 0.8984375\n",
            "Step: 324500, Loss: 396.8958740234375, Accuracy: 0.8984375\n",
            "Step: 324600, Loss: 399.9118957519531, Accuracy: 0.88671875\n",
            "Step: 324700, Loss: 411.6845703125, Accuracy: 0.8359375\n",
            "Step: 324800, Loss: 406.87115478515625, Accuracy: 0.85546875\n",
            "Step: 324900, Loss: 398.1178283691406, Accuracy: 0.88671875\n",
            "Step: 325000, Loss: 394.85797119140625, Accuracy: 0.90234375\n",
            "Step: 325100, Loss: 406.35577392578125, Accuracy: 0.8359375\n",
            "Step: 325200, Loss: 396.8870849609375, Accuracy: 0.8828125\n",
            "Step: 325300, Loss: 400.0958251953125, Accuracy: 0.875\n",
            "Step: 325400, Loss: 394.15985107421875, Accuracy: 0.90625\n",
            "Step: 325500, Loss: 399.94000244140625, Accuracy: 0.8671875\n",
            "Step: 325600, Loss: 396.1248779296875, Accuracy: 0.8984375\n",
            "Step: 325700, Loss: 398.3275146484375, Accuracy: 0.89453125\n",
            "Step: 325800, Loss: 402.9853210449219, Accuracy: 0.8671875\n",
            "Step: 325900, Loss: 397.6722412109375, Accuracy: 0.90234375\n",
            "Step: 326000, Loss: 403.2493896484375, Accuracy: 0.87109375\n",
            "Step: 326100, Loss: 403.9170227050781, Accuracy: 0.8515625\n",
            "Step: 326200, Loss: 399.26483154296875, Accuracy: 0.88671875\n",
            "Step: 326300, Loss: 399.9981689453125, Accuracy: 0.87890625\n",
            "Step: 326400, Loss: 397.84027099609375, Accuracy: 0.89453125\n",
            "Step: 326500, Loss: 399.6817626953125, Accuracy: 0.890625\n",
            "Step: 326600, Loss: 410.9266357421875, Accuracy: 0.82421875\n",
            "Step: 326700, Loss: 394.079345703125, Accuracy: 0.89453125\n",
            "Step: 326800, Loss: 395.2281494140625, Accuracy: 0.89453125\n",
            "Step: 326900, Loss: 395.9351806640625, Accuracy: 0.8984375\n",
            "Step: 327000, Loss: 394.70721435546875, Accuracy: 0.90625\n",
            "Step: 327100, Loss: 402.812744140625, Accuracy: 0.859375\n",
            "Step: 327200, Loss: 384.43280029296875, Accuracy: 0.9453125\n",
            "Step: 327300, Loss: 406.301025390625, Accuracy: 0.8515625\n",
            "Step: 327400, Loss: 394.63604736328125, Accuracy: 0.90234375\n",
            "Step: 327500, Loss: 399.59588623046875, Accuracy: 0.87890625\n",
            "Step: 327600, Loss: 398.3459777832031, Accuracy: 0.88671875\n",
            "Step: 327700, Loss: 401.3057861328125, Accuracy: 0.87890625\n",
            "Step: 327800, Loss: 403.7305908203125, Accuracy: 0.8671875\n",
            "Step: 327900, Loss: 392.9549560546875, Accuracy: 0.91015625\n",
            "Step: 328000, Loss: 396.5192565917969, Accuracy: 0.90234375\n",
            "Step: 328100, Loss: 397.1243896484375, Accuracy: 0.890625\n",
            "Step: 328200, Loss: 404.7775573730469, Accuracy: 0.8515625\n",
            "Step: 328300, Loss: 404.0397033691406, Accuracy: 0.8671875\n",
            "Step: 328400, Loss: 405.2700500488281, Accuracy: 0.85546875\n",
            "Step: 328500, Loss: 399.2810974121094, Accuracy: 0.87890625\n",
            "Step: 328600, Loss: 408.0775146484375, Accuracy: 0.85546875\n",
            "Step: 328700, Loss: 398.2752380371094, Accuracy: 0.89453125\n",
            "Step: 328800, Loss: 399.56439208984375, Accuracy: 0.8828125\n",
            "Step: 328900, Loss: 393.1575927734375, Accuracy: 0.89453125\n",
            "Step: 329000, Loss: 398.3511047363281, Accuracy: 0.88671875\n",
            "Step: 329100, Loss: 401.7950439453125, Accuracy: 0.8671875\n",
            "Step: 329200, Loss: 399.75390625, Accuracy: 0.87890625\n",
            "Step: 329300, Loss: 398.363037109375, Accuracy: 0.890625\n",
            "Step: 329400, Loss: 400.1741638183594, Accuracy: 0.88671875\n",
            "Step: 329500, Loss: 392.9825744628906, Accuracy: 0.91015625\n",
            "Step: 329600, Loss: 403.1450500488281, Accuracy: 0.8671875\n",
            "Step: 329700, Loss: 403.3975830078125, Accuracy: 0.8671875\n",
            "Step: 329800, Loss: 400.7024230957031, Accuracy: 0.87109375\n",
            "Step: 329900, Loss: 396.26318359375, Accuracy: 0.89453125\n",
            "Step: 330000, Loss: 399.66326904296875, Accuracy: 0.88671875\n",
            "Step: 330100, Loss: 399.7525329589844, Accuracy: 0.890625\n",
            "Step: 330200, Loss: 406.1773681640625, Accuracy: 0.859375\n",
            "Step: 330300, Loss: 394.8276062011719, Accuracy: 0.90625\n",
            "Step: 330400, Loss: 401.620361328125, Accuracy: 0.87109375\n",
            "Step: 330500, Loss: 396.244140625, Accuracy: 0.8984375\n",
            "Step: 330600, Loss: 404.31005859375, Accuracy: 0.8671875\n",
            "Step: 330700, Loss: 398.6338806152344, Accuracy: 0.8828125\n",
            "Step: 330800, Loss: 406.2363586425781, Accuracy: 0.859375\n",
            "Step: 330900, Loss: 406.106201171875, Accuracy: 0.8515625\n",
            "Step: 331000, Loss: 401.36590576171875, Accuracy: 0.875\n",
            "Step: 331100, Loss: 392.86627197265625, Accuracy: 0.9140625\n",
            "Step: 331200, Loss: 405.6208190917969, Accuracy: 0.8671875\n",
            "Step: 331300, Loss: 398.57421875, Accuracy: 0.88671875\n",
            "Step: 331400, Loss: 405.43359375, Accuracy: 0.85546875\n",
            "Step: 331500, Loss: 400.7214660644531, Accuracy: 0.890625\n",
            "Step: 331600, Loss: 403.7308654785156, Accuracy: 0.859375\n",
            "Step: 331700, Loss: 399.1761169433594, Accuracy: 0.875\n",
            "Step: 331800, Loss: 403.55499267578125, Accuracy: 0.875\n",
            "Step: 331900, Loss: 402.00616455078125, Accuracy: 0.87109375\n",
            "Step: 332000, Loss: 396.2696228027344, Accuracy: 0.89453125\n",
            "Step: 332100, Loss: 397.8494873046875, Accuracy: 0.890625\n",
            "Step: 332200, Loss: 400.5921325683594, Accuracy: 0.87109375\n",
            "Step: 332300, Loss: 394.31744384765625, Accuracy: 0.90625\n",
            "Step: 332400, Loss: 392.2691650390625, Accuracy: 0.91796875\n",
            "Step: 332500, Loss: 406.4952697753906, Accuracy: 0.84765625\n",
            "Step: 332600, Loss: 403.11029052734375, Accuracy: 0.875\n",
            "Step: 332700, Loss: 397.82647705078125, Accuracy: 0.88671875\n",
            "Step: 332800, Loss: 402.4775695800781, Accuracy: 0.8671875\n",
            "Step: 332900, Loss: 398.13018798828125, Accuracy: 0.89453125\n",
            "Step: 333000, Loss: 404.7953186035156, Accuracy: 0.86328125\n",
            "Step: 333100, Loss: 402.41900634765625, Accuracy: 0.8671875\n",
            "Step: 333200, Loss: 404.68505859375, Accuracy: 0.859375\n",
            "Step: 333300, Loss: 397.9014892578125, Accuracy: 0.88671875\n",
            "Step: 333400, Loss: 394.40911865234375, Accuracy: 0.90234375\n",
            "Step: 333500, Loss: 398.54052734375, Accuracy: 0.89453125\n",
            "Step: 333600, Loss: 397.491943359375, Accuracy: 0.90234375\n",
            "Step: 333700, Loss: 402.0038146972656, Accuracy: 0.875\n",
            "Step: 333800, Loss: 401.819091796875, Accuracy: 0.8671875\n",
            "Step: 333900, Loss: 401.53125, Accuracy: 0.87890625\n",
            "Step: 334000, Loss: 405.2275390625, Accuracy: 0.86328125\n",
            "Step: 334100, Loss: 396.289306640625, Accuracy: 0.89453125\n",
            "Step: 334200, Loss: 405.9337158203125, Accuracy: 0.8515625\n",
            "Step: 334300, Loss: 408.64178466796875, Accuracy: 0.83984375\n",
            "Step: 334400, Loss: 390.115234375, Accuracy: 0.9296875\n",
            "Step: 334500, Loss: 395.667236328125, Accuracy: 0.8984375\n",
            "Step: 334600, Loss: 405.8941955566406, Accuracy: 0.8515625\n",
            "Step: 334700, Loss: 401.58953857421875, Accuracy: 0.87109375\n",
            "Step: 334800, Loss: 400.23663330078125, Accuracy: 0.87890625\n",
            "Step: 334900, Loss: 399.28240966796875, Accuracy: 0.8828125\n",
            "Step: 335000, Loss: 400.3298034667969, Accuracy: 0.87890625\n",
            "Step: 335100, Loss: 402.102783203125, Accuracy: 0.8671875\n",
            "Step: 335200, Loss: 400.4532470703125, Accuracy: 0.8828125\n",
            "Step: 335300, Loss: 411.8720703125, Accuracy: 0.82421875\n",
            "Step: 335400, Loss: 408.20086669921875, Accuracy: 0.84375\n",
            "Step: 335500, Loss: 405.6151123046875, Accuracy: 0.8671875\n",
            "Step: 335600, Loss: 399.29852294921875, Accuracy: 0.8828125\n",
            "Step: 335700, Loss: 401.2735900878906, Accuracy: 0.875\n",
            "Step: 335800, Loss: 399.6576843261719, Accuracy: 0.87890625\n",
            "Step: 335900, Loss: 399.86236572265625, Accuracy: 0.88671875\n",
            "Step: 336000, Loss: 393.5948791503906, Accuracy: 0.90234375\n",
            "Step: 336100, Loss: 403.61126708984375, Accuracy: 0.8671875\n",
            "Step: 336200, Loss: 398.6531982421875, Accuracy: 0.8828125\n",
            "Step: 336300, Loss: 400.717041015625, Accuracy: 0.87109375\n",
            "Step: 336400, Loss: 399.3414306640625, Accuracy: 0.8828125\n",
            "Step: 336500, Loss: 398.40972900390625, Accuracy: 0.88671875\n",
            "Step: 336600, Loss: 403.58135986328125, Accuracy: 0.8671875\n",
            "Step: 336700, Loss: 403.159423828125, Accuracy: 0.87109375\n",
            "Step: 336800, Loss: 408.00872802734375, Accuracy: 0.84765625\n",
            "Step: 336900, Loss: 388.0243225097656, Accuracy: 0.92578125\n",
            "Step: 337000, Loss: 408.38970947265625, Accuracy: 0.84765625\n",
            "Step: 337100, Loss: 404.28240966796875, Accuracy: 0.86328125\n",
            "Step: 337200, Loss: 397.59320068359375, Accuracy: 0.890625\n",
            "Step: 337300, Loss: 396.4609375, Accuracy: 0.8984375\n",
            "Step: 337400, Loss: 401.52667236328125, Accuracy: 0.87109375\n",
            "Step: 337500, Loss: 392.31915283203125, Accuracy: 0.91796875\n",
            "Step: 337600, Loss: 406.4795837402344, Accuracy: 0.8515625\n",
            "Step: 337700, Loss: 399.8243103027344, Accuracy: 0.88671875\n",
            "Step: 337800, Loss: 403.6278991699219, Accuracy: 0.85546875\n",
            "Step: 337900, Loss: 401.1271667480469, Accuracy: 0.8828125\n",
            "Step: 338000, Loss: 399.6361083984375, Accuracy: 0.88671875\n",
            "Step: 338100, Loss: 395.69000244140625, Accuracy: 0.90625\n",
            "Step: 338200, Loss: 394.6168212890625, Accuracy: 0.89453125\n",
            "Step: 338300, Loss: 405.89300537109375, Accuracy: 0.87109375\n",
            "Step: 338400, Loss: 401.5159912109375, Accuracy: 0.87890625\n",
            "Step: 338500, Loss: 404.2829895019531, Accuracy: 0.84765625\n",
            "Step: 338600, Loss: 403.9201965332031, Accuracy: 0.875\n",
            "Step: 338700, Loss: 404.0498046875, Accuracy: 0.8671875\n",
            "Step: 338800, Loss: 410.88763427734375, Accuracy: 0.8359375\n",
            "Step: 338900, Loss: 403.6754150390625, Accuracy: 0.8671875\n",
            "Step: 339000, Loss: 400.6303405761719, Accuracy: 0.890625\n",
            "Step: 339100, Loss: 391.87139892578125, Accuracy: 0.90625\n",
            "Step: 339200, Loss: 397.95489501953125, Accuracy: 0.8984375\n",
            "Step: 339300, Loss: 404.90789794921875, Accuracy: 0.85546875\n",
            "Step: 339400, Loss: 396.91424560546875, Accuracy: 0.89453125\n",
            "Step: 339500, Loss: 389.3765869140625, Accuracy: 0.93359375\n",
            "Step: 339600, Loss: 394.1962890625, Accuracy: 0.91015625\n",
            "Step: 339700, Loss: 394.82781982421875, Accuracy: 0.90625\n",
            "Step: 339800, Loss: 398.5980224609375, Accuracy: 0.88671875\n",
            "Step: 339900, Loss: 411.67962646484375, Accuracy: 0.8359375\n",
            "Step: 340000, Loss: 396.03936767578125, Accuracy: 0.890625\n",
            "Step: 340100, Loss: 403.35760498046875, Accuracy: 0.8671875\n",
            "Step: 340200, Loss: 399.614013671875, Accuracy: 0.875\n",
            "Step: 340300, Loss: 395.4234313964844, Accuracy: 0.91015625\n",
            "Step: 340400, Loss: 397.84881591796875, Accuracy: 0.89453125\n",
            "Step: 340500, Loss: 395.24932861328125, Accuracy: 0.90625\n",
            "Step: 340600, Loss: 404.36883544921875, Accuracy: 0.859375\n",
            "Step: 340700, Loss: 402.6418762207031, Accuracy: 0.85546875\n",
            "Step: 340800, Loss: 406.9728698730469, Accuracy: 0.84765625\n",
            "Step: 340900, Loss: 401.5819091796875, Accuracy: 0.875\n",
            "Step: 341000, Loss: 396.8434143066406, Accuracy: 0.89453125\n",
            "Step: 341100, Loss: 393.40643310546875, Accuracy: 0.90625\n",
            "Step: 341200, Loss: 399.5757751464844, Accuracy: 0.875\n",
            "Step: 341300, Loss: 393.86639404296875, Accuracy: 0.9140625\n",
            "Step: 341400, Loss: 401.1621398925781, Accuracy: 0.87109375\n",
            "Step: 341500, Loss: 403.01702880859375, Accuracy: 0.87890625\n",
            "Step: 341600, Loss: 393.213134765625, Accuracy: 0.91015625\n",
            "Step: 341700, Loss: 402.62408447265625, Accuracy: 0.875\n",
            "Step: 341800, Loss: 395.4053649902344, Accuracy: 0.8984375\n",
            "Step: 341900, Loss: 401.9249267578125, Accuracy: 0.8828125\n",
            "Step: 342000, Loss: 397.17950439453125, Accuracy: 0.8984375\n",
            "Step: 342100, Loss: 393.79132080078125, Accuracy: 0.91015625\n",
            "Step: 342200, Loss: 403.6422119140625, Accuracy: 0.859375\n",
            "Step: 342300, Loss: 398.026123046875, Accuracy: 0.88671875\n",
            "Step: 342400, Loss: 398.5885009765625, Accuracy: 0.8828125\n",
            "Step: 342500, Loss: 395.4395751953125, Accuracy: 0.8984375\n",
            "Step: 342600, Loss: 398.48577880859375, Accuracy: 0.89453125\n",
            "Step: 342700, Loss: 395.0905456542969, Accuracy: 0.91015625\n",
            "Step: 342800, Loss: 403.0236511230469, Accuracy: 0.87109375\n",
            "Step: 342900, Loss: 399.3349304199219, Accuracy: 0.88671875\n",
            "Step: 343000, Loss: 401.26031494140625, Accuracy: 0.875\n",
            "Step: 343100, Loss: 407.1292724609375, Accuracy: 0.8515625\n",
            "Step: 343200, Loss: 400.629150390625, Accuracy: 0.875\n",
            "Step: 343300, Loss: 411.51312255859375, Accuracy: 0.8359375\n",
            "Step: 343400, Loss: 396.97271728515625, Accuracy: 0.89453125\n",
            "Step: 343500, Loss: 405.3846435546875, Accuracy: 0.86328125\n",
            "Step: 343600, Loss: 398.5047302246094, Accuracy: 0.8828125\n",
            "Step: 343700, Loss: 390.1360778808594, Accuracy: 0.92578125\n",
            "Step: 343800, Loss: 399.3840637207031, Accuracy: 0.88671875\n",
            "Step: 343900, Loss: 400.4539489746094, Accuracy: 0.875\n",
            "Step: 344000, Loss: 402.52301025390625, Accuracy: 0.87109375\n",
            "Step: 344100, Loss: 400.29931640625, Accuracy: 0.87109375\n",
            "Step: 344200, Loss: 404.6953125, Accuracy: 0.86328125\n",
            "Step: 344300, Loss: 398.3487243652344, Accuracy: 0.8828125\n",
            "Step: 344400, Loss: 398.03466796875, Accuracy: 0.8984375\n",
            "Step: 344500, Loss: 398.1059265136719, Accuracy: 0.890625\n",
            "Step: 344600, Loss: 397.8431396484375, Accuracy: 0.88671875\n",
            "Step: 344700, Loss: 400.679443359375, Accuracy: 0.875\n",
            "Step: 344800, Loss: 403.70843505859375, Accuracy: 0.87890625\n",
            "Step: 344900, Loss: 404.7571105957031, Accuracy: 0.87109375\n",
            "Step: 345000, Loss: 399.8201904296875, Accuracy: 0.87890625\n",
            "Step: 345100, Loss: 398.72698974609375, Accuracy: 0.8828125\n",
            "Step: 345200, Loss: 405.3916320800781, Accuracy: 0.859375\n",
            "Step: 345300, Loss: 399.9244384765625, Accuracy: 0.87890625\n",
            "Step: 345400, Loss: 399.59765625, Accuracy: 0.87890625\n",
            "Step: 345500, Loss: 401.2143249511719, Accuracy: 0.87109375\n",
            "Step: 345600, Loss: 402.60614013671875, Accuracy: 0.8671875\n",
            "Step: 345700, Loss: 395.9162902832031, Accuracy: 0.89453125\n",
            "Step: 345800, Loss: 405.2940979003906, Accuracy: 0.86328125\n",
            "Step: 345900, Loss: 391.4018859863281, Accuracy: 0.921875\n",
            "Step: 346000, Loss: 398.34307861328125, Accuracy: 0.88671875\n",
            "Step: 346100, Loss: 402.68585205078125, Accuracy: 0.87109375\n",
            "Step: 346200, Loss: 398.18731689453125, Accuracy: 0.890625\n",
            "Step: 346300, Loss: 409.25848388671875, Accuracy: 0.83984375\n",
            "Step: 346400, Loss: 398.71331787109375, Accuracy: 0.890625\n",
            "Step: 346500, Loss: 397.54730224609375, Accuracy: 0.87890625\n",
            "Step: 346600, Loss: 405.0264587402344, Accuracy: 0.84375\n",
            "Step: 346700, Loss: 400.4059143066406, Accuracy: 0.87890625\n",
            "Step: 346800, Loss: 399.7950744628906, Accuracy: 0.875\n",
            "Step: 346900, Loss: 394.4745788574219, Accuracy: 0.90625\n",
            "Step: 347000, Loss: 402.58306884765625, Accuracy: 0.8671875\n",
            "Step: 347100, Loss: 398.7123718261719, Accuracy: 0.8828125\n",
            "Step: 347200, Loss: 404.39990234375, Accuracy: 0.84765625\n",
            "Step: 347300, Loss: 405.4779357910156, Accuracy: 0.85546875\n",
            "Step: 347400, Loss: 399.76824951171875, Accuracy: 0.875\n",
            "Step: 347500, Loss: 400.72979736328125, Accuracy: 0.8828125\n",
            "Step: 347600, Loss: 399.32843017578125, Accuracy: 0.8828125\n",
            "Step: 347700, Loss: 405.3669128417969, Accuracy: 0.859375\n",
            "Step: 347800, Loss: 395.1374816894531, Accuracy: 0.90625\n",
            "Step: 347900, Loss: 396.51898193359375, Accuracy: 0.8828125\n",
            "Step: 348000, Loss: 400.305419921875, Accuracy: 0.875\n",
            "Step: 348100, Loss: 397.6023864746094, Accuracy: 0.89453125\n",
            "Step: 348200, Loss: 399.86639404296875, Accuracy: 0.88671875\n",
            "Step: 348300, Loss: 400.61376953125, Accuracy: 0.875\n",
            "Step: 348400, Loss: 398.1519470214844, Accuracy: 0.88671875\n",
            "Step: 348500, Loss: 396.39471435546875, Accuracy: 0.90234375\n",
            "Step: 348600, Loss: 396.7466735839844, Accuracy: 0.89453125\n",
            "Step: 348700, Loss: 403.34027099609375, Accuracy: 0.8671875\n",
            "Step: 348800, Loss: 394.6947021484375, Accuracy: 0.8984375\n",
            "Step: 348900, Loss: 406.353271484375, Accuracy: 0.8515625\n",
            "Step: 349000, Loss: 405.8046569824219, Accuracy: 0.859375\n",
            "Step: 349100, Loss: 402.51995849609375, Accuracy: 0.86328125\n",
            "Step: 349200, Loss: 404.19940185546875, Accuracy: 0.859375\n",
            "Step: 349300, Loss: 401.88031005859375, Accuracy: 0.87109375\n",
            "Step: 349400, Loss: 400.87652587890625, Accuracy: 0.87890625\n",
            "Step: 349500, Loss: 400.5292053222656, Accuracy: 0.8828125\n",
            "Step: 349600, Loss: 399.918212890625, Accuracy: 0.87890625\n",
            "Step: 349700, Loss: 398.72308349609375, Accuracy: 0.88671875\n",
            "Step: 349800, Loss: 407.6279602050781, Accuracy: 0.84375\n",
            "Step: 349900, Loss: 396.9656677246094, Accuracy: 0.8984375\n",
            "Step: 350000, Loss: 400.18463134765625, Accuracy: 0.875\n",
            "Step: 350100, Loss: 406.4632873535156, Accuracy: 0.84375\n",
            "Step: 350200, Loss: 399.18310546875, Accuracy: 0.890625\n",
            "Step: 350300, Loss: 400.842529296875, Accuracy: 0.87109375\n",
            "Step: 350400, Loss: 391.8537902832031, Accuracy: 0.92578125\n",
            "Step: 350500, Loss: 406.3102722167969, Accuracy: 0.8515625\n",
            "Step: 350600, Loss: 410.16259765625, Accuracy: 0.8359375\n",
            "Step: 350700, Loss: 393.65948486328125, Accuracy: 0.91015625\n",
            "Step: 350800, Loss: 403.513671875, Accuracy: 0.86328125\n",
            "Step: 350900, Loss: 396.41302490234375, Accuracy: 0.890625\n",
            "Step: 351000, Loss: 407.44207763671875, Accuracy: 0.84765625\n",
            "Step: 351100, Loss: 393.30377197265625, Accuracy: 0.9140625\n",
            "Step: 351200, Loss: 398.7243347167969, Accuracy: 0.8828125\n",
            "Step: 351300, Loss: 398.68426513671875, Accuracy: 0.87890625\n",
            "Step: 351400, Loss: 397.3398132324219, Accuracy: 0.88671875\n",
            "Step: 351500, Loss: 400.236328125, Accuracy: 0.87890625\n",
            "Step: 351600, Loss: 403.58197021484375, Accuracy: 0.8671875\n",
            "Step: 351700, Loss: 399.45672607421875, Accuracy: 0.890625\n",
            "Step: 351800, Loss: 401.7603759765625, Accuracy: 0.87890625\n",
            "Step: 351900, Loss: 400.5044860839844, Accuracy: 0.8828125\n",
            "Step: 352000, Loss: 393.288330078125, Accuracy: 0.91015625\n",
            "Step: 352100, Loss: 398.0568542480469, Accuracy: 0.890625\n",
            "Step: 352200, Loss: 406.4833984375, Accuracy: 0.8515625\n",
            "Step: 352300, Loss: 407.46826171875, Accuracy: 0.84765625\n",
            "Step: 352400, Loss: 404.80401611328125, Accuracy: 0.86328125\n",
            "Step: 352500, Loss: 396.09283447265625, Accuracy: 0.89453125\n",
            "Step: 352600, Loss: 402.625732421875, Accuracy: 0.85546875\n",
            "Step: 352700, Loss: 404.7901916503906, Accuracy: 0.859375\n",
            "Step: 352800, Loss: 402.22381591796875, Accuracy: 0.8671875\n",
            "Step: 352900, Loss: 405.5330810546875, Accuracy: 0.87109375\n",
            "Step: 353000, Loss: 403.2139892578125, Accuracy: 0.8671875\n",
            "Step: 353100, Loss: 400.9615783691406, Accuracy: 0.87890625\n",
            "Step: 353200, Loss: 399.1329040527344, Accuracy: 0.8984375\n",
            "Step: 353300, Loss: 402.4385070800781, Accuracy: 0.86328125\n",
            "Step: 353400, Loss: 404.2793884277344, Accuracy: 0.8515625\n",
            "Step: 353500, Loss: 400.29266357421875, Accuracy: 0.87109375\n",
            "Step: 353600, Loss: 395.67559814453125, Accuracy: 0.90625\n",
            "Step: 353700, Loss: 396.700439453125, Accuracy: 0.890625\n",
            "Step: 353800, Loss: 395.7506408691406, Accuracy: 0.91015625\n",
            "Step: 353900, Loss: 399.5347900390625, Accuracy: 0.87890625\n",
            "Step: 354000, Loss: 406.218994140625, Accuracy: 0.8515625\n",
            "Step: 354100, Loss: 402.2196044921875, Accuracy: 0.859375\n",
            "Step: 354200, Loss: 395.5104064941406, Accuracy: 0.8984375\n",
            "Step: 354300, Loss: 398.66180419921875, Accuracy: 0.890625\n",
            "Step: 354400, Loss: 401.609619140625, Accuracy: 0.875\n",
            "Step: 354500, Loss: 403.4075012207031, Accuracy: 0.86328125\n",
            "Step: 354600, Loss: 397.89996337890625, Accuracy: 0.890625\n",
            "Step: 354700, Loss: 399.38494873046875, Accuracy: 0.8828125\n",
            "Step: 354800, Loss: 403.3726806640625, Accuracy: 0.87109375\n",
            "Step: 354900, Loss: 395.25079345703125, Accuracy: 0.8984375\n",
            "Step: 355000, Loss: 395.047607421875, Accuracy: 0.90234375\n",
            "Step: 355100, Loss: 404.5577392578125, Accuracy: 0.86328125\n",
            "Step: 355200, Loss: 394.03619384765625, Accuracy: 0.91015625\n",
            "Step: 355300, Loss: 402.12823486328125, Accuracy: 0.87109375\n",
            "Step: 355400, Loss: 399.56329345703125, Accuracy: 0.87890625\n",
            "Step: 355500, Loss: 397.1990661621094, Accuracy: 0.89453125\n",
            "Step: 355600, Loss: 405.0799560546875, Accuracy: 0.859375\n",
            "Step: 355700, Loss: 399.37701416015625, Accuracy: 0.875\n",
            "Step: 355800, Loss: 398.5939636230469, Accuracy: 0.8828125\n",
            "Step: 355900, Loss: 402.8411560058594, Accuracy: 0.86328125\n",
            "Step: 356000, Loss: 399.776123046875, Accuracy: 0.8828125\n",
            "Step: 356100, Loss: 396.2081604003906, Accuracy: 0.890625\n",
            "Step: 356200, Loss: 402.3709716796875, Accuracy: 0.86328125\n",
            "Step: 356300, Loss: 390.8550720214844, Accuracy: 0.921875\n",
            "Step: 356400, Loss: 395.74908447265625, Accuracy: 0.890625\n",
            "Step: 356500, Loss: 405.85272216796875, Accuracy: 0.859375\n",
            "Step: 356600, Loss: 396.4854736328125, Accuracy: 0.890625\n",
            "Step: 356700, Loss: 399.1475524902344, Accuracy: 0.8828125\n",
            "Step: 356800, Loss: 400.82232666015625, Accuracy: 0.88671875\n",
            "Step: 356900, Loss: 399.69793701171875, Accuracy: 0.87890625\n",
            "Step: 357000, Loss: 401.65509033203125, Accuracy: 0.87109375\n",
            "Step: 357100, Loss: 389.93115234375, Accuracy: 0.92578125\n",
            "Step: 357200, Loss: 394.04217529296875, Accuracy: 0.91015625\n",
            "Step: 357300, Loss: 398.63739013671875, Accuracy: 0.88671875\n",
            "Step: 357400, Loss: 393.7306213378906, Accuracy: 0.90234375\n",
            "Step: 357500, Loss: 400.9957275390625, Accuracy: 0.87109375\n",
            "Step: 357600, Loss: 403.5723876953125, Accuracy: 0.859375\n",
            "Step: 357700, Loss: 400.20135498046875, Accuracy: 0.87890625\n",
            "Step: 357800, Loss: 401.0191345214844, Accuracy: 0.87890625\n",
            "Step: 357900, Loss: 393.8468933105469, Accuracy: 0.90625\n",
            "Step: 358000, Loss: 401.3902587890625, Accuracy: 0.87890625\n",
            "Step: 358100, Loss: 393.5533752441406, Accuracy: 0.90625\n",
            "Step: 358200, Loss: 397.04913330078125, Accuracy: 0.8828125\n",
            "Step: 358300, Loss: 401.7867431640625, Accuracy: 0.875\n",
            "Step: 358400, Loss: 397.625, Accuracy: 0.89453125\n",
            "Step: 358500, Loss: 401.1124572753906, Accuracy: 0.87890625\n",
            "Step: 358600, Loss: 401.2772521972656, Accuracy: 0.87109375\n",
            "Step: 358700, Loss: 398.45953369140625, Accuracy: 0.890625\n",
            "Step: 358800, Loss: 397.521240234375, Accuracy: 0.90625\n",
            "Step: 358900, Loss: 407.1158447265625, Accuracy: 0.8515625\n",
            "Step: 359000, Loss: 401.0675048828125, Accuracy: 0.87109375\n",
            "Step: 359100, Loss: 399.66461181640625, Accuracy: 0.87109375\n",
            "Step: 359200, Loss: 400.90753173828125, Accuracy: 0.87109375\n",
            "Step: 359300, Loss: 393.2149353027344, Accuracy: 0.9140625\n",
            "Step: 359400, Loss: 400.9864196777344, Accuracy: 0.88671875\n",
            "Step: 359500, Loss: 399.5030517578125, Accuracy: 0.88671875\n",
            "Step: 359600, Loss: 396.8285827636719, Accuracy: 0.89453125\n",
            "Step: 359700, Loss: 401.49652099609375, Accuracy: 0.875\n",
            "Step: 359800, Loss: 401.35235595703125, Accuracy: 0.875\n",
            "Step: 359900, Loss: 399.3177795410156, Accuracy: 0.89453125\n",
            "Step: 360000, Loss: 396.0368957519531, Accuracy: 0.91015625\n",
            "Step: 360100, Loss: 396.71966552734375, Accuracy: 0.890625\n",
            "Step: 360200, Loss: 400.8708190917969, Accuracy: 0.87890625\n",
            "Step: 360300, Loss: 396.56707763671875, Accuracy: 0.8984375\n",
            "Step: 360400, Loss: 399.40576171875, Accuracy: 0.88671875\n",
            "Step: 360500, Loss: 406.46331787109375, Accuracy: 0.8515625\n",
            "Step: 360600, Loss: 401.952392578125, Accuracy: 0.8828125\n",
            "Step: 360700, Loss: 396.04931640625, Accuracy: 0.8984375\n",
            "Step: 360800, Loss: 400.791015625, Accuracy: 0.8671875\n",
            "Step: 360900, Loss: 401.83563232421875, Accuracy: 0.87109375\n",
            "Step: 361000, Loss: 409.4739990234375, Accuracy: 0.84765625\n",
            "Step: 361100, Loss: 408.4150390625, Accuracy: 0.8515625\n",
            "Step: 361200, Loss: 402.4956970214844, Accuracy: 0.87109375\n",
            "Step: 361300, Loss: 403.6491394042969, Accuracy: 0.86328125\n",
            "Step: 361400, Loss: 400.56573486328125, Accuracy: 0.859375\n",
            "Step: 361500, Loss: 402.71270751953125, Accuracy: 0.8671875\n",
            "Step: 361600, Loss: 401.31439208984375, Accuracy: 0.8828125\n",
            "Step: 361700, Loss: 396.25738525390625, Accuracy: 0.890625\n",
            "Step: 361800, Loss: 404.530517578125, Accuracy: 0.859375\n",
            "Step: 361900, Loss: 403.14727783203125, Accuracy: 0.87109375\n",
            "Step: 362000, Loss: 401.8388671875, Accuracy: 0.875\n",
            "Step: 362100, Loss: 394.31268310546875, Accuracy: 0.8984375\n",
            "Step: 362200, Loss: 398.2137451171875, Accuracy: 0.88671875\n",
            "Step: 362300, Loss: 404.49407958984375, Accuracy: 0.8515625\n",
            "Step: 362400, Loss: 397.698486328125, Accuracy: 0.8828125\n",
            "Step: 362500, Loss: 400.301025390625, Accuracy: 0.890625\n",
            "Step: 362600, Loss: 398.343505859375, Accuracy: 0.88671875\n",
            "Step: 362700, Loss: 397.0257873535156, Accuracy: 0.89453125\n",
            "Step: 362800, Loss: 397.33721923828125, Accuracy: 0.88671875\n",
            "Step: 362900, Loss: 396.0628662109375, Accuracy: 0.89453125\n",
            "Step: 363000, Loss: 398.63189697265625, Accuracy: 0.890625\n",
            "Step: 363100, Loss: 406.785400390625, Accuracy: 0.84375\n",
            "Step: 363200, Loss: 401.76971435546875, Accuracy: 0.8671875\n",
            "Step: 363300, Loss: 400.6087341308594, Accuracy: 0.890625\n",
            "Step: 363400, Loss: 400.013916015625, Accuracy: 0.87890625\n",
            "Step: 363500, Loss: 399.7618713378906, Accuracy: 0.88671875\n",
            "Step: 363600, Loss: 394.6723937988281, Accuracy: 0.8984375\n",
            "Step: 363700, Loss: 394.33441162109375, Accuracy: 0.9140625\n",
            "Step: 363800, Loss: 404.65802001953125, Accuracy: 0.859375\n",
            "Step: 363900, Loss: 400.35101318359375, Accuracy: 0.8828125\n",
            "Step: 364000, Loss: 403.9429626464844, Accuracy: 0.85546875\n",
            "Step: 364100, Loss: 397.1103820800781, Accuracy: 0.88671875\n",
            "Step: 364200, Loss: 402.4188232421875, Accuracy: 0.8671875\n",
            "Step: 364300, Loss: 391.03271484375, Accuracy: 0.91796875\n",
            "Step: 364400, Loss: 400.4205322265625, Accuracy: 0.875\n",
            "Step: 364500, Loss: 399.8282470703125, Accuracy: 0.87890625\n",
            "Step: 364600, Loss: 400.1626892089844, Accuracy: 0.88671875\n",
            "Step: 364700, Loss: 403.569580078125, Accuracy: 0.8671875\n",
            "Step: 364800, Loss: 400.2136535644531, Accuracy: 0.87890625\n",
            "Step: 364900, Loss: 398.89385986328125, Accuracy: 0.89453125\n",
            "Step: 365000, Loss: 407.24273681640625, Accuracy: 0.83984375\n",
            "Step: 365100, Loss: 402.0872802734375, Accuracy: 0.875\n",
            "Step: 365200, Loss: 395.64788818359375, Accuracy: 0.89453125\n",
            "Step: 365300, Loss: 400.9757995605469, Accuracy: 0.87890625\n",
            "Step: 365400, Loss: 403.3577880859375, Accuracy: 0.87109375\n",
            "Step: 365500, Loss: 401.3667907714844, Accuracy: 0.875\n",
            "Step: 365600, Loss: 400.9282531738281, Accuracy: 0.8828125\n",
            "Step: 365700, Loss: 398.02886962890625, Accuracy: 0.890625\n",
            "Step: 365800, Loss: 407.2704772949219, Accuracy: 0.8515625\n",
            "Step: 365900, Loss: 406.39801025390625, Accuracy: 0.859375\n",
            "Step: 366000, Loss: 392.3164978027344, Accuracy: 0.91015625\n",
            "Step: 366100, Loss: 405.314453125, Accuracy: 0.859375\n",
            "Step: 366200, Loss: 396.77117919921875, Accuracy: 0.89453125\n",
            "Step: 366300, Loss: 401.77691650390625, Accuracy: 0.8671875\n",
            "Step: 366400, Loss: 390.3181457519531, Accuracy: 0.92578125\n",
            "Step: 366500, Loss: 395.6011047363281, Accuracy: 0.89453125\n",
            "Step: 366600, Loss: 394.1403503417969, Accuracy: 0.91015625\n",
            "Step: 366700, Loss: 395.5541076660156, Accuracy: 0.90234375\n",
            "Step: 366800, Loss: 396.9578857421875, Accuracy: 0.890625\n",
            "Step: 366900, Loss: 404.54681396484375, Accuracy: 0.859375\n",
            "Step: 367000, Loss: 393.795654296875, Accuracy: 0.90625\n",
            "Step: 367100, Loss: 399.35504150390625, Accuracy: 0.890625\n",
            "Step: 367200, Loss: 405.7065734863281, Accuracy: 0.84375\n",
            "Step: 367300, Loss: 401.2099609375, Accuracy: 0.8671875\n",
            "Step: 367400, Loss: 396.3143005371094, Accuracy: 0.890625\n",
            "Step: 367500, Loss: 403.03375244140625, Accuracy: 0.87109375\n",
            "Step: 367600, Loss: 391.7587890625, Accuracy: 0.91796875\n",
            "Step: 367700, Loss: 401.087646484375, Accuracy: 0.875\n",
            "Step: 367800, Loss: 401.2835388183594, Accuracy: 0.875\n",
            "Step: 367900, Loss: 407.826171875, Accuracy: 0.84765625\n",
            "Step: 368000, Loss: 405.7527160644531, Accuracy: 0.8515625\n",
            "Step: 368100, Loss: 399.096923828125, Accuracy: 0.875\n",
            "Step: 368200, Loss: 395.1330871582031, Accuracy: 0.90625\n",
            "Step: 368300, Loss: 400.61712646484375, Accuracy: 0.8828125\n",
            "Step: 368400, Loss: 408.9941101074219, Accuracy: 0.83984375\n",
            "Step: 368500, Loss: 408.443359375, Accuracy: 0.8359375\n",
            "Step: 368600, Loss: 403.2686767578125, Accuracy: 0.8515625\n",
            "Step: 368700, Loss: 396.02679443359375, Accuracy: 0.89453125\n",
            "Step: 368800, Loss: 397.2074890136719, Accuracy: 0.8984375\n",
            "Step: 368900, Loss: 402.34271240234375, Accuracy: 0.86328125\n",
            "Step: 369000, Loss: 394.7068786621094, Accuracy: 0.90625\n",
            "Step: 369100, Loss: 397.22210693359375, Accuracy: 0.88671875\n",
            "Step: 369200, Loss: 393.41546630859375, Accuracy: 0.91015625\n",
            "Step: 369300, Loss: 407.49127197265625, Accuracy: 0.8515625\n",
            "Step: 369400, Loss: 405.0181579589844, Accuracy: 0.86328125\n",
            "Step: 369500, Loss: 397.6656494140625, Accuracy: 0.890625\n",
            "Step: 369600, Loss: 397.68621826171875, Accuracy: 0.890625\n",
            "Step: 369700, Loss: 398.81195068359375, Accuracy: 0.8984375\n",
            "Step: 369800, Loss: 401.20245361328125, Accuracy: 0.875\n",
            "Step: 369900, Loss: 402.15252685546875, Accuracy: 0.859375\n",
            "Step: 370000, Loss: 403.59619140625, Accuracy: 0.8671875\n",
            "Step: 370100, Loss: 401.8056640625, Accuracy: 0.8828125\n",
            "Step: 370200, Loss: 397.89776611328125, Accuracy: 0.88671875\n",
            "Step: 370300, Loss: 399.2649230957031, Accuracy: 0.8828125\n",
            "Step: 370400, Loss: 400.8702392578125, Accuracy: 0.87890625\n",
            "Step: 370500, Loss: 399.2255859375, Accuracy: 0.890625\n",
            "Step: 370600, Loss: 397.826171875, Accuracy: 0.890625\n",
            "Step: 370700, Loss: 392.484375, Accuracy: 0.91015625\n",
            "Step: 370800, Loss: 409.63067626953125, Accuracy: 0.83203125\n",
            "Step: 370900, Loss: 405.45367431640625, Accuracy: 0.85546875\n",
            "Step: 371000, Loss: 400.81793212890625, Accuracy: 0.875\n",
            "Step: 371100, Loss: 399.24395751953125, Accuracy: 0.875\n",
            "Step: 371200, Loss: 402.34747314453125, Accuracy: 0.859375\n",
            "Step: 371300, Loss: 395.1383056640625, Accuracy: 0.8984375\n",
            "Step: 371400, Loss: 401.34967041015625, Accuracy: 0.87109375\n",
            "Step: 371500, Loss: 403.077880859375, Accuracy: 0.87109375\n",
            "Step: 371600, Loss: 400.6126708984375, Accuracy: 0.8828125\n",
            "Step: 371700, Loss: 401.1614990234375, Accuracy: 0.875\n",
            "Step: 371800, Loss: 401.25579833984375, Accuracy: 0.87109375\n",
            "Step: 371900, Loss: 399.869140625, Accuracy: 0.8828125\n",
            "Step: 372000, Loss: 395.74078369140625, Accuracy: 0.90234375\n",
            "Step: 372100, Loss: 399.87445068359375, Accuracy: 0.890625\n",
            "Step: 372200, Loss: 400.4698791503906, Accuracy: 0.88671875\n",
            "Step: 372300, Loss: 398.7929382324219, Accuracy: 0.890625\n",
            "Step: 372400, Loss: 398.84088134765625, Accuracy: 0.88671875\n",
            "Step: 372500, Loss: 403.738037109375, Accuracy: 0.86328125\n",
            "Step: 372600, Loss: 405.6724853515625, Accuracy: 0.8515625\n",
            "Step: 372700, Loss: 400.3659362792969, Accuracy: 0.87890625\n",
            "Step: 372800, Loss: 403.21697998046875, Accuracy: 0.87109375\n",
            "Step: 372900, Loss: 398.7430725097656, Accuracy: 0.8984375\n",
            "Step: 373000, Loss: 407.56634521484375, Accuracy: 0.8515625\n",
            "Step: 373100, Loss: 400.848876953125, Accuracy: 0.88671875\n",
            "Step: 373200, Loss: 404.482421875, Accuracy: 0.85546875\n",
            "Step: 373300, Loss: 407.220947265625, Accuracy: 0.8515625\n",
            "Step: 373400, Loss: 414.1665954589844, Accuracy: 0.8203125\n",
            "Step: 373500, Loss: 409.154052734375, Accuracy: 0.8359375\n",
            "Step: 373600, Loss: 400.52972412109375, Accuracy: 0.890625\n",
            "Step: 373700, Loss: 399.93133544921875, Accuracy: 0.875\n",
            "Step: 373800, Loss: 406.25347900390625, Accuracy: 0.86328125\n",
            "Step: 373900, Loss: 399.9053955078125, Accuracy: 0.87890625\n",
            "Step: 374000, Loss: 403.3388671875, Accuracy: 0.87109375\n",
            "Step: 374100, Loss: 406.15618896484375, Accuracy: 0.85546875\n",
            "Step: 374200, Loss: 393.6253356933594, Accuracy: 0.9140625\n",
            "Step: 374300, Loss: 393.8572082519531, Accuracy: 0.9140625\n",
            "Step: 374400, Loss: 399.3979187011719, Accuracy: 0.87109375\n",
            "Step: 374500, Loss: 398.9432373046875, Accuracy: 0.890625\n",
            "Step: 374600, Loss: 395.16766357421875, Accuracy: 0.90234375\n",
            "Step: 374700, Loss: 397.9353942871094, Accuracy: 0.8828125\n",
            "Step: 374800, Loss: 408.18560791015625, Accuracy: 0.83984375\n",
            "Step: 374900, Loss: 407.5317687988281, Accuracy: 0.84765625\n",
            "Step: 375000, Loss: 397.6052551269531, Accuracy: 0.89453125\n",
            "Step: 375100, Loss: 398.7198181152344, Accuracy: 0.8828125\n",
            "Step: 375200, Loss: 406.5172424316406, Accuracy: 0.84765625\n",
            "Step: 375300, Loss: 396.2137145996094, Accuracy: 0.8984375\n",
            "Step: 375400, Loss: 393.4924621582031, Accuracy: 0.90625\n",
            "Step: 375500, Loss: 397.86944580078125, Accuracy: 0.890625\n",
            "Step: 375600, Loss: 404.83673095703125, Accuracy: 0.859375\n",
            "Step: 375700, Loss: 398.411865234375, Accuracy: 0.890625\n",
            "Step: 375800, Loss: 407.438232421875, Accuracy: 0.84375\n",
            "Step: 375900, Loss: 398.57928466796875, Accuracy: 0.87890625\n",
            "Step: 376000, Loss: 400.148681640625, Accuracy: 0.8828125\n",
            "Step: 376100, Loss: 399.0130920410156, Accuracy: 0.8828125\n",
            "Step: 376200, Loss: 394.8613586425781, Accuracy: 0.90234375\n",
            "Step: 376300, Loss: 400.08013916015625, Accuracy: 0.87890625\n",
            "Step: 376400, Loss: 409.6244201660156, Accuracy: 0.84765625\n",
            "Step: 376500, Loss: 405.24725341796875, Accuracy: 0.87109375\n",
            "Step: 376600, Loss: 398.5788269042969, Accuracy: 0.90234375\n",
            "Step: 376700, Loss: 399.5876770019531, Accuracy: 0.8828125\n",
            "Step: 376800, Loss: 399.8817138671875, Accuracy: 0.87890625\n",
            "Step: 376900, Loss: 399.4004211425781, Accuracy: 0.8828125\n",
            "Step: 377000, Loss: 395.216552734375, Accuracy: 0.890625\n",
            "Step: 377100, Loss: 403.0848388671875, Accuracy: 0.875\n",
            "Step: 377200, Loss: 399.8216857910156, Accuracy: 0.88671875\n",
            "Step: 377300, Loss: 402.0977783203125, Accuracy: 0.87109375\n",
            "Step: 377400, Loss: 397.81640625, Accuracy: 0.8984375\n",
            "Step: 377500, Loss: 395.3035583496094, Accuracy: 0.90625\n",
            "Step: 377600, Loss: 402.53570556640625, Accuracy: 0.859375\n",
            "Step: 377700, Loss: 402.76153564453125, Accuracy: 0.87109375\n",
            "Step: 377800, Loss: 394.135009765625, Accuracy: 0.9140625\n",
            "Step: 377900, Loss: 394.42071533203125, Accuracy: 0.91015625\n",
            "Step: 378000, Loss: 402.230224609375, Accuracy: 0.87890625\n",
            "Step: 378100, Loss: 399.3570861816406, Accuracy: 0.87890625\n",
            "Step: 378200, Loss: 399.4723205566406, Accuracy: 0.8828125\n",
            "Step: 378300, Loss: 403.9617614746094, Accuracy: 0.859375\n",
            "Step: 378400, Loss: 399.483642578125, Accuracy: 0.87890625\n",
            "Step: 378500, Loss: 402.3728942871094, Accuracy: 0.87109375\n",
            "Step: 378600, Loss: 396.0949401855469, Accuracy: 0.89453125\n",
            "Step: 378700, Loss: 402.5306701660156, Accuracy: 0.87890625\n",
            "Step: 378800, Loss: 401.377197265625, Accuracy: 0.875\n",
            "Step: 378900, Loss: 395.2945251464844, Accuracy: 0.8984375\n",
            "Step: 379000, Loss: 407.03857421875, Accuracy: 0.85546875\n",
            "Step: 379100, Loss: 399.4718322753906, Accuracy: 0.87109375\n",
            "Step: 379200, Loss: 405.3587646484375, Accuracy: 0.85546875\n",
            "Step: 379300, Loss: 397.6266174316406, Accuracy: 0.8984375\n",
            "Step: 379400, Loss: 401.36566162109375, Accuracy: 0.8828125\n",
            "Step: 379500, Loss: 401.9920654296875, Accuracy: 0.87109375\n",
            "Step: 379600, Loss: 396.3105163574219, Accuracy: 0.89453125\n",
            "Step: 379700, Loss: 405.36260986328125, Accuracy: 0.84765625\n",
            "Step: 379800, Loss: 396.0714111328125, Accuracy: 0.89453125\n",
            "Step: 379900, Loss: 400.536865234375, Accuracy: 0.87109375\n",
            "Step: 380000, Loss: 394.4192199707031, Accuracy: 0.8984375\n",
            "Step: 380100, Loss: 406.01776123046875, Accuracy: 0.8515625\n",
            "Step: 380200, Loss: 399.7214050292969, Accuracy: 0.88671875\n",
            "Step: 380300, Loss: 397.08074951171875, Accuracy: 0.890625\n",
            "Step: 380400, Loss: 398.7576599121094, Accuracy: 0.88671875\n",
            "Step: 380500, Loss: 401.5332336425781, Accuracy: 0.87890625\n",
            "Step: 380600, Loss: 404.68011474609375, Accuracy: 0.87109375\n",
            "Step: 380700, Loss: 393.7503662109375, Accuracy: 0.9140625\n",
            "Step: 380800, Loss: 409.93756103515625, Accuracy: 0.83203125\n",
            "Step: 380900, Loss: 402.5785827636719, Accuracy: 0.87109375\n",
            "Step: 381000, Loss: 407.02764892578125, Accuracy: 0.83984375\n",
            "Step: 381100, Loss: 391.79803466796875, Accuracy: 0.9140625\n",
            "Step: 381200, Loss: 396.7209167480469, Accuracy: 0.8984375\n",
            "Step: 381300, Loss: 397.7835388183594, Accuracy: 0.89453125\n",
            "Step: 381400, Loss: 395.76312255859375, Accuracy: 0.90234375\n",
            "Step: 381500, Loss: 405.229736328125, Accuracy: 0.859375\n",
            "Step: 381600, Loss: 401.18084716796875, Accuracy: 0.87109375\n",
            "Step: 381700, Loss: 403.6842041015625, Accuracy: 0.87109375\n",
            "Step: 381800, Loss: 400.8433837890625, Accuracy: 0.8828125\n",
            "Step: 381900, Loss: 396.36309814453125, Accuracy: 0.90625\n",
            "Step: 382000, Loss: 396.2327880859375, Accuracy: 0.91015625\n",
            "Step: 382100, Loss: 399.42608642578125, Accuracy: 0.88671875\n",
            "Step: 382200, Loss: 401.98602294921875, Accuracy: 0.87109375\n",
            "Step: 382300, Loss: 401.5212707519531, Accuracy: 0.87890625\n",
            "Step: 382400, Loss: 401.178955078125, Accuracy: 0.875\n",
            "Step: 382500, Loss: 392.6991882324219, Accuracy: 0.90234375\n",
            "Step: 382600, Loss: 402.2957763671875, Accuracy: 0.87109375\n",
            "Step: 382700, Loss: 401.08917236328125, Accuracy: 0.8828125\n",
            "Step: 382800, Loss: 402.27801513671875, Accuracy: 0.87109375\n",
            "Step: 382900, Loss: 408.3403015136719, Accuracy: 0.83984375\n",
            "Step: 383000, Loss: 401.9181823730469, Accuracy: 0.87109375\n",
            "Step: 383100, Loss: 399.112060546875, Accuracy: 0.88671875\n",
            "Step: 383200, Loss: 396.30767822265625, Accuracy: 0.8984375\n",
            "Step: 383300, Loss: 410.1490173339844, Accuracy: 0.8359375\n",
            "Step: 383400, Loss: 395.6307373046875, Accuracy: 0.890625\n",
            "Step: 383500, Loss: 402.521728515625, Accuracy: 0.87890625\n",
            "Step: 383600, Loss: 391.5077209472656, Accuracy: 0.9140625\n",
            "Step: 383700, Loss: 394.45843505859375, Accuracy: 0.9140625\n",
            "Step: 383800, Loss: 403.2486267089844, Accuracy: 0.8671875\n",
            "Step: 383900, Loss: 398.4481201171875, Accuracy: 0.890625\n",
            "Step: 384000, Loss: 393.55072021484375, Accuracy: 0.9140625\n",
            "Step: 384100, Loss: 397.053466796875, Accuracy: 0.8828125\n",
            "Step: 384200, Loss: 401.16802978515625, Accuracy: 0.8828125\n",
            "Step: 384300, Loss: 394.5753173828125, Accuracy: 0.90625\n",
            "Step: 384400, Loss: 396.41009521484375, Accuracy: 0.89453125\n",
            "Step: 384500, Loss: 405.25933837890625, Accuracy: 0.84375\n",
            "Step: 384600, Loss: 396.7216796875, Accuracy: 0.89453125\n",
            "Step: 384700, Loss: 396.397216796875, Accuracy: 0.90234375\n",
            "Step: 384800, Loss: 405.4989318847656, Accuracy: 0.85546875\n",
            "Step: 384900, Loss: 396.4416809082031, Accuracy: 0.890625\n",
            "Step: 385000, Loss: 401.2530212402344, Accuracy: 0.87109375\n",
            "Step: 385100, Loss: 398.61859130859375, Accuracy: 0.8828125\n",
            "Step: 385200, Loss: 396.1484375, Accuracy: 0.8984375\n",
            "Step: 385300, Loss: 397.6288146972656, Accuracy: 0.89453125\n",
            "Step: 385400, Loss: 400.358642578125, Accuracy: 0.8828125\n",
            "Step: 385500, Loss: 407.406982421875, Accuracy: 0.8359375\n",
            "Step: 385600, Loss: 401.3706970214844, Accuracy: 0.8828125\n",
            "Step: 385700, Loss: 401.4595642089844, Accuracy: 0.8671875\n",
            "Step: 385800, Loss: 395.4071350097656, Accuracy: 0.8984375\n",
            "Step: 385900, Loss: 398.92669677734375, Accuracy: 0.87109375\n",
            "Step: 386000, Loss: 403.7384033203125, Accuracy: 0.87109375\n",
            "Step: 386100, Loss: 398.3938293457031, Accuracy: 0.890625\n",
            "Step: 386200, Loss: 408.335205078125, Accuracy: 0.83984375\n",
            "Step: 386300, Loss: 398.76007080078125, Accuracy: 0.89453125\n",
            "Step: 386400, Loss: 399.229248046875, Accuracy: 0.87890625\n",
            "Step: 386500, Loss: 400.9250793457031, Accuracy: 0.8671875\n",
            "Step: 386600, Loss: 400.8699645996094, Accuracy: 0.875\n",
            "Step: 386700, Loss: 396.2139892578125, Accuracy: 0.89453125\n",
            "Step: 386800, Loss: 398.9495544433594, Accuracy: 0.88671875\n",
            "Step: 386900, Loss: 405.07177734375, Accuracy: 0.85546875\n",
            "Step: 387000, Loss: 401.71795654296875, Accuracy: 0.8671875\n",
            "Step: 387100, Loss: 397.61395263671875, Accuracy: 0.88671875\n",
            "Step: 387200, Loss: 396.1729736328125, Accuracy: 0.8984375\n",
            "Step: 387300, Loss: 401.8248291015625, Accuracy: 0.86328125\n",
            "Step: 387400, Loss: 399.0281982421875, Accuracy: 0.8828125\n",
            "Step: 387500, Loss: 392.65557861328125, Accuracy: 0.90625\n",
            "Step: 387600, Loss: 401.3855285644531, Accuracy: 0.87890625\n",
            "Step: 387700, Loss: 400.06475830078125, Accuracy: 0.8828125\n",
            "Step: 387800, Loss: 396.6838684082031, Accuracy: 0.89453125\n",
            "Step: 387900, Loss: 396.9581298828125, Accuracy: 0.90234375\n",
            "Step: 388000, Loss: 397.8598937988281, Accuracy: 0.87890625\n",
            "Step: 388100, Loss: 400.6829833984375, Accuracy: 0.88671875\n",
            "Step: 388200, Loss: 404.7955322265625, Accuracy: 0.8671875\n",
            "Step: 388300, Loss: 401.2537841796875, Accuracy: 0.8828125\n",
            "Step: 388400, Loss: 398.38232421875, Accuracy: 0.890625\n",
            "Step: 388500, Loss: 389.2135009765625, Accuracy: 0.9296875\n",
            "Step: 388600, Loss: 398.0091857910156, Accuracy: 0.890625\n",
            "Step: 388700, Loss: 402.05584716796875, Accuracy: 0.86328125\n",
            "Step: 388800, Loss: 398.4119873046875, Accuracy: 0.88671875\n",
            "Step: 388900, Loss: 405.4297180175781, Accuracy: 0.84375\n",
            "Step: 389000, Loss: 401.8650817871094, Accuracy: 0.87890625\n",
            "Step: 389100, Loss: 397.9727783203125, Accuracy: 0.90234375\n",
            "Step: 389200, Loss: 401.74822998046875, Accuracy: 0.87109375\n",
            "Step: 389300, Loss: 397.1190490722656, Accuracy: 0.89453125\n",
            "Step: 389400, Loss: 405.36474609375, Accuracy: 0.859375\n",
            "Step: 389500, Loss: 401.2962341308594, Accuracy: 0.87890625\n",
            "Step: 389600, Loss: 397.6002197265625, Accuracy: 0.88671875\n",
            "Step: 389700, Loss: 403.62628173828125, Accuracy: 0.8671875\n",
            "Step: 389800, Loss: 395.8319091796875, Accuracy: 0.89453125\n",
            "Step: 389900, Loss: 409.22314453125, Accuracy: 0.84765625\n",
            "Step: 390000, Loss: 394.7031555175781, Accuracy: 0.8984375\n",
            "Step: 390100, Loss: 396.2246398925781, Accuracy: 0.8984375\n",
            "Step: 390200, Loss: 397.2100830078125, Accuracy: 0.890625\n",
            "Step: 390300, Loss: 399.31353759765625, Accuracy: 0.890625\n",
            "Step: 390400, Loss: 396.28826904296875, Accuracy: 0.890625\n",
            "Step: 390500, Loss: 400.7164306640625, Accuracy: 0.875\n",
            "Step: 390600, Loss: 397.3551940917969, Accuracy: 0.89453125\n",
            "Step: 390700, Loss: 400.0194396972656, Accuracy: 0.87890625\n",
            "Step: 390800, Loss: 403.1493225097656, Accuracy: 0.86328125\n",
            "Step: 390900, Loss: 399.5625, Accuracy: 0.890625\n",
            "Step: 391000, Loss: 404.63140869140625, Accuracy: 0.859375\n",
            "Step: 391100, Loss: 395.97784423828125, Accuracy: 0.90234375\n",
            "Step: 391200, Loss: 399.6836242675781, Accuracy: 0.87890625\n",
            "Step: 391300, Loss: 395.2135314941406, Accuracy: 0.90234375\n",
            "Step: 391400, Loss: 394.22894287109375, Accuracy: 0.91015625\n",
            "Step: 391500, Loss: 402.7683410644531, Accuracy: 0.8671875\n",
            "Step: 391600, Loss: 397.716796875, Accuracy: 0.8828125\n",
            "Step: 391700, Loss: 409.982421875, Accuracy: 0.83203125\n",
            "Step: 391800, Loss: 407.980712890625, Accuracy: 0.83984375\n",
            "Step: 391900, Loss: 398.4320068359375, Accuracy: 0.890625\n",
            "Step: 392000, Loss: 400.24151611328125, Accuracy: 0.890625\n",
            "Step: 392100, Loss: 399.2513427734375, Accuracy: 0.875\n",
            "Step: 392200, Loss: 400.937255859375, Accuracy: 0.875\n",
            "Step: 392300, Loss: 396.8194580078125, Accuracy: 0.89453125\n",
            "Step: 392400, Loss: 399.88861083984375, Accuracy: 0.88671875\n",
            "Step: 392500, Loss: 403.1253662109375, Accuracy: 0.8671875\n",
            "Step: 392600, Loss: 404.9130554199219, Accuracy: 0.86328125\n",
            "Step: 392700, Loss: 393.4989013671875, Accuracy: 0.91015625\n",
            "Step: 392800, Loss: 403.33868408203125, Accuracy: 0.8671875\n",
            "Step: 392900, Loss: 399.6793212890625, Accuracy: 0.8828125\n",
            "Step: 393000, Loss: 405.8455810546875, Accuracy: 0.859375\n",
            "Step: 393100, Loss: 395.97784423828125, Accuracy: 0.87890625\n",
            "Step: 393200, Loss: 404.9128723144531, Accuracy: 0.86328125\n",
            "Step: 393300, Loss: 407.9969482421875, Accuracy: 0.84765625\n",
            "Step: 393400, Loss: 397.02532958984375, Accuracy: 0.89453125\n",
            "Step: 393500, Loss: 394.123779296875, Accuracy: 0.91015625\n",
            "Step: 393600, Loss: 403.42852783203125, Accuracy: 0.859375\n",
            "Step: 393700, Loss: 399.32818603515625, Accuracy: 0.87890625\n",
            "Step: 393800, Loss: 400.0182800292969, Accuracy: 0.88671875\n",
            "Step: 393900, Loss: 398.9522705078125, Accuracy: 0.8828125\n",
            "Step: 394000, Loss: 396.6682434082031, Accuracy: 0.89453125\n",
            "Step: 394100, Loss: 396.88702392578125, Accuracy: 0.89453125\n",
            "Step: 394200, Loss: 401.35736083984375, Accuracy: 0.890625\n",
            "Step: 394300, Loss: 403.69476318359375, Accuracy: 0.859375\n",
            "Step: 394400, Loss: 402.6859130859375, Accuracy: 0.87109375\n",
            "Step: 394500, Loss: 392.52117919921875, Accuracy: 0.91796875\n",
            "Step: 394600, Loss: 401.7921447753906, Accuracy: 0.87109375\n",
            "Step: 394700, Loss: 402.071533203125, Accuracy: 0.875\n",
            "Step: 394800, Loss: 398.0011291503906, Accuracy: 0.890625\n",
            "Step: 394900, Loss: 396.10870361328125, Accuracy: 0.89453125\n",
            "Step: 395000, Loss: 401.3738708496094, Accuracy: 0.87109375\n",
            "Step: 395100, Loss: 404.70391845703125, Accuracy: 0.86328125\n",
            "Step: 395200, Loss: 405.46533203125, Accuracy: 0.859375\n",
            "Step: 395300, Loss: 404.19305419921875, Accuracy: 0.85546875\n",
            "Step: 395400, Loss: 404.3398132324219, Accuracy: 0.86328125\n",
            "Step: 395500, Loss: 397.0391845703125, Accuracy: 0.8984375\n",
            "Step: 395600, Loss: 401.2020263671875, Accuracy: 0.87109375\n",
            "Step: 395700, Loss: 399.87127685546875, Accuracy: 0.8828125\n",
            "Step: 395800, Loss: 396.022705078125, Accuracy: 0.89453125\n",
            "Step: 395900, Loss: 396.5732421875, Accuracy: 0.89453125\n",
            "Step: 396000, Loss: 392.9790344238281, Accuracy: 0.91015625\n",
            "Step: 396100, Loss: 400.32183837890625, Accuracy: 0.87890625\n",
            "Step: 396200, Loss: 397.50323486328125, Accuracy: 0.88671875\n",
            "Step: 396300, Loss: 411.9964599609375, Accuracy: 0.82421875\n",
            "Step: 396400, Loss: 395.152099609375, Accuracy: 0.8984375\n",
            "Step: 396500, Loss: 391.8790588378906, Accuracy: 0.91796875\n",
            "Step: 396600, Loss: 400.91448974609375, Accuracy: 0.875\n",
            "Step: 396700, Loss: 394.64544677734375, Accuracy: 0.90625\n",
            "Step: 396800, Loss: 395.4281921386719, Accuracy: 0.91015625\n",
            "Step: 396900, Loss: 400.11181640625, Accuracy: 0.88671875\n",
            "Step: 397000, Loss: 398.8658447265625, Accuracy: 0.88671875\n",
            "Step: 397100, Loss: 393.405029296875, Accuracy: 0.9140625\n",
            "Step: 397200, Loss: 400.1183166503906, Accuracy: 0.87890625\n",
            "Step: 397300, Loss: 396.98577880859375, Accuracy: 0.90234375\n",
            "Step: 397400, Loss: 392.4397277832031, Accuracy: 0.91015625\n",
            "Step: 397500, Loss: 397.06109619140625, Accuracy: 0.890625\n",
            "Step: 397600, Loss: 399.3631591796875, Accuracy: 0.890625\n",
            "Step: 397700, Loss: 398.7032165527344, Accuracy: 0.88671875\n",
            "Step: 397800, Loss: 401.0585632324219, Accuracy: 0.87109375\n",
            "Step: 397900, Loss: 400.5909423828125, Accuracy: 0.875\n",
            "Step: 398000, Loss: 400.07470703125, Accuracy: 0.890625\n",
            "Step: 398100, Loss: 404.6256408691406, Accuracy: 0.8671875\n",
            "Step: 398200, Loss: 395.78131103515625, Accuracy: 0.90234375\n",
            "Step: 398300, Loss: 404.5728454589844, Accuracy: 0.8671875\n",
            "Step: 398400, Loss: 396.7760925292969, Accuracy: 0.89453125\n",
            "Step: 398500, Loss: 405.33148193359375, Accuracy: 0.86328125\n",
            "Step: 398600, Loss: 397.46923828125, Accuracy: 0.88671875\n",
            "Step: 398700, Loss: 399.15582275390625, Accuracy: 0.875\n",
            "Step: 398800, Loss: 397.46063232421875, Accuracy: 0.8984375\n",
            "Step: 398900, Loss: 403.90374755859375, Accuracy: 0.86328125\n",
            "Step: 399000, Loss: 398.44390869140625, Accuracy: 0.890625\n",
            "Step: 399100, Loss: 399.7398376464844, Accuracy: 0.8828125\n",
            "Step: 399200, Loss: 400.4148254394531, Accuracy: 0.8671875\n",
            "Step: 399300, Loss: 399.4202880859375, Accuracy: 0.88671875\n",
            "Step: 399400, Loss: 399.2342529296875, Accuracy: 0.8828125\n",
            "Step: 399500, Loss: 403.03125, Accuracy: 0.8671875\n",
            "Step: 399600, Loss: 392.90692138671875, Accuracy: 0.921875\n",
            "Step: 399700, Loss: 397.98052978515625, Accuracy: 0.88671875\n",
            "Step: 399800, Loss: 394.88250732421875, Accuracy: 0.90625\n",
            "Step: 399900, Loss: 398.0699768066406, Accuracy: 0.890625\n",
            "Step: 400000, Loss: 401.4202880859375, Accuracy: 0.87109375\n",
            "Step: 400100, Loss: 399.979736328125, Accuracy: 0.88671875\n",
            "Step: 400200, Loss: 401.1561584472656, Accuracy: 0.87109375\n",
            "Step: 400300, Loss: 387.9752197265625, Accuracy: 0.9375\n",
            "Step: 400400, Loss: 403.4079284667969, Accuracy: 0.8828125\n",
            "Step: 400500, Loss: 396.7725830078125, Accuracy: 0.8984375\n",
            "Step: 400600, Loss: 397.81005859375, Accuracy: 0.8984375\n",
            "Step: 400700, Loss: 398.549560546875, Accuracy: 0.88671875\n",
            "Step: 400800, Loss: 400.38177490234375, Accuracy: 0.875\n",
            "Step: 400900, Loss: 393.08538818359375, Accuracy: 0.9140625\n",
            "Step: 401000, Loss: 395.18389892578125, Accuracy: 0.90234375\n",
            "Step: 401100, Loss: 394.8304138183594, Accuracy: 0.91015625\n",
            "Step: 401200, Loss: 394.7103271484375, Accuracy: 0.91015625\n",
            "Step: 401300, Loss: 398.5579833984375, Accuracy: 0.890625\n",
            "Step: 401400, Loss: 399.38275146484375, Accuracy: 0.875\n",
            "Step: 401500, Loss: 399.993408203125, Accuracy: 0.8828125\n",
            "Step: 401600, Loss: 400.110107421875, Accuracy: 0.86328125\n",
            "Step: 401700, Loss: 399.1458740234375, Accuracy: 0.87109375\n",
            "Step: 401800, Loss: 402.0620422363281, Accuracy: 0.8671875\n",
            "Step: 401900, Loss: 397.71234130859375, Accuracy: 0.89453125\n",
            "Step: 402000, Loss: 402.5807800292969, Accuracy: 0.875\n",
            "Step: 402100, Loss: 392.6537170410156, Accuracy: 0.9140625\n",
            "Step: 402200, Loss: 403.1102294921875, Accuracy: 0.87109375\n",
            "Step: 402300, Loss: 399.8942565917969, Accuracy: 0.890625\n",
            "Step: 402400, Loss: 402.0664367675781, Accuracy: 0.87109375\n",
            "Step: 402500, Loss: 396.380615234375, Accuracy: 0.8984375\n",
            "Step: 402600, Loss: 400.4559326171875, Accuracy: 0.88671875\n",
            "Step: 402700, Loss: 397.9093017578125, Accuracy: 0.89453125\n",
            "Step: 402800, Loss: 400.34124755859375, Accuracy: 0.875\n",
            "Step: 402900, Loss: 406.904541015625, Accuracy: 0.859375\n",
            "Step: 403000, Loss: 392.7511291503906, Accuracy: 0.921875\n",
            "Step: 403100, Loss: 402.93743896484375, Accuracy: 0.8671875\n",
            "Step: 403200, Loss: 400.2835693359375, Accuracy: 0.8828125\n",
            "Step: 403300, Loss: 403.6802978515625, Accuracy: 0.87109375\n",
            "Step: 403400, Loss: 401.04510498046875, Accuracy: 0.875\n",
            "Step: 403500, Loss: 400.77288818359375, Accuracy: 0.8828125\n",
            "Step: 403600, Loss: 399.1174621582031, Accuracy: 0.88671875\n",
            "Step: 403700, Loss: 395.65545654296875, Accuracy: 0.90234375\n",
            "Step: 403800, Loss: 395.3642272949219, Accuracy: 0.8984375\n",
            "Step: 403900, Loss: 390.3027038574219, Accuracy: 0.92578125\n",
            "Step: 404000, Loss: 403.8844909667969, Accuracy: 0.85546875\n",
            "Step: 404100, Loss: 399.8944396972656, Accuracy: 0.87890625\n",
            "Step: 404200, Loss: 394.39892578125, Accuracy: 0.91015625\n",
            "Step: 404300, Loss: 395.5641784667969, Accuracy: 0.90625\n",
            "Step: 404400, Loss: 404.0143127441406, Accuracy: 0.85546875\n",
            "Step: 404500, Loss: 397.97576904296875, Accuracy: 0.90625\n",
            "Step: 404600, Loss: 399.76544189453125, Accuracy: 0.88671875\n",
            "Step: 404700, Loss: 394.436279296875, Accuracy: 0.90625\n",
            "Step: 404800, Loss: 394.4599609375, Accuracy: 0.91015625\n",
            "Step: 404900, Loss: 399.46771240234375, Accuracy: 0.8828125\n",
            "Step: 405000, Loss: 400.89129638671875, Accuracy: 0.875\n",
            "Step: 405100, Loss: 392.6842346191406, Accuracy: 0.90234375\n",
            "Step: 405200, Loss: 391.0533142089844, Accuracy: 0.9140625\n",
            "Step: 405300, Loss: 398.45159912109375, Accuracy: 0.87890625\n",
            "Step: 405400, Loss: 404.7860412597656, Accuracy: 0.87109375\n",
            "Step: 405500, Loss: 399.78936767578125, Accuracy: 0.890625\n",
            "Step: 405600, Loss: 394.00396728515625, Accuracy: 0.8984375\n",
            "Step: 405700, Loss: 399.17706298828125, Accuracy: 0.88671875\n",
            "Step: 405800, Loss: 403.3753662109375, Accuracy: 0.875\n",
            "Step: 405900, Loss: 405.70758056640625, Accuracy: 0.8515625\n",
            "Step: 406000, Loss: 393.7895812988281, Accuracy: 0.90625\n",
            "Step: 406100, Loss: 398.8428649902344, Accuracy: 0.8984375\n",
            "Step: 406200, Loss: 394.8238525390625, Accuracy: 0.90625\n",
            "Step: 406300, Loss: 397.00262451171875, Accuracy: 0.8984375\n",
            "Step: 406400, Loss: 403.7305908203125, Accuracy: 0.87109375\n",
            "Step: 406500, Loss: 395.453125, Accuracy: 0.90234375\n",
            "Step: 406600, Loss: 404.16546630859375, Accuracy: 0.859375\n",
            "Step: 406700, Loss: 404.92572021484375, Accuracy: 0.8671875\n",
            "Step: 406800, Loss: 402.65936279296875, Accuracy: 0.87890625\n",
            "Step: 406900, Loss: 394.3886413574219, Accuracy: 0.91015625\n",
            "Step: 407000, Loss: 404.35797119140625, Accuracy: 0.875\n",
            "Step: 407100, Loss: 405.24029541015625, Accuracy: 0.859375\n",
            "Step: 407200, Loss: 402.7707824707031, Accuracy: 0.87109375\n",
            "Step: 407300, Loss: 398.960693359375, Accuracy: 0.88671875\n",
            "Step: 407400, Loss: 397.50909423828125, Accuracy: 0.890625\n",
            "Step: 407500, Loss: 406.7130432128906, Accuracy: 0.84375\n",
            "Step: 407600, Loss: 398.21270751953125, Accuracy: 0.890625\n",
            "Step: 407700, Loss: 392.940673828125, Accuracy: 0.90625\n",
            "Step: 407800, Loss: 396.29718017578125, Accuracy: 0.90234375\n",
            "Step: 407900, Loss: 398.5638732910156, Accuracy: 0.8984375\n",
            "Step: 408000, Loss: 401.6041259765625, Accuracy: 0.87890625\n",
            "Step: 408100, Loss: 406.032470703125, Accuracy: 0.85546875\n",
            "Step: 408200, Loss: 394.9963073730469, Accuracy: 0.90625\n",
            "Step: 408300, Loss: 400.5708312988281, Accuracy: 0.875\n",
            "Step: 408400, Loss: 402.4775695800781, Accuracy: 0.87109375\n",
            "Step: 408500, Loss: 408.824951171875, Accuracy: 0.8515625\n",
            "Step: 408600, Loss: 400.73681640625, Accuracy: 0.87890625\n",
            "Step: 408700, Loss: 407.79400634765625, Accuracy: 0.83984375\n",
            "Step: 408800, Loss: 397.36956787109375, Accuracy: 0.890625\n",
            "Step: 408900, Loss: 395.46453857421875, Accuracy: 0.8984375\n",
            "Step: 409000, Loss: 397.081787109375, Accuracy: 0.88671875\n",
            "Step: 409100, Loss: 403.8747253417969, Accuracy: 0.87109375\n",
            "Step: 409200, Loss: 393.5369873046875, Accuracy: 0.91796875\n",
            "Step: 409300, Loss: 399.60540771484375, Accuracy: 0.875\n",
            "Step: 409400, Loss: 402.0773010253906, Accuracy: 0.8828125\n",
            "Step: 409500, Loss: 401.50799560546875, Accuracy: 0.87890625\n",
            "Step: 409600, Loss: 401.72723388671875, Accuracy: 0.87890625\n",
            "Step: 409700, Loss: 397.9034118652344, Accuracy: 0.89453125\n",
            "Step: 409800, Loss: 406.22442626953125, Accuracy: 0.85546875\n",
            "Step: 409900, Loss: 396.9652099609375, Accuracy: 0.89453125\n",
            "Step: 410000, Loss: 401.34405517578125, Accuracy: 0.8671875\n",
            "Step: 410100, Loss: 401.3212890625, Accuracy: 0.88671875\n",
            "Step: 410200, Loss: 401.62530517578125, Accuracy: 0.87890625\n",
            "Step: 410300, Loss: 398.19427490234375, Accuracy: 0.890625\n",
            "Step: 410400, Loss: 401.7777404785156, Accuracy: 0.87890625\n",
            "Step: 410500, Loss: 406.8164367675781, Accuracy: 0.8515625\n",
            "Step: 410600, Loss: 408.72332763671875, Accuracy: 0.84375\n",
            "Step: 410700, Loss: 402.3018798828125, Accuracy: 0.8671875\n",
            "Step: 410800, Loss: 403.6253662109375, Accuracy: 0.859375\n",
            "Step: 410900, Loss: 396.17236328125, Accuracy: 0.8984375\n",
            "Step: 411000, Loss: 400.4490051269531, Accuracy: 0.875\n",
            "Step: 411100, Loss: 397.79296875, Accuracy: 0.90234375\n",
            "Step: 411200, Loss: 392.91558837890625, Accuracy: 0.91796875\n",
            "Step: 411300, Loss: 393.65484619140625, Accuracy: 0.91015625\n",
            "Step: 411400, Loss: 403.70037841796875, Accuracy: 0.86328125\n",
            "Step: 411500, Loss: 397.7281799316406, Accuracy: 0.88671875\n",
            "Step: 411600, Loss: 399.60650634765625, Accuracy: 0.8828125\n",
            "Step: 411700, Loss: 402.98065185546875, Accuracy: 0.87109375\n",
            "Step: 411800, Loss: 392.8282470703125, Accuracy: 0.921875\n",
            "Step: 411900, Loss: 406.6012268066406, Accuracy: 0.86328125\n",
            "Step: 412000, Loss: 393.84576416015625, Accuracy: 0.90234375\n",
            "Step: 412100, Loss: 403.705322265625, Accuracy: 0.859375\n",
            "Step: 412200, Loss: 401.55340576171875, Accuracy: 0.87109375\n",
            "Step: 412300, Loss: 398.45977783203125, Accuracy: 0.8828125\n",
            "Step: 412400, Loss: 407.53955078125, Accuracy: 0.84765625\n",
            "Step: 412500, Loss: 394.1073913574219, Accuracy: 0.90625\n",
            "Step: 412600, Loss: 397.8019714355469, Accuracy: 0.89453125\n",
            "Step: 412700, Loss: 399.518798828125, Accuracy: 0.87890625\n",
            "Step: 412800, Loss: 400.04278564453125, Accuracy: 0.87890625\n",
            "Step: 412900, Loss: 396.0599365234375, Accuracy: 0.89453125\n",
            "Step: 413000, Loss: 401.6324157714844, Accuracy: 0.87109375\n",
            "Step: 413100, Loss: 400.28887939453125, Accuracy: 0.88671875\n",
            "Step: 413200, Loss: 391.2115478515625, Accuracy: 0.91796875\n",
            "Step: 413300, Loss: 398.05517578125, Accuracy: 0.89453125\n",
            "Step: 413400, Loss: 390.8935546875, Accuracy: 0.921875\n",
            "Step: 413500, Loss: 398.6710205078125, Accuracy: 0.890625\n",
            "Step: 413600, Loss: 402.4579162597656, Accuracy: 0.8671875\n",
            "Step: 413700, Loss: 405.3725891113281, Accuracy: 0.859375\n",
            "Step: 413800, Loss: 397.92572021484375, Accuracy: 0.890625\n",
            "Step: 413900, Loss: 397.469970703125, Accuracy: 0.8828125\n",
            "Step: 414000, Loss: 398.4635314941406, Accuracy: 0.8828125\n",
            "Step: 414100, Loss: 402.75244140625, Accuracy: 0.875\n",
            "Step: 414200, Loss: 403.4162292480469, Accuracy: 0.8671875\n",
            "Step: 414300, Loss: 400.3478088378906, Accuracy: 0.87890625\n",
            "Step: 414400, Loss: 401.41619873046875, Accuracy: 0.8828125\n",
            "Step: 414500, Loss: 402.3175048828125, Accuracy: 0.86328125\n",
            "Step: 414600, Loss: 389.9820861816406, Accuracy: 0.93359375\n",
            "Step: 414700, Loss: 391.59454345703125, Accuracy: 0.9140625\n",
            "Step: 414800, Loss: 396.00274658203125, Accuracy: 0.8984375\n",
            "Step: 414900, Loss: 397.87548828125, Accuracy: 0.87890625\n",
            "Step: 415000, Loss: 392.1929931640625, Accuracy: 0.91796875\n",
            "Step: 415100, Loss: 396.4959411621094, Accuracy: 0.8984375\n",
            "Step: 415200, Loss: 400.521240234375, Accuracy: 0.87890625\n",
            "Step: 415300, Loss: 395.5438232421875, Accuracy: 0.90234375\n",
            "Step: 415400, Loss: 398.194580078125, Accuracy: 0.88671875\n",
            "Step: 415500, Loss: 402.48431396484375, Accuracy: 0.8671875\n",
            "Step: 415600, Loss: 401.95904541015625, Accuracy: 0.87109375\n",
            "Step: 415700, Loss: 404.85345458984375, Accuracy: 0.859375\n",
            "Step: 415800, Loss: 401.28033447265625, Accuracy: 0.87890625\n",
            "Step: 415900, Loss: 399.43695068359375, Accuracy: 0.88671875\n",
            "Step: 416000, Loss: 406.37738037109375, Accuracy: 0.8515625\n",
            "Step: 416100, Loss: 400.00946044921875, Accuracy: 0.875\n",
            "Step: 416200, Loss: 402.92730712890625, Accuracy: 0.8671875\n",
            "Step: 416300, Loss: 401.98236083984375, Accuracy: 0.87890625\n",
            "Step: 416400, Loss: 405.8487548828125, Accuracy: 0.8515625\n",
            "Step: 416500, Loss: 398.13568115234375, Accuracy: 0.87890625\n",
            "Step: 416600, Loss: 393.9073486328125, Accuracy: 0.91015625\n",
            "Step: 416700, Loss: 397.75872802734375, Accuracy: 0.89453125\n",
            "Step: 416800, Loss: 400.9469909667969, Accuracy: 0.875\n",
            "Step: 416900, Loss: 405.0712890625, Accuracy: 0.84765625\n",
            "Step: 417000, Loss: 396.0848083496094, Accuracy: 0.90625\n",
            "Step: 417100, Loss: 402.2607116699219, Accuracy: 0.87109375\n",
            "Step: 417200, Loss: 406.1846008300781, Accuracy: 0.8515625\n",
            "Step: 417300, Loss: 397.1781005859375, Accuracy: 0.8984375\n",
            "Step: 417400, Loss: 391.0546875, Accuracy: 0.92578125\n",
            "Step: 417500, Loss: 397.953369140625, Accuracy: 0.890625\n",
            "Step: 417600, Loss: 397.48193359375, Accuracy: 0.88671875\n",
            "Step: 417700, Loss: 402.063720703125, Accuracy: 0.87109375\n",
            "Step: 417800, Loss: 394.6888732910156, Accuracy: 0.90234375\n",
            "Step: 417900, Loss: 408.5161437988281, Accuracy: 0.83984375\n",
            "Step: 418000, Loss: 400.1912841796875, Accuracy: 0.88671875\n",
            "Step: 418100, Loss: 398.8721618652344, Accuracy: 0.8828125\n",
            "Step: 418200, Loss: 399.1004638671875, Accuracy: 0.890625\n",
            "Step: 418300, Loss: 401.0526123046875, Accuracy: 0.87890625\n",
            "Step: 418400, Loss: 396.53045654296875, Accuracy: 0.8984375\n",
            "Step: 418500, Loss: 401.8194580078125, Accuracy: 0.87109375\n",
            "Step: 418600, Loss: 398.6029052734375, Accuracy: 0.890625\n",
            "Step: 418700, Loss: 401.27105712890625, Accuracy: 0.87109375\n",
            "Step: 418800, Loss: 399.48565673828125, Accuracy: 0.8828125\n",
            "Step: 418900, Loss: 403.853759765625, Accuracy: 0.859375\n",
            "Step: 419000, Loss: 405.07330322265625, Accuracy: 0.875\n",
            "Step: 419100, Loss: 397.573486328125, Accuracy: 0.89453125\n",
            "Step: 419200, Loss: 404.0032043457031, Accuracy: 0.86328125\n",
            "Step: 419300, Loss: 407.796142578125, Accuracy: 0.8515625\n",
            "Step: 419400, Loss: 396.0932312011719, Accuracy: 0.91796875\n",
            "Step: 419500, Loss: 401.3424377441406, Accuracy: 0.8828125\n",
            "Step: 419600, Loss: 398.28448486328125, Accuracy: 0.88671875\n",
            "Step: 419700, Loss: 407.8116760253906, Accuracy: 0.84375\n",
            "Step: 419800, Loss: 410.9787292480469, Accuracy: 0.83203125\n",
            "Step: 419900, Loss: 395.424560546875, Accuracy: 0.8984375\n",
            "Step: 420000, Loss: 393.9460754394531, Accuracy: 0.91015625\n",
            "Step: 420100, Loss: 409.9010009765625, Accuracy: 0.83203125\n",
            "Step: 420200, Loss: 401.0345458984375, Accuracy: 0.875\n",
            "Step: 420300, Loss: 399.03997802734375, Accuracy: 0.8828125\n",
            "Step: 420400, Loss: 401.7939453125, Accuracy: 0.87890625\n",
            "Step: 420500, Loss: 403.6622009277344, Accuracy: 0.8671875\n",
            "Step: 420600, Loss: 398.8704528808594, Accuracy: 0.88671875\n",
            "Step: 420700, Loss: 400.07574462890625, Accuracy: 0.87890625\n",
            "Step: 420800, Loss: 399.6849670410156, Accuracy: 0.8828125\n",
            "Step: 420900, Loss: 406.819091796875, Accuracy: 0.84375\n",
            "Step: 421000, Loss: 398.27685546875, Accuracy: 0.875\n",
            "Step: 421100, Loss: 400.274658203125, Accuracy: 0.88671875\n",
            "Step: 421200, Loss: 402.77520751953125, Accuracy: 0.87890625\n",
            "Step: 421300, Loss: 403.1051025390625, Accuracy: 0.875\n",
            "Step: 421400, Loss: 394.6380615234375, Accuracy: 0.90234375\n",
            "Step: 421500, Loss: 401.639892578125, Accuracy: 0.8671875\n",
            "Step: 421600, Loss: 401.5191650390625, Accuracy: 0.87109375\n",
            "Step: 421700, Loss: 398.4580078125, Accuracy: 0.88671875\n",
            "Step: 421800, Loss: 404.83013916015625, Accuracy: 0.86328125\n",
            "Step: 421900, Loss: 397.12103271484375, Accuracy: 0.8984375\n",
            "Step: 422000, Loss: 393.3642272949219, Accuracy: 0.91015625\n",
            "Step: 422100, Loss: 401.2555236816406, Accuracy: 0.875\n",
            "Step: 422200, Loss: 405.2702331542969, Accuracy: 0.859375\n",
            "Step: 422300, Loss: 399.567626953125, Accuracy: 0.87890625\n",
            "Step: 422400, Loss: 396.404052734375, Accuracy: 0.8984375\n",
            "Step: 422500, Loss: 401.9057922363281, Accuracy: 0.8671875\n",
            "Step: 422600, Loss: 404.45660400390625, Accuracy: 0.86328125\n",
            "Step: 422700, Loss: 399.1769104003906, Accuracy: 0.88671875\n",
            "Step: 422800, Loss: 395.79071044921875, Accuracy: 0.90234375\n",
            "Step: 422900, Loss: 397.8831787109375, Accuracy: 0.890625\n",
            "Step: 423000, Loss: 399.18218994140625, Accuracy: 0.87890625\n",
            "Step: 423100, Loss: 398.638671875, Accuracy: 0.890625\n",
            "Step: 423200, Loss: 403.43048095703125, Accuracy: 0.87109375\n",
            "Step: 423300, Loss: 401.09393310546875, Accuracy: 0.875\n",
            "Step: 423400, Loss: 396.312255859375, Accuracy: 0.890625\n",
            "Step: 423500, Loss: 399.99285888671875, Accuracy: 0.88671875\n",
            "Step: 423600, Loss: 392.88067626953125, Accuracy: 0.9140625\n",
            "Step: 423700, Loss: 404.7213134765625, Accuracy: 0.859375\n",
            "Step: 423800, Loss: 400.39410400390625, Accuracy: 0.87890625\n",
            "Step: 423900, Loss: 405.3838195800781, Accuracy: 0.8515625\n",
            "Step: 424000, Loss: 393.2596740722656, Accuracy: 0.91015625\n",
            "Step: 424100, Loss: 406.2226867675781, Accuracy: 0.85546875\n",
            "Step: 424200, Loss: 390.1319580078125, Accuracy: 0.9296875\n",
            "Step: 424300, Loss: 396.53948974609375, Accuracy: 0.90234375\n",
            "Step: 424400, Loss: 394.22344970703125, Accuracy: 0.91015625\n",
            "Step: 424500, Loss: 394.1574401855469, Accuracy: 0.91015625\n",
            "Step: 424600, Loss: 396.54620361328125, Accuracy: 0.89453125\n",
            "Step: 424700, Loss: 398.34259033203125, Accuracy: 0.89453125\n",
            "Step: 424800, Loss: 394.1539306640625, Accuracy: 0.9140625\n",
            "Step: 424900, Loss: 399.0872802734375, Accuracy: 0.8828125\n",
            "Step: 425000, Loss: 402.0919189453125, Accuracy: 0.859375\n",
            "Step: 425100, Loss: 405.86846923828125, Accuracy: 0.859375\n",
            "Step: 425200, Loss: 403.203369140625, Accuracy: 0.8671875\n",
            "Step: 425300, Loss: 395.4289855957031, Accuracy: 0.91796875\n",
            "Step: 425400, Loss: 401.8152770996094, Accuracy: 0.87109375\n",
            "Step: 425500, Loss: 397.1346435546875, Accuracy: 0.890625\n",
            "Step: 425600, Loss: 402.659912109375, Accuracy: 0.87109375\n",
            "Step: 425700, Loss: 398.72509765625, Accuracy: 0.8828125\n",
            "Step: 425800, Loss: 397.1299133300781, Accuracy: 0.88671875\n",
            "Step: 425900, Loss: 401.0555419921875, Accuracy: 0.875\n",
            "Step: 426000, Loss: 394.37567138671875, Accuracy: 0.9140625\n",
            "Step: 426100, Loss: 402.775634765625, Accuracy: 0.87109375\n",
            "Step: 426200, Loss: 392.46282958984375, Accuracy: 0.9140625\n",
            "Step: 426300, Loss: 395.51873779296875, Accuracy: 0.90234375\n",
            "Step: 426400, Loss: 400.0278015136719, Accuracy: 0.87890625\n",
            "Step: 426500, Loss: 400.72247314453125, Accuracy: 0.875\n",
            "Step: 426600, Loss: 405.88336181640625, Accuracy: 0.85546875\n",
            "Step: 426700, Loss: 400.9097900390625, Accuracy: 0.87109375\n",
            "Step: 426800, Loss: 406.7337951660156, Accuracy: 0.84765625\n",
            "Step: 426900, Loss: 400.052490234375, Accuracy: 0.87890625\n",
            "Step: 427000, Loss: 398.74505615234375, Accuracy: 0.88671875\n",
            "Step: 427100, Loss: 398.6222229003906, Accuracy: 0.8984375\n",
            "Step: 427200, Loss: 402.1519470214844, Accuracy: 0.8671875\n",
            "Step: 427300, Loss: 399.6326904296875, Accuracy: 0.87890625\n",
            "Step: 427400, Loss: 398.2962951660156, Accuracy: 0.8828125\n",
            "Step: 427500, Loss: 401.5230712890625, Accuracy: 0.87890625\n",
            "Step: 427600, Loss: 405.8841552734375, Accuracy: 0.85546875\n",
            "Step: 427700, Loss: 403.99114990234375, Accuracy: 0.84765625\n",
            "Step: 427800, Loss: 397.9570617675781, Accuracy: 0.8984375\n",
            "Step: 427900, Loss: 401.71478271484375, Accuracy: 0.87109375\n",
            "Step: 428000, Loss: 399.3546142578125, Accuracy: 0.89453125\n",
            "Step: 428100, Loss: 394.5824279785156, Accuracy: 0.90234375\n",
            "Step: 428200, Loss: 404.29754638671875, Accuracy: 0.859375\n",
            "Step: 428300, Loss: 397.6661682128906, Accuracy: 0.890625\n",
            "Step: 428400, Loss: 404.6662292480469, Accuracy: 0.86328125\n",
            "Step: 428500, Loss: 403.2301940917969, Accuracy: 0.8671875\n",
            "Step: 428600, Loss: 395.7369384765625, Accuracy: 0.8984375\n",
            "Step: 428700, Loss: 398.2730712890625, Accuracy: 0.8828125\n",
            "Step: 428800, Loss: 400.07244873046875, Accuracy: 0.87890625\n",
            "Step: 428900, Loss: 396.52777099609375, Accuracy: 0.890625\n",
            "Step: 429000, Loss: 397.72869873046875, Accuracy: 0.89453125\n",
            "Step: 429100, Loss: 401.62384033203125, Accuracy: 0.87109375\n",
            "Step: 429200, Loss: 394.51300048828125, Accuracy: 0.9140625\n",
            "Step: 429300, Loss: 399.200927734375, Accuracy: 0.890625\n",
            "Step: 429400, Loss: 396.80914306640625, Accuracy: 0.89453125\n",
            "Step: 429500, Loss: 402.08355712890625, Accuracy: 0.87109375\n",
            "Step: 429600, Loss: 401.7757568359375, Accuracy: 0.86328125\n",
            "Step: 429700, Loss: 398.497802734375, Accuracy: 0.88671875\n",
            "Step: 429800, Loss: 400.0704650878906, Accuracy: 0.87890625\n",
            "Step: 429900, Loss: 407.0322265625, Accuracy: 0.8515625\n",
            "Step: 430000, Loss: 401.569580078125, Accuracy: 0.88671875\n",
            "Step: 430100, Loss: 394.995361328125, Accuracy: 0.91015625\n",
            "Step: 430200, Loss: 395.84552001953125, Accuracy: 0.89453125\n",
            "Step: 430300, Loss: 399.72576904296875, Accuracy: 0.875\n",
            "Step: 430400, Loss: 402.21337890625, Accuracy: 0.85546875\n",
            "Step: 430500, Loss: 397.9272155761719, Accuracy: 0.8984375\n",
            "Step: 430600, Loss: 396.13751220703125, Accuracy: 0.90234375\n",
            "Step: 430700, Loss: 402.4351501464844, Accuracy: 0.87890625\n",
            "Step: 430800, Loss: 398.1368408203125, Accuracy: 0.88671875\n",
            "Step: 430900, Loss: 393.5430908203125, Accuracy: 0.91796875\n",
            "Step: 431000, Loss: 397.7564392089844, Accuracy: 0.88671875\n",
            "Step: 431100, Loss: 399.3428649902344, Accuracy: 0.8828125\n",
            "Step: 431200, Loss: 394.441162109375, Accuracy: 0.91015625\n",
            "Step: 431300, Loss: 402.4413146972656, Accuracy: 0.87109375\n",
            "Step: 431400, Loss: 399.6140441894531, Accuracy: 0.87890625\n",
            "Step: 431500, Loss: 399.2542724609375, Accuracy: 0.88671875\n",
            "Step: 431600, Loss: 398.374755859375, Accuracy: 0.890625\n",
            "Step: 431700, Loss: 396.8079833984375, Accuracy: 0.8984375\n",
            "Step: 431800, Loss: 388.79632568359375, Accuracy: 0.93359375\n",
            "Step: 431900, Loss: 393.696044921875, Accuracy: 0.90625\n",
            "Step: 432000, Loss: 398.7794494628906, Accuracy: 0.8828125\n",
            "Step: 432100, Loss: 397.49884033203125, Accuracy: 0.89453125\n",
            "Step: 432200, Loss: 404.3648376464844, Accuracy: 0.86328125\n",
            "Step: 432300, Loss: 404.10272216796875, Accuracy: 0.8671875\n",
            "Step: 432400, Loss: 395.0787353515625, Accuracy: 0.90234375\n",
            "Step: 432500, Loss: 399.4185791015625, Accuracy: 0.890625\n",
            "Step: 432600, Loss: 398.78021240234375, Accuracy: 0.8828125\n",
            "Step: 432700, Loss: 399.5545654296875, Accuracy: 0.890625\n",
            "Step: 432800, Loss: 396.8556823730469, Accuracy: 0.8984375\n",
            "Step: 432900, Loss: 393.53521728515625, Accuracy: 0.91796875\n",
            "Step: 433000, Loss: 404.167724609375, Accuracy: 0.86328125\n",
            "Step: 433100, Loss: 400.3421630859375, Accuracy: 0.8828125\n",
            "Step: 433200, Loss: 394.3395080566406, Accuracy: 0.8984375\n",
            "Step: 433300, Loss: 396.09869384765625, Accuracy: 0.91015625\n",
            "Step: 433400, Loss: 399.7686767578125, Accuracy: 0.88671875\n",
            "Step: 433500, Loss: 394.94976806640625, Accuracy: 0.90625\n",
            "Step: 433600, Loss: 392.02099609375, Accuracy: 0.921875\n",
            "Step: 433700, Loss: 390.56256103515625, Accuracy: 0.9375\n",
            "Step: 433800, Loss: 405.7314147949219, Accuracy: 0.8515625\n",
            "Step: 433900, Loss: 398.42266845703125, Accuracy: 0.88671875\n",
            "Step: 434000, Loss: 397.059326171875, Accuracy: 0.890625\n",
            "Step: 434100, Loss: 394.9689025878906, Accuracy: 0.8984375\n",
            "Step: 434200, Loss: 403.46282958984375, Accuracy: 0.86328125\n",
            "Step: 434300, Loss: 398.4048156738281, Accuracy: 0.890625\n",
            "Step: 434400, Loss: 394.45941162109375, Accuracy: 0.8984375\n",
            "Step: 434500, Loss: 393.1490478515625, Accuracy: 0.9140625\n",
            "Step: 434600, Loss: 398.7520751953125, Accuracy: 0.8828125\n",
            "Step: 434700, Loss: 405.5038146972656, Accuracy: 0.85546875\n",
            "Step: 434800, Loss: 398.50640869140625, Accuracy: 0.8828125\n",
            "Step: 434900, Loss: 394.0325927734375, Accuracy: 0.90234375\n",
            "Step: 435000, Loss: 396.7593994140625, Accuracy: 0.8984375\n",
            "Step: 435100, Loss: 407.2986755371094, Accuracy: 0.84765625\n",
            "Step: 435200, Loss: 405.41461181640625, Accuracy: 0.85546875\n",
            "Step: 435300, Loss: 401.6212158203125, Accuracy: 0.87109375\n",
            "Step: 435400, Loss: 397.0053405761719, Accuracy: 0.89453125\n",
            "Step: 435500, Loss: 392.51519775390625, Accuracy: 0.91796875\n",
            "Step: 435600, Loss: 397.39306640625, Accuracy: 0.89453125\n",
            "Step: 435700, Loss: 395.9303283691406, Accuracy: 0.90234375\n",
            "Step: 435800, Loss: 402.34808349609375, Accuracy: 0.8671875\n",
            "Step: 435900, Loss: 401.6912841796875, Accuracy: 0.85546875\n",
            "Step: 436000, Loss: 400.03399658203125, Accuracy: 0.87890625\n",
            "Step: 436100, Loss: 406.451416015625, Accuracy: 0.8515625\n",
            "Step: 436200, Loss: 407.113037109375, Accuracy: 0.83984375\n",
            "Step: 436300, Loss: 393.9278259277344, Accuracy: 0.91015625\n",
            "Step: 436400, Loss: 401.99273681640625, Accuracy: 0.87890625\n",
            "Step: 436500, Loss: 395.056884765625, Accuracy: 0.91015625\n",
            "Step: 436600, Loss: 399.35675048828125, Accuracy: 0.890625\n",
            "Step: 436700, Loss: 402.08587646484375, Accuracy: 0.8671875\n",
            "Step: 436800, Loss: 397.0048828125, Accuracy: 0.890625\n",
            "Step: 436900, Loss: 395.909912109375, Accuracy: 0.890625\n",
            "Step: 437000, Loss: 401.6695556640625, Accuracy: 0.87890625\n",
            "Step: 437100, Loss: 406.0982666015625, Accuracy: 0.859375\n",
            "Step: 437200, Loss: 399.8539733886719, Accuracy: 0.87890625\n",
            "Step: 437300, Loss: 402.8682861328125, Accuracy: 0.8671875\n",
            "Step: 437400, Loss: 395.2947082519531, Accuracy: 0.90234375\n",
            "Step: 437500, Loss: 405.28289794921875, Accuracy: 0.859375\n",
            "Step: 437600, Loss: 399.270263671875, Accuracy: 0.875\n",
            "Step: 437700, Loss: 402.0537414550781, Accuracy: 0.8671875\n",
            "Step: 437800, Loss: 395.40179443359375, Accuracy: 0.91015625\n",
            "Step: 437900, Loss: 397.40838623046875, Accuracy: 0.8984375\n",
            "Step: 438000, Loss: 398.19989013671875, Accuracy: 0.875\n",
            "Step: 438100, Loss: 403.5003967285156, Accuracy: 0.86328125\n",
            "Step: 438200, Loss: 397.6601257324219, Accuracy: 0.88671875\n",
            "Step: 438300, Loss: 398.76300048828125, Accuracy: 0.88671875\n",
            "Step: 438400, Loss: 400.17041015625, Accuracy: 0.8671875\n",
            "Step: 438500, Loss: 398.9969482421875, Accuracy: 0.890625\n",
            "Step: 438600, Loss: 395.99188232421875, Accuracy: 0.90234375\n",
            "Step: 438700, Loss: 405.73382568359375, Accuracy: 0.859375\n",
            "Step: 438800, Loss: 397.95404052734375, Accuracy: 0.89453125\n",
            "Step: 438900, Loss: 400.092041015625, Accuracy: 0.8671875\n",
            "Step: 439000, Loss: 406.3463134765625, Accuracy: 0.859375\n",
            "Step: 439100, Loss: 398.24609375, Accuracy: 0.890625\n",
            "Step: 439200, Loss: 395.9698486328125, Accuracy: 0.8984375\n",
            "Step: 439300, Loss: 395.21099853515625, Accuracy: 0.89453125\n",
            "Step: 439400, Loss: 402.48858642578125, Accuracy: 0.8671875\n",
            "Step: 439500, Loss: 406.5687255859375, Accuracy: 0.8515625\n",
            "Step: 439600, Loss: 392.1549377441406, Accuracy: 0.921875\n",
            "Step: 439700, Loss: 400.881591796875, Accuracy: 0.875\n",
            "Step: 439800, Loss: 396.039794921875, Accuracy: 0.8984375\n",
            "Step: 439900, Loss: 399.2834777832031, Accuracy: 0.8828125\n",
            "Step: 440000, Loss: 399.4006042480469, Accuracy: 0.8984375\n",
            "Step: 440100, Loss: 396.33343505859375, Accuracy: 0.8984375\n",
            "Step: 440200, Loss: 401.4093322753906, Accuracy: 0.875\n",
            "Step: 440300, Loss: 404.6340637207031, Accuracy: 0.85546875\n",
            "Step: 440400, Loss: 395.9942626953125, Accuracy: 0.89453125\n",
            "Step: 440500, Loss: 403.507568359375, Accuracy: 0.87109375\n",
            "Step: 440600, Loss: 396.88214111328125, Accuracy: 0.89453125\n",
            "Step: 440700, Loss: 393.20294189453125, Accuracy: 0.91796875\n",
            "Step: 440800, Loss: 398.66546630859375, Accuracy: 0.8828125\n",
            "Step: 440900, Loss: 399.4927062988281, Accuracy: 0.88671875\n",
            "Step: 441000, Loss: 410.4891357421875, Accuracy: 0.828125\n",
            "Step: 441100, Loss: 394.93878173828125, Accuracy: 0.890625\n",
            "Step: 441200, Loss: 398.5276184082031, Accuracy: 0.90234375\n",
            "Step: 441300, Loss: 393.408935546875, Accuracy: 0.91796875\n",
            "Step: 441400, Loss: 402.5396728515625, Accuracy: 0.8671875\n",
            "Step: 441500, Loss: 394.2275695800781, Accuracy: 0.91015625\n",
            "Step: 441600, Loss: 398.8606872558594, Accuracy: 0.89453125\n",
            "Step: 441700, Loss: 398.44329833984375, Accuracy: 0.890625\n",
            "Step: 441800, Loss: 400.0782775878906, Accuracy: 0.8828125\n",
            "Step: 441900, Loss: 404.58367919921875, Accuracy: 0.859375\n",
            "Step: 442000, Loss: 399.1358642578125, Accuracy: 0.890625\n",
            "Step: 442100, Loss: 394.0343017578125, Accuracy: 0.9140625\n",
            "Step: 442200, Loss: 408.4828796386719, Accuracy: 0.8515625\n",
            "Step: 442300, Loss: 395.6295471191406, Accuracy: 0.9140625\n",
            "Step: 442400, Loss: 397.49053955078125, Accuracy: 0.8984375\n",
            "Step: 442500, Loss: 395.33154296875, Accuracy: 0.8984375\n",
            "Step: 442600, Loss: 403.1331787109375, Accuracy: 0.86328125\n",
            "Step: 442700, Loss: 399.51312255859375, Accuracy: 0.8828125\n",
            "Step: 442800, Loss: 406.3708801269531, Accuracy: 0.84375\n",
            "Step: 442900, Loss: 390.838134765625, Accuracy: 0.921875\n",
            "Step: 443000, Loss: 406.76324462890625, Accuracy: 0.859375\n",
            "Step: 443100, Loss: 395.1858215332031, Accuracy: 0.90234375\n",
            "Step: 443200, Loss: 405.8336486816406, Accuracy: 0.8671875\n",
            "Step: 443300, Loss: 398.3854675292969, Accuracy: 0.88671875\n",
            "Step: 443400, Loss: 397.0130920410156, Accuracy: 0.91015625\n",
            "Step: 443500, Loss: 397.73150634765625, Accuracy: 0.89453125\n",
            "Step: 443600, Loss: 389.5696105957031, Accuracy: 0.92578125\n",
            "Step: 443700, Loss: 411.7480773925781, Accuracy: 0.828125\n",
            "Step: 443800, Loss: 396.3205871582031, Accuracy: 0.89453125\n",
            "Step: 443900, Loss: 394.70721435546875, Accuracy: 0.91015625\n",
            "Step: 444000, Loss: 404.4912414550781, Accuracy: 0.87109375\n",
            "Step: 444100, Loss: 402.93414306640625, Accuracy: 0.85546875\n",
            "Step: 444200, Loss: 402.83917236328125, Accuracy: 0.8671875\n",
            "Step: 444300, Loss: 398.4169921875, Accuracy: 0.89453125\n",
            "Step: 444400, Loss: 396.47540283203125, Accuracy: 0.90625\n",
            "Step: 444500, Loss: 389.19915771484375, Accuracy: 0.92578125\n",
            "Step: 444600, Loss: 397.475830078125, Accuracy: 0.890625\n",
            "Step: 444700, Loss: 390.3423156738281, Accuracy: 0.9296875\n",
            "Step: 444800, Loss: 397.23577880859375, Accuracy: 0.890625\n",
            "Step: 444900, Loss: 401.02703857421875, Accuracy: 0.875\n",
            "Step: 445000, Loss: 400.7868347167969, Accuracy: 0.87109375\n",
            "Step: 445100, Loss: 398.5657653808594, Accuracy: 0.88671875\n",
            "Step: 445200, Loss: 398.4227294921875, Accuracy: 0.875\n",
            "Step: 445300, Loss: 395.2986755371094, Accuracy: 0.89453125\n",
            "Step: 445400, Loss: 395.130615234375, Accuracy: 0.9140625\n",
            "Step: 445500, Loss: 397.4961242675781, Accuracy: 0.890625\n",
            "Step: 445600, Loss: 400.17645263671875, Accuracy: 0.87890625\n",
            "Step: 445700, Loss: 399.40191650390625, Accuracy: 0.89453125\n",
            "Step: 445800, Loss: 399.6720275878906, Accuracy: 0.88671875\n",
            "Step: 445900, Loss: 404.8365478515625, Accuracy: 0.86328125\n",
            "Step: 446000, Loss: 395.0806884765625, Accuracy: 0.90625\n",
            "Step: 446100, Loss: 405.04547119140625, Accuracy: 0.85546875\n",
            "Step: 446200, Loss: 399.86920166015625, Accuracy: 0.87109375\n",
            "Step: 446300, Loss: 393.1956787109375, Accuracy: 0.91015625\n",
            "Step: 446400, Loss: 397.0984191894531, Accuracy: 0.890625\n",
            "Step: 446500, Loss: 399.94842529296875, Accuracy: 0.875\n",
            "Step: 446600, Loss: 396.63775634765625, Accuracy: 0.8984375\n",
            "Step: 446700, Loss: 402.3294677734375, Accuracy: 0.875\n",
            "Step: 446800, Loss: 392.01239013671875, Accuracy: 0.92578125\n",
            "Step: 446900, Loss: 399.39215087890625, Accuracy: 0.8828125\n",
            "Step: 447000, Loss: 396.39019775390625, Accuracy: 0.90234375\n",
            "Step: 447100, Loss: 401.4024658203125, Accuracy: 0.875\n",
            "Step: 447200, Loss: 406.5982666015625, Accuracy: 0.84765625\n",
            "Step: 447300, Loss: 400.52081298828125, Accuracy: 0.89453125\n",
            "Step: 447400, Loss: 401.18890380859375, Accuracy: 0.875\n",
            "Step: 447500, Loss: 400.5006103515625, Accuracy: 0.88671875\n",
            "Step: 447600, Loss: 405.02587890625, Accuracy: 0.859375\n",
            "Step: 447700, Loss: 405.38470458984375, Accuracy: 0.86328125\n",
            "Step: 447800, Loss: 394.939453125, Accuracy: 0.91015625\n",
            "Step: 447900, Loss: 395.057861328125, Accuracy: 0.90234375\n",
            "Step: 448000, Loss: 403.376953125, Accuracy: 0.875\n",
            "Step: 448100, Loss: 400.4234619140625, Accuracy: 0.8828125\n",
            "Step: 448200, Loss: 399.110595703125, Accuracy: 0.875\n",
            "Step: 448300, Loss: 406.6468200683594, Accuracy: 0.84765625\n",
            "Step: 448400, Loss: 397.81219482421875, Accuracy: 0.89453125\n",
            "Step: 448500, Loss: 403.47186279296875, Accuracy: 0.87109375\n",
            "Step: 448600, Loss: 393.64178466796875, Accuracy: 0.9140625\n",
            "Step: 448700, Loss: 399.6803894042969, Accuracy: 0.8828125\n",
            "Step: 448800, Loss: 407.4168395996094, Accuracy: 0.84375\n",
            "Step: 448900, Loss: 403.76580810546875, Accuracy: 0.86328125\n",
            "Step: 449000, Loss: 404.61602783203125, Accuracy: 0.859375\n",
            "Step: 449100, Loss: 397.4084777832031, Accuracy: 0.890625\n",
            "Step: 449200, Loss: 403.73040771484375, Accuracy: 0.86328125\n",
            "Step: 449300, Loss: 394.5830383300781, Accuracy: 0.90625\n",
            "Step: 449400, Loss: 402.323486328125, Accuracy: 0.8671875\n",
            "Step: 449500, Loss: 399.14776611328125, Accuracy: 0.890625\n",
            "Step: 449600, Loss: 398.346923828125, Accuracy: 0.8828125\n",
            "Step: 449700, Loss: 403.577880859375, Accuracy: 0.86328125\n",
            "Step: 449800, Loss: 405.8648986816406, Accuracy: 0.86328125\n",
            "Step: 449900, Loss: 393.908447265625, Accuracy: 0.9140625\n",
            "Step: 450000, Loss: 400.539794921875, Accuracy: 0.8828125\n",
            "Step: 450100, Loss: 399.24151611328125, Accuracy: 0.8984375\n",
            "Step: 450200, Loss: 397.4770202636719, Accuracy: 0.890625\n",
            "Step: 450300, Loss: 397.463623046875, Accuracy: 0.89453125\n",
            "Step: 450400, Loss: 400.4720153808594, Accuracy: 0.875\n",
            "Step: 450500, Loss: 395.3201904296875, Accuracy: 0.90234375\n",
            "Step: 450600, Loss: 401.6538391113281, Accuracy: 0.875\n",
            "Step: 450700, Loss: 391.70166015625, Accuracy: 0.9140625\n",
            "Step: 450800, Loss: 398.76129150390625, Accuracy: 0.89453125\n",
            "Step: 450900, Loss: 397.1377868652344, Accuracy: 0.89453125\n",
            "Step: 451000, Loss: 401.234375, Accuracy: 0.87109375\n",
            "Step: 451100, Loss: 398.2889709472656, Accuracy: 0.8828125\n",
            "Step: 451200, Loss: 394.17633056640625, Accuracy: 0.90625\n",
            "Step: 451300, Loss: 398.9167175292969, Accuracy: 0.8828125\n",
            "Step: 451400, Loss: 399.64599609375, Accuracy: 0.88671875\n",
            "Step: 451500, Loss: 395.9703674316406, Accuracy: 0.91015625\n",
            "Step: 451600, Loss: 402.7339782714844, Accuracy: 0.8671875\n",
            "Step: 451700, Loss: 397.9361267089844, Accuracy: 0.88671875\n",
            "Step: 451800, Loss: 398.25897216796875, Accuracy: 0.89453125\n",
            "Step: 451900, Loss: 396.2927551269531, Accuracy: 0.89453125\n",
            "Step: 452000, Loss: 395.0104675292969, Accuracy: 0.9140625\n",
            "Step: 452100, Loss: 402.8937072753906, Accuracy: 0.86328125\n",
            "Step: 452200, Loss: 401.421875, Accuracy: 0.8828125\n",
            "Step: 452300, Loss: 394.53607177734375, Accuracy: 0.90234375\n",
            "Step: 452400, Loss: 403.86785888671875, Accuracy: 0.87109375\n",
            "Step: 452500, Loss: 396.64697265625, Accuracy: 0.89453125\n",
            "Step: 452600, Loss: 406.89569091796875, Accuracy: 0.86328125\n",
            "Step: 452700, Loss: 397.38330078125, Accuracy: 0.89453125\n",
            "Step: 452800, Loss: 395.359375, Accuracy: 0.90625\n",
            "Step: 452900, Loss: 392.29876708984375, Accuracy: 0.9140625\n",
            "Step: 453000, Loss: 400.50018310546875, Accuracy: 0.88671875\n",
            "Step: 453100, Loss: 395.8729248046875, Accuracy: 0.8984375\n",
            "Step: 453200, Loss: 389.62738037109375, Accuracy: 0.921875\n",
            "Step: 453300, Loss: 404.6423034667969, Accuracy: 0.86328125\n",
            "Step: 453400, Loss: 403.490966796875, Accuracy: 0.87109375\n",
            "Step: 453500, Loss: 399.819091796875, Accuracy: 0.87890625\n",
            "Step: 453600, Loss: 394.9061279296875, Accuracy: 0.90625\n",
            "Step: 453700, Loss: 393.2831115722656, Accuracy: 0.9140625\n",
            "Step: 453800, Loss: 395.6587829589844, Accuracy: 0.90234375\n",
            "Step: 453900, Loss: 406.9363708496094, Accuracy: 0.84765625\n",
            "Step: 454000, Loss: 406.1950378417969, Accuracy: 0.86328125\n",
            "Step: 454100, Loss: 398.7259521484375, Accuracy: 0.88671875\n",
            "Step: 454200, Loss: 395.52362060546875, Accuracy: 0.90234375\n",
            "Step: 454300, Loss: 393.4198303222656, Accuracy: 0.9140625\n",
            "Step: 454400, Loss: 402.47113037109375, Accuracy: 0.8671875\n",
            "Step: 454500, Loss: 404.65509033203125, Accuracy: 0.8671875\n",
            "Step: 454600, Loss: 398.6234436035156, Accuracy: 0.89453125\n",
            "Step: 454700, Loss: 400.57025146484375, Accuracy: 0.875\n",
            "Step: 454800, Loss: 399.31060791015625, Accuracy: 0.88671875\n",
            "Step: 454900, Loss: 398.800537109375, Accuracy: 0.89453125\n",
            "Step: 455000, Loss: 396.6860046386719, Accuracy: 0.90625\n",
            "Step: 455100, Loss: 393.95050048828125, Accuracy: 0.91796875\n",
            "Step: 455200, Loss: 402.154296875, Accuracy: 0.87109375\n",
            "Step: 455300, Loss: 398.90264892578125, Accuracy: 0.8828125\n",
            "Step: 455400, Loss: 400.7212829589844, Accuracy: 0.87109375\n",
            "Step: 455500, Loss: 403.6518249511719, Accuracy: 0.87109375\n",
            "Step: 455600, Loss: 400.71771240234375, Accuracy: 0.8828125\n",
            "Step: 455700, Loss: 395.983154296875, Accuracy: 0.89453125\n",
            "Step: 455800, Loss: 399.60308837890625, Accuracy: 0.87890625\n",
            "Step: 455900, Loss: 399.41497802734375, Accuracy: 0.88671875\n",
            "Step: 456000, Loss: 396.92193603515625, Accuracy: 0.88671875\n",
            "Step: 456100, Loss: 400.98968505859375, Accuracy: 0.87109375\n",
            "Step: 456200, Loss: 397.2718505859375, Accuracy: 0.8984375\n",
            "Step: 456300, Loss: 397.0201416015625, Accuracy: 0.90234375\n",
            "Step: 456400, Loss: 400.5823059082031, Accuracy: 0.8671875\n",
            "Step: 456500, Loss: 413.9769287109375, Accuracy: 0.8203125\n",
            "Step: 456600, Loss: 400.0155334472656, Accuracy: 0.88671875\n",
            "Step: 456700, Loss: 394.44586181640625, Accuracy: 0.91015625\n",
            "Step: 456800, Loss: 402.35858154296875, Accuracy: 0.875\n",
            "Step: 456900, Loss: 390.3993225097656, Accuracy: 0.9140625\n",
            "Step: 457000, Loss: 405.86639404296875, Accuracy: 0.84765625\n",
            "Step: 457100, Loss: 400.24969482421875, Accuracy: 0.875\n",
            "Step: 457200, Loss: 397.7956848144531, Accuracy: 0.88671875\n",
            "Step: 457300, Loss: 410.01556396484375, Accuracy: 0.8359375\n",
            "Step: 457400, Loss: 395.783935546875, Accuracy: 0.90234375\n",
            "Step: 457500, Loss: 393.99810791015625, Accuracy: 0.9140625\n",
            "Step: 457600, Loss: 398.7194519042969, Accuracy: 0.87890625\n",
            "Step: 457700, Loss: 399.14727783203125, Accuracy: 0.890625\n",
            "Step: 457800, Loss: 399.89306640625, Accuracy: 0.88671875\n",
            "Step: 457900, Loss: 399.33447265625, Accuracy: 0.88671875\n",
            "Step: 458000, Loss: 395.96954345703125, Accuracy: 0.89453125\n",
            "Step: 458100, Loss: 401.69696044921875, Accuracy: 0.875\n",
            "Step: 458200, Loss: 404.8569030761719, Accuracy: 0.8671875\n",
            "Step: 458300, Loss: 405.8348083496094, Accuracy: 0.87109375\n",
            "Step: 458400, Loss: 399.958251953125, Accuracy: 0.87890625\n",
            "Step: 458500, Loss: 399.34527587890625, Accuracy: 0.8828125\n",
            "Step: 458600, Loss: 403.45965576171875, Accuracy: 0.8671875\n",
            "Step: 458700, Loss: 396.64227294921875, Accuracy: 0.90625\n",
            "Step: 458800, Loss: 402.58563232421875, Accuracy: 0.87109375\n",
            "Step: 458900, Loss: 390.32574462890625, Accuracy: 0.9296875\n",
            "Step: 459000, Loss: 399.0431213378906, Accuracy: 0.8828125\n",
            "Step: 459100, Loss: 396.47393798828125, Accuracy: 0.89453125\n",
            "Step: 459200, Loss: 397.56121826171875, Accuracy: 0.8828125\n",
            "Step: 459300, Loss: 396.9196472167969, Accuracy: 0.8828125\n",
            "Step: 459400, Loss: 402.528564453125, Accuracy: 0.875\n",
            "Step: 459500, Loss: 392.45953369140625, Accuracy: 0.921875\n",
            "Step: 459600, Loss: 395.86456298828125, Accuracy: 0.90234375\n",
            "Step: 459700, Loss: 395.365478515625, Accuracy: 0.90625\n",
            "Step: 459800, Loss: 404.4993896484375, Accuracy: 0.8515625\n",
            "Step: 459900, Loss: 403.39642333984375, Accuracy: 0.87890625\n",
            "Step: 460000, Loss: 405.5079040527344, Accuracy: 0.859375\n",
            "Step: 460100, Loss: 406.0372009277344, Accuracy: 0.859375\n",
            "Step: 460200, Loss: 398.5184326171875, Accuracy: 0.88671875\n",
            "Step: 460300, Loss: 398.0106201171875, Accuracy: 0.89453125\n",
            "Step: 460400, Loss: 394.926513671875, Accuracy: 0.9140625\n",
            "Step: 460500, Loss: 401.2444152832031, Accuracy: 0.875\n",
            "Step: 460600, Loss: 397.09661865234375, Accuracy: 0.88671875\n",
            "Step: 460700, Loss: 401.7762756347656, Accuracy: 0.87109375\n",
            "Step: 460800, Loss: 398.11224365234375, Accuracy: 0.890625\n",
            "Step: 460900, Loss: 396.16900634765625, Accuracy: 0.88671875\n",
            "Step: 461000, Loss: 398.2594909667969, Accuracy: 0.8984375\n",
            "Step: 461100, Loss: 396.7057800292969, Accuracy: 0.89453125\n",
            "Step: 461200, Loss: 403.08050537109375, Accuracy: 0.859375\n",
            "Step: 461300, Loss: 402.82696533203125, Accuracy: 0.8671875\n",
            "Step: 461400, Loss: 398.34735107421875, Accuracy: 0.890625\n",
            "Step: 461500, Loss: 398.30548095703125, Accuracy: 0.90234375\n",
            "Step: 461600, Loss: 396.7070007324219, Accuracy: 0.89453125\n",
            "Step: 461700, Loss: 395.1044921875, Accuracy: 0.8984375\n",
            "Step: 461800, Loss: 393.37005615234375, Accuracy: 0.91015625\n",
            "Step: 461900, Loss: 399.210693359375, Accuracy: 0.89453125\n",
            "Step: 462000, Loss: 397.6396484375, Accuracy: 0.88671875\n",
            "Step: 462100, Loss: 399.13604736328125, Accuracy: 0.890625\n",
            "Step: 462200, Loss: 395.2545471191406, Accuracy: 0.90625\n",
            "Step: 462300, Loss: 399.88555908203125, Accuracy: 0.88671875\n",
            "Step: 462400, Loss: 395.412353515625, Accuracy: 0.90625\n",
            "Step: 462500, Loss: 397.2174072265625, Accuracy: 0.90234375\n",
            "Step: 462600, Loss: 408.8955383300781, Accuracy: 0.84765625\n",
            "Step: 462700, Loss: 398.6461486816406, Accuracy: 0.87890625\n",
            "Step: 462800, Loss: 400.13720703125, Accuracy: 0.87890625\n",
            "Step: 462900, Loss: 399.4532165527344, Accuracy: 0.8828125\n",
            "Step: 463000, Loss: 397.2294921875, Accuracy: 0.8984375\n",
            "Step: 463100, Loss: 399.9824523925781, Accuracy: 0.875\n",
            "Step: 463200, Loss: 404.2022705078125, Accuracy: 0.86328125\n",
            "Step: 463300, Loss: 398.84600830078125, Accuracy: 0.890625\n",
            "Step: 463400, Loss: 399.3323974609375, Accuracy: 0.890625\n",
            "Step: 463500, Loss: 399.3899841308594, Accuracy: 0.8828125\n",
            "Step: 463600, Loss: 398.6798095703125, Accuracy: 0.890625\n",
            "Step: 463700, Loss: 395.0245361328125, Accuracy: 0.9140625\n",
            "Step: 463800, Loss: 397.13287353515625, Accuracy: 0.90234375\n",
            "Step: 463900, Loss: 400.60015869140625, Accuracy: 0.87890625\n",
            "Step: 464000, Loss: 403.4486083984375, Accuracy: 0.87890625\n",
            "Step: 464100, Loss: 396.52777099609375, Accuracy: 0.89453125\n",
            "Step: 464200, Loss: 404.42486572265625, Accuracy: 0.8671875\n",
            "Step: 464300, Loss: 394.0740051269531, Accuracy: 0.90625\n",
            "Step: 464400, Loss: 404.0138244628906, Accuracy: 0.8671875\n",
            "Step: 464500, Loss: 394.75054931640625, Accuracy: 0.90234375\n",
            "Step: 464600, Loss: 403.84130859375, Accuracy: 0.86328125\n",
            "Step: 464700, Loss: 406.769775390625, Accuracy: 0.84375\n",
            "Step: 464800, Loss: 395.55303955078125, Accuracy: 0.890625\n",
            "Step: 464900, Loss: 407.5351867675781, Accuracy: 0.85546875\n",
            "Step: 465000, Loss: 404.3800048828125, Accuracy: 0.86328125\n",
            "Step: 465100, Loss: 398.63458251953125, Accuracy: 0.8984375\n",
            "Step: 465200, Loss: 406.6017150878906, Accuracy: 0.859375\n",
            "Step: 465300, Loss: 398.2247314453125, Accuracy: 0.875\n",
            "Step: 465400, Loss: 396.46905517578125, Accuracy: 0.90234375\n",
            "Step: 465500, Loss: 404.0692138671875, Accuracy: 0.86328125\n",
            "Step: 465600, Loss: 396.80157470703125, Accuracy: 0.89453125\n",
            "Step: 465700, Loss: 399.89013671875, Accuracy: 0.89453125\n",
            "Step: 465800, Loss: 396.16571044921875, Accuracy: 0.90625\n",
            "Step: 465900, Loss: 400.8711853027344, Accuracy: 0.8828125\n",
            "Step: 466000, Loss: 398.21807861328125, Accuracy: 0.88671875\n",
            "Step: 466100, Loss: 392.76043701171875, Accuracy: 0.91015625\n",
            "Step: 466200, Loss: 397.9664306640625, Accuracy: 0.890625\n",
            "Step: 466300, Loss: 403.5838623046875, Accuracy: 0.86328125\n",
            "Step: 466400, Loss: 402.19293212890625, Accuracy: 0.875\n",
            "Step: 466500, Loss: 395.49261474609375, Accuracy: 0.90234375\n",
            "Step: 466600, Loss: 402.59320068359375, Accuracy: 0.8828125\n",
            "Step: 466700, Loss: 403.87408447265625, Accuracy: 0.875\n",
            "Step: 466800, Loss: 398.4753112792969, Accuracy: 0.8984375\n",
            "Step: 466900, Loss: 398.151611328125, Accuracy: 0.90234375\n",
            "Step: 467000, Loss: 397.51434326171875, Accuracy: 0.88671875\n",
            "Step: 467100, Loss: 400.7894287109375, Accuracy: 0.87890625\n",
            "Step: 467200, Loss: 401.01751708984375, Accuracy: 0.87890625\n",
            "Step: 467300, Loss: 401.15673828125, Accuracy: 0.87890625\n",
            "Step: 467400, Loss: 401.070068359375, Accuracy: 0.875\n",
            "Step: 467500, Loss: 400.02069091796875, Accuracy: 0.8828125\n",
            "Step: 467600, Loss: 398.5805358886719, Accuracy: 0.8984375\n",
            "Step: 467700, Loss: 400.4124755859375, Accuracy: 0.88671875\n",
            "Step: 467800, Loss: 400.9375305175781, Accuracy: 0.8828125\n",
            "Step: 467900, Loss: 402.768798828125, Accuracy: 0.8671875\n",
            "Step: 468000, Loss: 403.08062744140625, Accuracy: 0.875\n",
            "Step: 468100, Loss: 392.64990234375, Accuracy: 0.91015625\n",
            "Step: 468200, Loss: 406.35711669921875, Accuracy: 0.859375\n",
            "Step: 468300, Loss: 404.35955810546875, Accuracy: 0.859375\n",
            "Step: 468400, Loss: 399.49908447265625, Accuracy: 0.8828125\n",
            "Step: 468500, Loss: 403.3887939453125, Accuracy: 0.859375\n",
            "Step: 468600, Loss: 394.72564697265625, Accuracy: 0.90625\n",
            "Step: 468700, Loss: 394.6390075683594, Accuracy: 0.90625\n",
            "Step: 468800, Loss: 399.2119445800781, Accuracy: 0.890625\n",
            "Step: 468900, Loss: 398.9786071777344, Accuracy: 0.87890625\n",
            "Step: 469000, Loss: 403.12908935546875, Accuracy: 0.86328125\n",
            "Step: 469100, Loss: 396.3250732421875, Accuracy: 0.890625\n",
            "Step: 469200, Loss: 393.0645751953125, Accuracy: 0.92578125\n",
            "Step: 469300, Loss: 401.014404296875, Accuracy: 0.87890625\n",
            "Step: 469400, Loss: 397.736083984375, Accuracy: 0.90234375\n",
            "Step: 469500, Loss: 401.401611328125, Accuracy: 0.87890625\n",
            "Step: 469600, Loss: 389.76654052734375, Accuracy: 0.91796875\n",
            "Step: 469700, Loss: 397.803955078125, Accuracy: 0.8984375\n",
            "Step: 469800, Loss: 405.0633544921875, Accuracy: 0.86328125\n",
            "Step: 469900, Loss: 400.0970458984375, Accuracy: 0.8828125\n",
            "Step: 470000, Loss: 395.11590576171875, Accuracy: 0.89453125\n",
            "Step: 470100, Loss: 394.83966064453125, Accuracy: 0.90625\n",
            "Step: 470200, Loss: 388.43975830078125, Accuracy: 0.9375\n",
            "Step: 470300, Loss: 407.56085205078125, Accuracy: 0.8515625\n",
            "Step: 470400, Loss: 394.1645202636719, Accuracy: 0.90625\n",
            "Step: 470500, Loss: 391.840087890625, Accuracy: 0.91796875\n",
            "Step: 470600, Loss: 398.49163818359375, Accuracy: 0.88671875\n",
            "Step: 470700, Loss: 400.6196594238281, Accuracy: 0.87890625\n",
            "Step: 470800, Loss: 406.67987060546875, Accuracy: 0.85546875\n",
            "Step: 470900, Loss: 405.20172119140625, Accuracy: 0.86328125\n",
            "Step: 471000, Loss: 407.4077453613281, Accuracy: 0.85546875\n",
            "Step: 471100, Loss: 399.96728515625, Accuracy: 0.890625\n",
            "Step: 471200, Loss: 403.8916320800781, Accuracy: 0.875\n",
            "Step: 471300, Loss: 394.580810546875, Accuracy: 0.90625\n",
            "Step: 471400, Loss: 393.238525390625, Accuracy: 0.90625\n",
            "Step: 471500, Loss: 399.3564758300781, Accuracy: 0.88671875\n",
            "Step: 471600, Loss: 403.8790283203125, Accuracy: 0.8671875\n",
            "Step: 471700, Loss: 395.8951416015625, Accuracy: 0.90234375\n",
            "Step: 471800, Loss: 400.031005859375, Accuracy: 0.87890625\n",
            "Step: 471900, Loss: 400.92767333984375, Accuracy: 0.8828125\n",
            "Step: 472000, Loss: 397.99261474609375, Accuracy: 0.88671875\n",
            "Step: 472100, Loss: 404.306884765625, Accuracy: 0.86328125\n",
            "Step: 472200, Loss: 405.72613525390625, Accuracy: 0.8515625\n",
            "Step: 472300, Loss: 394.485107421875, Accuracy: 0.91015625\n",
            "Step: 472400, Loss: 396.648681640625, Accuracy: 0.88671875\n",
            "Step: 472500, Loss: 398.6332092285156, Accuracy: 0.8828125\n",
            "Step: 472600, Loss: 397.1949768066406, Accuracy: 0.890625\n",
            "Step: 472700, Loss: 395.9668884277344, Accuracy: 0.9140625\n",
            "Step: 472800, Loss: 395.7516784667969, Accuracy: 0.89453125\n",
            "Step: 472900, Loss: 400.507080078125, Accuracy: 0.875\n",
            "Step: 473000, Loss: 398.552734375, Accuracy: 0.89453125\n",
            "Step: 473100, Loss: 407.84014892578125, Accuracy: 0.84765625\n",
            "Step: 473200, Loss: 389.09326171875, Accuracy: 0.93359375\n",
            "Step: 473300, Loss: 404.7340087890625, Accuracy: 0.87109375\n",
            "Step: 473400, Loss: 392.57550048828125, Accuracy: 0.921875\n",
            "Step: 473500, Loss: 401.1585388183594, Accuracy: 0.87890625\n",
            "Step: 473600, Loss: 393.5088195800781, Accuracy: 0.91015625\n",
            "Step: 473700, Loss: 405.2320556640625, Accuracy: 0.86328125\n",
            "Step: 473800, Loss: 399.93585205078125, Accuracy: 0.8828125\n",
            "Step: 473900, Loss: 398.5702209472656, Accuracy: 0.89453125\n",
            "Step: 474000, Loss: 400.5912170410156, Accuracy: 0.88671875\n",
            "Step: 474100, Loss: 400.12823486328125, Accuracy: 0.8828125\n",
            "Step: 474200, Loss: 405.49688720703125, Accuracy: 0.85546875\n",
            "Step: 474300, Loss: 391.4368896484375, Accuracy: 0.92578125\n",
            "Step: 474400, Loss: 397.64801025390625, Accuracy: 0.890625\n",
            "Step: 474500, Loss: 395.15252685546875, Accuracy: 0.89453125\n",
            "Step: 474600, Loss: 401.9591979980469, Accuracy: 0.875\n",
            "Step: 474700, Loss: 400.68878173828125, Accuracy: 0.890625\n",
            "Step: 474800, Loss: 407.8265686035156, Accuracy: 0.84765625\n",
            "Step: 474900, Loss: 397.7301025390625, Accuracy: 0.890625\n",
            "Step: 475000, Loss: 394.3414001464844, Accuracy: 0.91015625\n",
            "Step: 475100, Loss: 403.4246520996094, Accuracy: 0.875\n",
            "Step: 475200, Loss: 395.6993408203125, Accuracy: 0.90234375\n",
            "Step: 475300, Loss: 393.14166259765625, Accuracy: 0.9140625\n",
            "Step: 475400, Loss: 399.978515625, Accuracy: 0.87890625\n",
            "Step: 475500, Loss: 407.80853271484375, Accuracy: 0.84375\n",
            "Step: 475600, Loss: 397.2632141113281, Accuracy: 0.88671875\n",
            "Step: 475700, Loss: 394.73638916015625, Accuracy: 0.90234375\n",
            "Step: 475800, Loss: 409.5738525390625, Accuracy: 0.83984375\n",
            "Step: 475900, Loss: 395.7916564941406, Accuracy: 0.90625\n",
            "Step: 476000, Loss: 400.9879150390625, Accuracy: 0.875\n",
            "Step: 476100, Loss: 399.31915283203125, Accuracy: 0.88671875\n",
            "Step: 476200, Loss: 403.28387451171875, Accuracy: 0.87109375\n",
            "Step: 476300, Loss: 395.12548828125, Accuracy: 0.8984375\n",
            "Step: 476400, Loss: 407.2584228515625, Accuracy: 0.8515625\n",
            "Step: 476500, Loss: 405.70904541015625, Accuracy: 0.8515625\n",
            "Step: 476600, Loss: 390.7895812988281, Accuracy: 0.921875\n",
            "Step: 476700, Loss: 393.5684814453125, Accuracy: 0.921875\n",
            "Step: 476800, Loss: 410.97088623046875, Accuracy: 0.8359375\n",
            "Step: 476900, Loss: 399.00311279296875, Accuracy: 0.890625\n",
            "Step: 477000, Loss: 404.66876220703125, Accuracy: 0.86328125\n",
            "Step: 477100, Loss: 403.9207458496094, Accuracy: 0.8671875\n",
            "Step: 477200, Loss: 405.8375244140625, Accuracy: 0.8515625\n",
            "Step: 477300, Loss: 395.82501220703125, Accuracy: 0.90625\n",
            "Step: 477400, Loss: 396.476806640625, Accuracy: 0.90234375\n",
            "Step: 477500, Loss: 395.8017578125, Accuracy: 0.90234375\n",
            "Step: 477600, Loss: 399.55889892578125, Accuracy: 0.875\n",
            "Step: 477700, Loss: 398.63287353515625, Accuracy: 0.89453125\n",
            "Step: 477800, Loss: 399.880615234375, Accuracy: 0.88671875\n",
            "Step: 477900, Loss: 396.2928466796875, Accuracy: 0.89453125\n",
            "Step: 478000, Loss: 401.9951171875, Accuracy: 0.8671875\n",
            "Step: 478100, Loss: 394.9217834472656, Accuracy: 0.8984375\n",
            "Step: 478200, Loss: 394.56646728515625, Accuracy: 0.8984375\n",
            "Step: 478300, Loss: 395.2314147949219, Accuracy: 0.8984375\n",
            "Step: 478400, Loss: 403.3253173828125, Accuracy: 0.86328125\n",
            "Step: 478500, Loss: 399.44287109375, Accuracy: 0.8828125\n",
            "Step: 478600, Loss: 400.9208679199219, Accuracy: 0.8828125\n",
            "Step: 478700, Loss: 398.3070068359375, Accuracy: 0.8828125\n",
            "Step: 478800, Loss: 402.43621826171875, Accuracy: 0.86328125\n",
            "Step: 478900, Loss: 396.4386901855469, Accuracy: 0.89453125\n",
            "Step: 479000, Loss: 402.9858093261719, Accuracy: 0.87109375\n",
            "Step: 479100, Loss: 404.0894775390625, Accuracy: 0.875\n",
            "Step: 479200, Loss: 390.99407958984375, Accuracy: 0.9296875\n",
            "Step: 479300, Loss: 401.17840576171875, Accuracy: 0.87890625\n",
            "Step: 479400, Loss: 404.3900146484375, Accuracy: 0.84765625\n",
            "Step: 479500, Loss: 402.30352783203125, Accuracy: 0.875\n",
            "Step: 479600, Loss: 401.10052490234375, Accuracy: 0.8828125\n",
            "Step: 479700, Loss: 399.99285888671875, Accuracy: 0.87890625\n",
            "Step: 479800, Loss: 399.4720764160156, Accuracy: 0.87890625\n",
            "Step: 479900, Loss: 406.90802001953125, Accuracy: 0.84765625\n",
            "Step: 480000, Loss: 396.73565673828125, Accuracy: 0.8984375\n",
            "Step: 480100, Loss: 400.90582275390625, Accuracy: 0.8828125\n",
            "Step: 480200, Loss: 401.86016845703125, Accuracy: 0.8671875\n",
            "Step: 480300, Loss: 393.9010314941406, Accuracy: 0.91015625\n",
            "Step: 480400, Loss: 398.50274658203125, Accuracy: 0.88671875\n",
            "Step: 480500, Loss: 393.700927734375, Accuracy: 0.91796875\n",
            "Step: 480600, Loss: 401.3873291015625, Accuracy: 0.87109375\n",
            "Step: 480700, Loss: 399.18865966796875, Accuracy: 0.8828125\n",
            "Step: 480800, Loss: 393.828369140625, Accuracy: 0.90234375\n",
            "Step: 480900, Loss: 400.56732177734375, Accuracy: 0.87890625\n",
            "Step: 481000, Loss: 398.11285400390625, Accuracy: 0.89453125\n",
            "Step: 481100, Loss: 397.37689208984375, Accuracy: 0.90234375\n",
            "Step: 481200, Loss: 403.486572265625, Accuracy: 0.86328125\n",
            "Step: 481300, Loss: 397.3550720214844, Accuracy: 0.890625\n",
            "Step: 481400, Loss: 401.2329406738281, Accuracy: 0.87890625\n",
            "Step: 481500, Loss: 392.11981201171875, Accuracy: 0.91796875\n",
            "Step: 481600, Loss: 410.313232421875, Accuracy: 0.84375\n",
            "Step: 481700, Loss: 393.38055419921875, Accuracy: 0.91015625\n",
            "Step: 481800, Loss: 394.9459228515625, Accuracy: 0.90625\n",
            "Step: 481900, Loss: 399.4790344238281, Accuracy: 0.890625\n",
            "Step: 482000, Loss: 398.7356872558594, Accuracy: 0.890625\n",
            "Step: 482100, Loss: 395.95782470703125, Accuracy: 0.8984375\n",
            "Step: 482200, Loss: 402.90460205078125, Accuracy: 0.8671875\n",
            "Step: 482300, Loss: 411.5647277832031, Accuracy: 0.8359375\n",
            "Step: 482400, Loss: 401.1924133300781, Accuracy: 0.87890625\n",
            "Step: 482500, Loss: 400.6842041015625, Accuracy: 0.87890625\n",
            "Step: 482600, Loss: 393.5035705566406, Accuracy: 0.91015625\n",
            "Step: 482700, Loss: 396.1615295410156, Accuracy: 0.88671875\n",
            "Step: 482800, Loss: 403.3330078125, Accuracy: 0.8671875\n",
            "Step: 482900, Loss: 401.2236633300781, Accuracy: 0.890625\n",
            "Step: 483000, Loss: 400.411865234375, Accuracy: 0.88671875\n",
            "Step: 483100, Loss: 397.22930908203125, Accuracy: 0.90625\n",
            "Step: 483200, Loss: 398.3345642089844, Accuracy: 0.890625\n",
            "Step: 483300, Loss: 396.013427734375, Accuracy: 0.91015625\n",
            "Step: 483400, Loss: 398.38409423828125, Accuracy: 0.88671875\n",
            "Step: 483500, Loss: 400.5616149902344, Accuracy: 0.875\n",
            "Step: 483600, Loss: 396.223388671875, Accuracy: 0.890625\n",
            "Step: 483700, Loss: 397.48577880859375, Accuracy: 0.90234375\n",
            "Step: 483800, Loss: 397.99188232421875, Accuracy: 0.890625\n",
            "Step: 483900, Loss: 398.0181884765625, Accuracy: 0.890625\n",
            "Step: 484000, Loss: 401.54888916015625, Accuracy: 0.87109375\n",
            "Step: 484100, Loss: 405.5777587890625, Accuracy: 0.8515625\n",
            "Step: 484200, Loss: 400.50726318359375, Accuracy: 0.88671875\n",
            "Step: 484300, Loss: 399.2693176269531, Accuracy: 0.87109375\n",
            "Step: 484400, Loss: 401.59033203125, Accuracy: 0.8671875\n",
            "Step: 484500, Loss: 402.03369140625, Accuracy: 0.86328125\n",
            "Step: 484600, Loss: 399.571533203125, Accuracy: 0.88671875\n",
            "Step: 484700, Loss: 394.1711730957031, Accuracy: 0.90234375\n",
            "Step: 484800, Loss: 399.41473388671875, Accuracy: 0.88671875\n",
            "Step: 484900, Loss: 396.61932373046875, Accuracy: 0.89453125\n",
            "Step: 485000, Loss: 398.7871398925781, Accuracy: 0.890625\n",
            "Step: 485100, Loss: 402.86688232421875, Accuracy: 0.8671875\n",
            "Step: 485200, Loss: 397.5254821777344, Accuracy: 0.8984375\n",
            "Step: 485300, Loss: 393.966552734375, Accuracy: 0.91796875\n",
            "Step: 485400, Loss: 398.52484130859375, Accuracy: 0.8828125\n",
            "Step: 485500, Loss: 403.5535888671875, Accuracy: 0.875\n",
            "Step: 485600, Loss: 406.3611145019531, Accuracy: 0.85546875\n",
            "Step: 485700, Loss: 399.34307861328125, Accuracy: 0.87890625\n",
            "Step: 485800, Loss: 405.59283447265625, Accuracy: 0.85546875\n",
            "Step: 485900, Loss: 400.84405517578125, Accuracy: 0.87890625\n",
            "Step: 486000, Loss: 404.51910400390625, Accuracy: 0.86328125\n",
            "Step: 486100, Loss: 400.52349853515625, Accuracy: 0.88671875\n",
            "Step: 486200, Loss: 397.76373291015625, Accuracy: 0.88671875\n",
            "Step: 486300, Loss: 399.41485595703125, Accuracy: 0.87890625\n",
            "Step: 486400, Loss: 406.5743408203125, Accuracy: 0.8515625\n",
            "Step: 486500, Loss: 403.3857421875, Accuracy: 0.859375\n",
            "Step: 486600, Loss: 405.47918701171875, Accuracy: 0.8671875\n",
            "Step: 486700, Loss: 396.052978515625, Accuracy: 0.89453125\n",
            "Step: 486800, Loss: 392.5509033203125, Accuracy: 0.91796875\n",
            "Step: 486900, Loss: 405.82757568359375, Accuracy: 0.859375\n",
            "Step: 487000, Loss: 394.2913818359375, Accuracy: 0.90234375\n",
            "Step: 487100, Loss: 402.48876953125, Accuracy: 0.8671875\n",
            "Step: 487200, Loss: 398.9356689453125, Accuracy: 0.890625\n",
            "Step: 487300, Loss: 397.80975341796875, Accuracy: 0.8984375\n",
            "Step: 487400, Loss: 402.755126953125, Accuracy: 0.875\n",
            "Step: 487500, Loss: 408.6815185546875, Accuracy: 0.84375\n",
            "Step: 487600, Loss: 400.0159912109375, Accuracy: 0.8828125\n",
            "Step: 487700, Loss: 408.3918151855469, Accuracy: 0.85546875\n",
            "Step: 487800, Loss: 398.30078125, Accuracy: 0.890625\n",
            "Step: 487900, Loss: 397.7559814453125, Accuracy: 0.890625\n",
            "Step: 488000, Loss: 396.7879638671875, Accuracy: 0.89453125\n",
            "Step: 488100, Loss: 396.9791259765625, Accuracy: 0.8984375\n",
            "Step: 488200, Loss: 396.23052978515625, Accuracy: 0.90234375\n",
            "Step: 488300, Loss: 400.26861572265625, Accuracy: 0.88671875\n",
            "Step: 488400, Loss: 404.0013427734375, Accuracy: 0.87109375\n",
            "Step: 488500, Loss: 400.35870361328125, Accuracy: 0.8828125\n",
            "Step: 488600, Loss: 409.02374267578125, Accuracy: 0.84375\n",
            "Step: 488700, Loss: 404.1662902832031, Accuracy: 0.8671875\n",
            "Step: 488800, Loss: 402.15350341796875, Accuracy: 0.87890625\n",
            "Step: 488900, Loss: 407.1080322265625, Accuracy: 0.8515625\n",
            "Step: 489000, Loss: 402.92193603515625, Accuracy: 0.8828125\n",
            "Step: 489100, Loss: 392.3722839355469, Accuracy: 0.92578125\n",
            "Step: 489200, Loss: 397.41351318359375, Accuracy: 0.89453125\n",
            "Step: 489300, Loss: 402.0142822265625, Accuracy: 0.87890625\n",
            "Step: 489400, Loss: 401.9364013671875, Accuracy: 0.87109375\n",
            "Step: 489500, Loss: 403.15875244140625, Accuracy: 0.875\n",
            "Step: 489600, Loss: 394.81134033203125, Accuracy: 0.89453125\n",
            "Step: 489700, Loss: 398.37677001953125, Accuracy: 0.890625\n",
            "Step: 489800, Loss: 392.46697998046875, Accuracy: 0.92578125\n",
            "Step: 489900, Loss: 400.56341552734375, Accuracy: 0.87890625\n",
            "Step: 490000, Loss: 409.1060791015625, Accuracy: 0.84375\n",
            "Step: 490100, Loss: 401.52020263671875, Accuracy: 0.87890625\n",
            "Step: 490200, Loss: 399.0792236328125, Accuracy: 0.88671875\n",
            "Step: 490300, Loss: 397.10150146484375, Accuracy: 0.8984375\n",
            "Step: 490400, Loss: 407.1060791015625, Accuracy: 0.84375\n",
            "Step: 490500, Loss: 407.3755187988281, Accuracy: 0.859375\n",
            "Step: 490600, Loss: 395.99676513671875, Accuracy: 0.90234375\n",
            "Step: 490700, Loss: 398.73919677734375, Accuracy: 0.88671875\n",
            "Step: 490800, Loss: 397.4635009765625, Accuracy: 0.890625\n",
            "Step: 490900, Loss: 403.03125, Accuracy: 0.85546875\n",
            "Step: 491000, Loss: 400.0439758300781, Accuracy: 0.890625\n",
            "Step: 491100, Loss: 396.89080810546875, Accuracy: 0.890625\n",
            "Step: 491200, Loss: 394.8299560546875, Accuracy: 0.90234375\n",
            "Step: 491300, Loss: 405.9767150878906, Accuracy: 0.85546875\n",
            "Step: 491400, Loss: 399.4495849609375, Accuracy: 0.8828125\n",
            "Step: 491500, Loss: 405.8034973144531, Accuracy: 0.8515625\n",
            "Step: 491600, Loss: 400.94244384765625, Accuracy: 0.8671875\n",
            "Step: 491700, Loss: 395.0232849121094, Accuracy: 0.91015625\n",
            "Step: 491800, Loss: 400.89569091796875, Accuracy: 0.88671875\n",
            "Step: 491900, Loss: 394.7599182128906, Accuracy: 0.90625\n",
            "Step: 492000, Loss: 397.78558349609375, Accuracy: 0.88671875\n",
            "Step: 492100, Loss: 403.77593994140625, Accuracy: 0.8671875\n",
            "Step: 492200, Loss: 390.50274658203125, Accuracy: 0.92578125\n",
            "Step: 492300, Loss: 398.1964416503906, Accuracy: 0.890625\n",
            "Step: 492400, Loss: 406.21234130859375, Accuracy: 0.859375\n",
            "Step: 492500, Loss: 404.2134704589844, Accuracy: 0.84765625\n",
            "Step: 492600, Loss: 405.760986328125, Accuracy: 0.859375\n",
            "Step: 492700, Loss: 392.0274963378906, Accuracy: 0.91015625\n",
            "Step: 492800, Loss: 402.998046875, Accuracy: 0.87109375\n",
            "Step: 492900, Loss: 405.4560852050781, Accuracy: 0.859375\n",
            "Step: 493000, Loss: 397.6802978515625, Accuracy: 0.89453125\n",
            "Step: 493100, Loss: 401.9366760253906, Accuracy: 0.8671875\n",
            "Step: 493200, Loss: 404.7711486816406, Accuracy: 0.8671875\n",
            "Step: 493300, Loss: 406.83001708984375, Accuracy: 0.8515625\n",
            "Step: 493400, Loss: 399.03240966796875, Accuracy: 0.88671875\n",
            "Step: 493500, Loss: 396.91241455078125, Accuracy: 0.9140625\n",
            "Step: 493600, Loss: 394.1074523925781, Accuracy: 0.90625\n",
            "Step: 493700, Loss: 397.375, Accuracy: 0.88671875\n",
            "Step: 493800, Loss: 393.89801025390625, Accuracy: 0.921875\n",
            "Step: 493900, Loss: 397.1683349609375, Accuracy: 0.89453125\n",
            "Step: 494000, Loss: 397.85662841796875, Accuracy: 0.88671875\n",
            "Step: 494100, Loss: 398.13067626953125, Accuracy: 0.87890625\n",
            "Step: 494200, Loss: 400.037109375, Accuracy: 0.8828125\n",
            "Step: 494300, Loss: 406.0105285644531, Accuracy: 0.85546875\n",
            "Step: 494400, Loss: 406.1747131347656, Accuracy: 0.85546875\n",
            "Step: 494500, Loss: 403.53875732421875, Accuracy: 0.8671875\n",
            "Step: 494600, Loss: 404.39495849609375, Accuracy: 0.87109375\n",
            "Step: 494700, Loss: 390.8441162109375, Accuracy: 0.9140625\n",
            "Step: 494800, Loss: 398.8647155761719, Accuracy: 0.88671875\n",
            "Step: 494900, Loss: 402.49835205078125, Accuracy: 0.87890625\n",
            "Step: 495000, Loss: 400.08258056640625, Accuracy: 0.88671875\n",
            "Step: 495100, Loss: 394.77508544921875, Accuracy: 0.91015625\n",
            "Step: 495200, Loss: 406.387939453125, Accuracy: 0.8515625\n",
            "Step: 495300, Loss: 399.26788330078125, Accuracy: 0.875\n",
            "Step: 495400, Loss: 399.95111083984375, Accuracy: 0.875\n",
            "Step: 495500, Loss: 399.1981506347656, Accuracy: 0.890625\n",
            "Step: 495600, Loss: 403.5413818359375, Accuracy: 0.87109375\n",
            "Step: 495700, Loss: 407.581787109375, Accuracy: 0.86328125\n",
            "Step: 495800, Loss: 392.54779052734375, Accuracy: 0.9140625\n",
            "Step: 495900, Loss: 398.68804931640625, Accuracy: 0.88671875\n",
            "Step: 496000, Loss: 400.0918273925781, Accuracy: 0.8828125\n",
            "Step: 496100, Loss: 408.58563232421875, Accuracy: 0.83984375\n",
            "Step: 496200, Loss: 406.57794189453125, Accuracy: 0.87890625\n",
            "Step: 496300, Loss: 393.18402099609375, Accuracy: 0.9140625\n",
            "Step: 496400, Loss: 395.8658447265625, Accuracy: 0.8984375\n",
            "Step: 496500, Loss: 394.0069885253906, Accuracy: 0.9140625\n",
            "Step: 496600, Loss: 399.8276672363281, Accuracy: 0.88671875\n",
            "Step: 496700, Loss: 402.5517578125, Accuracy: 0.87109375\n",
            "Step: 496800, Loss: 403.5000305175781, Accuracy: 0.875\n",
            "Step: 496900, Loss: 392.0574951171875, Accuracy: 0.91796875\n",
            "Step: 497000, Loss: 401.39385986328125, Accuracy: 0.8828125\n",
            "Step: 497100, Loss: 400.9985046386719, Accuracy: 0.87890625\n",
            "Step: 497200, Loss: 396.98956298828125, Accuracy: 0.89453125\n",
            "Step: 497300, Loss: 398.3259582519531, Accuracy: 0.89453125\n",
            "Step: 497400, Loss: 397.0882568359375, Accuracy: 0.89453125\n",
            "Step: 497500, Loss: 404.41180419921875, Accuracy: 0.87109375\n",
            "Step: 497600, Loss: 395.41943359375, Accuracy: 0.90234375\n",
            "Step: 497700, Loss: 406.0860595703125, Accuracy: 0.85546875\n",
            "Step: 497800, Loss: 395.14654541015625, Accuracy: 0.90625\n",
            "Step: 497900, Loss: 389.8606872558594, Accuracy: 0.9375\n",
            "Step: 498000, Loss: 398.3551025390625, Accuracy: 0.88671875\n",
            "Step: 498100, Loss: 391.1807556152344, Accuracy: 0.91796875\n",
            "Step: 498200, Loss: 392.0086975097656, Accuracy: 0.921875\n",
            "Step: 498300, Loss: 401.58966064453125, Accuracy: 0.8828125\n",
            "Step: 498400, Loss: 407.0473937988281, Accuracy: 0.8515625\n",
            "Step: 498500, Loss: 395.19964599609375, Accuracy: 0.8984375\n",
            "Step: 498600, Loss: 401.7667541503906, Accuracy: 0.87109375\n",
            "Step: 498700, Loss: 400.5981140136719, Accuracy: 0.87890625\n",
            "Step: 498800, Loss: 402.9879150390625, Accuracy: 0.86328125\n",
            "Step: 498900, Loss: 399.55963134765625, Accuracy: 0.87890625\n",
            "Step: 499000, Loss: 402.69720458984375, Accuracy: 0.88671875\n",
            "Step: 499100, Loss: 396.1457824707031, Accuracy: 0.91015625\n",
            "Step: 499200, Loss: 390.219970703125, Accuracy: 0.921875\n",
            "Step: 499300, Loss: 392.2124938964844, Accuracy: 0.921875\n",
            "Step: 499400, Loss: 396.42926025390625, Accuracy: 0.8984375\n",
            "Step: 499500, Loss: 395.2764892578125, Accuracy: 0.90625\n",
            "Step: 499600, Loss: 408.71197509765625, Accuracy: 0.83984375\n",
            "Step: 499700, Loss: 400.3262939453125, Accuracy: 0.88671875\n",
            "Step: 499800, Loss: 401.9833984375, Accuracy: 0.87109375\n",
            "Step: 499900, Loss: 397.7293701171875, Accuracy: 0.890625\n",
            "Step: 500000, Loss: 396.256591796875, Accuracy: 0.890625\n",
            "Step: 500100, Loss: 403.617919921875, Accuracy: 0.86328125\n",
            "Step: 500200, Loss: 392.1514892578125, Accuracy: 0.90625\n",
            "Step: 500300, Loss: 404.017333984375, Accuracy: 0.86328125\n",
            "Step: 500400, Loss: 400.08685302734375, Accuracy: 0.87890625\n",
            "Step: 500500, Loss: 394.11822509765625, Accuracy: 0.9140625\n",
            "Step: 500600, Loss: 402.0665588378906, Accuracy: 0.8828125\n",
            "Step: 500700, Loss: 404.1505432128906, Accuracy: 0.859375\n",
            "Step: 500800, Loss: 399.87554931640625, Accuracy: 0.875\n",
            "Step: 500900, Loss: 393.57672119140625, Accuracy: 0.90625\n",
            "Step: 501000, Loss: 397.52813720703125, Accuracy: 0.890625\n",
            "Step: 501100, Loss: 405.889404296875, Accuracy: 0.8515625\n",
            "Step: 501200, Loss: 403.1924133300781, Accuracy: 0.87890625\n",
            "Step: 501300, Loss: 397.242919921875, Accuracy: 0.90234375\n",
            "Step: 501400, Loss: 402.11065673828125, Accuracy: 0.87109375\n",
            "Step: 501500, Loss: 400.1070556640625, Accuracy: 0.8828125\n",
            "Step: 501600, Loss: 408.73052978515625, Accuracy: 0.84375\n",
            "Step: 501700, Loss: 400.4945373535156, Accuracy: 0.8828125\n",
            "Step: 501800, Loss: 396.4149475097656, Accuracy: 0.88671875\n",
            "Step: 501900, Loss: 405.1689147949219, Accuracy: 0.8515625\n",
            "Step: 502000, Loss: 405.60345458984375, Accuracy: 0.8671875\n",
            "Step: 502100, Loss: 399.61376953125, Accuracy: 0.87109375\n",
            "Step: 502200, Loss: 395.1342468261719, Accuracy: 0.8984375\n",
            "Step: 502300, Loss: 397.54595947265625, Accuracy: 0.8984375\n",
            "Step: 502400, Loss: 400.1301574707031, Accuracy: 0.8828125\n",
            "Step: 502500, Loss: 403.3269348144531, Accuracy: 0.87109375\n",
            "Step: 502600, Loss: 399.4200439453125, Accuracy: 0.89453125\n",
            "Step: 502700, Loss: 404.3704833984375, Accuracy: 0.890625\n",
            "Step: 502800, Loss: 402.3396911621094, Accuracy: 0.8828125\n",
            "Step: 502900, Loss: 395.0674133300781, Accuracy: 0.90625\n",
            "Step: 503000, Loss: 398.99615478515625, Accuracy: 0.8984375\n",
            "Step: 503100, Loss: 402.16876220703125, Accuracy: 0.8671875\n",
            "Step: 503200, Loss: 399.74224853515625, Accuracy: 0.89453125\n",
            "Step: 503300, Loss: 398.4971618652344, Accuracy: 0.890625\n",
            "Step: 503400, Loss: 401.6533508300781, Accuracy: 0.8671875\n",
            "Step: 503500, Loss: 404.9125671386719, Accuracy: 0.8671875\n",
            "Step: 503600, Loss: 405.3719482421875, Accuracy: 0.8515625\n",
            "Step: 503700, Loss: 402.82293701171875, Accuracy: 0.875\n",
            "Step: 503800, Loss: 396.706787109375, Accuracy: 0.90234375\n",
            "Step: 503900, Loss: 402.4090576171875, Accuracy: 0.8671875\n",
            "Step: 504000, Loss: 394.2552490234375, Accuracy: 0.9140625\n",
            "Step: 504100, Loss: 400.177734375, Accuracy: 0.88671875\n",
            "Step: 504200, Loss: 398.609619140625, Accuracy: 0.8828125\n",
            "Step: 504300, Loss: 394.35308837890625, Accuracy: 0.91015625\n",
            "Step: 504400, Loss: 404.8072509765625, Accuracy: 0.86328125\n",
            "Step: 504500, Loss: 393.01361083984375, Accuracy: 0.90625\n",
            "Step: 504600, Loss: 393.30633544921875, Accuracy: 0.91796875\n",
            "Step: 504700, Loss: 399.5611572265625, Accuracy: 0.88671875\n",
            "Step: 504800, Loss: 393.376708984375, Accuracy: 0.9140625\n",
            "Step: 504900, Loss: 394.91314697265625, Accuracy: 0.90234375\n",
            "Step: 505000, Loss: 396.9356994628906, Accuracy: 0.890625\n",
            "Step: 505100, Loss: 401.48468017578125, Accuracy: 0.87109375\n",
            "Step: 505200, Loss: 397.3459167480469, Accuracy: 0.8984375\n",
            "Step: 505300, Loss: 404.2469482421875, Accuracy: 0.8671875\n",
            "Step: 505400, Loss: 402.341064453125, Accuracy: 0.875\n",
            "Step: 505500, Loss: 395.2526550292969, Accuracy: 0.90625\n",
            "Step: 505600, Loss: 400.12225341796875, Accuracy: 0.88671875\n",
            "Step: 505700, Loss: 398.8355712890625, Accuracy: 0.88671875\n",
            "Step: 505800, Loss: 400.08935546875, Accuracy: 0.890625\n",
            "Step: 505900, Loss: 395.65386962890625, Accuracy: 0.9140625\n",
            "Step: 506000, Loss: 405.0090637207031, Accuracy: 0.86328125\n",
            "Step: 506100, Loss: 395.57012939453125, Accuracy: 0.890625\n",
            "Step: 506200, Loss: 402.37957763671875, Accuracy: 0.87890625\n",
            "Step: 506300, Loss: 391.956787109375, Accuracy: 0.91796875\n",
            "Step: 506400, Loss: 397.2412109375, Accuracy: 0.8984375\n",
            "Step: 506500, Loss: 405.85638427734375, Accuracy: 0.8515625\n",
            "Step: 506600, Loss: 397.8680725097656, Accuracy: 0.8984375\n",
            "Step: 506700, Loss: 407.9427490234375, Accuracy: 0.8515625\n",
            "Step: 506800, Loss: 404.253173828125, Accuracy: 0.8671875\n",
            "Step: 506900, Loss: 404.3540954589844, Accuracy: 0.8671875\n",
            "Step: 507000, Loss: 397.9404296875, Accuracy: 0.90234375\n",
            "Step: 507100, Loss: 401.4541320800781, Accuracy: 0.875\n",
            "Step: 507200, Loss: 395.10906982421875, Accuracy: 0.90625\n",
            "Step: 507300, Loss: 401.05096435546875, Accuracy: 0.87890625\n",
            "Step: 507400, Loss: 392.13458251953125, Accuracy: 0.921875\n",
            "Step: 507500, Loss: 394.3432922363281, Accuracy: 0.90625\n",
            "Step: 507600, Loss: 406.1549987792969, Accuracy: 0.8671875\n",
            "Step: 507700, Loss: 401.4426574707031, Accuracy: 0.875\n",
            "Step: 507800, Loss: 401.1944885253906, Accuracy: 0.875\n",
            "Step: 507900, Loss: 398.136962890625, Accuracy: 0.8828125\n",
            "Step: 508000, Loss: 406.6776123046875, Accuracy: 0.859375\n",
            "Step: 508100, Loss: 398.85345458984375, Accuracy: 0.88671875\n",
            "Step: 508200, Loss: 394.4732666015625, Accuracy: 0.90625\n",
            "Step: 508300, Loss: 400.152099609375, Accuracy: 0.88671875\n",
            "Step: 508400, Loss: 407.56756591796875, Accuracy: 0.859375\n",
            "Step: 508500, Loss: 397.9224853515625, Accuracy: 0.89453125\n",
            "Step: 508600, Loss: 399.9472961425781, Accuracy: 0.8828125\n",
            "Step: 508700, Loss: 395.83453369140625, Accuracy: 0.8984375\n",
            "Step: 508800, Loss: 397.2986145019531, Accuracy: 0.90234375\n",
            "Step: 508900, Loss: 396.57843017578125, Accuracy: 0.90234375\n",
            "Step: 509000, Loss: 395.76739501953125, Accuracy: 0.89453125\n",
            "Step: 509100, Loss: 403.99951171875, Accuracy: 0.8671875\n",
            "Step: 509200, Loss: 391.20050048828125, Accuracy: 0.92578125\n",
            "Step: 509300, Loss: 396.82086181640625, Accuracy: 0.90234375\n",
            "Step: 509400, Loss: 401.403564453125, Accuracy: 0.87890625\n",
            "Step: 509500, Loss: 396.12200927734375, Accuracy: 0.8984375\n",
            "Step: 509600, Loss: 396.9206848144531, Accuracy: 0.890625\n",
            "Step: 509700, Loss: 393.90765380859375, Accuracy: 0.90625\n",
            "Step: 509800, Loss: 395.76959228515625, Accuracy: 0.90625\n",
            "Step: 509900, Loss: 400.02008056640625, Accuracy: 0.89453125\n",
            "Step: 510000, Loss: 400.8131103515625, Accuracy: 0.8828125\n",
            "Step: 510100, Loss: 409.5623779296875, Accuracy: 0.84375\n",
            "Step: 510200, Loss: 408.91461181640625, Accuracy: 0.84375\n",
            "Step: 510300, Loss: 405.1779479980469, Accuracy: 0.86328125\n",
            "Step: 510400, Loss: 403.66351318359375, Accuracy: 0.86328125\n",
            "Step: 510500, Loss: 401.27215576171875, Accuracy: 0.88671875\n",
            "Step: 510600, Loss: 396.59991455078125, Accuracy: 0.89453125\n",
            "Step: 510700, Loss: 394.8481750488281, Accuracy: 0.90625\n",
            "Step: 510800, Loss: 393.08258056640625, Accuracy: 0.90625\n",
            "Step: 510900, Loss: 400.7427673339844, Accuracy: 0.88671875\n",
            "Step: 511000, Loss: 398.92291259765625, Accuracy: 0.890625\n",
            "Step: 511100, Loss: 398.4940490722656, Accuracy: 0.88671875\n",
            "Step: 511200, Loss: 400.73095703125, Accuracy: 0.8828125\n",
            "Step: 511300, Loss: 400.0171203613281, Accuracy: 0.87890625\n",
            "Step: 511400, Loss: 399.4864807128906, Accuracy: 0.87890625\n",
            "Step: 511500, Loss: 397.1371765136719, Accuracy: 0.91015625\n",
            "Step: 511600, Loss: 400.585205078125, Accuracy: 0.875\n",
            "Step: 511700, Loss: 403.027099609375, Accuracy: 0.87109375\n",
            "Step: 511800, Loss: 400.6630859375, Accuracy: 0.875\n",
            "Step: 511900, Loss: 395.284423828125, Accuracy: 0.8984375\n",
            "Step: 512000, Loss: 397.54443359375, Accuracy: 0.89453125\n",
            "Step: 512100, Loss: 397.9837341308594, Accuracy: 0.890625\n",
            "Step: 512200, Loss: 399.95513916015625, Accuracy: 0.87890625\n",
            "Step: 512300, Loss: 398.4273681640625, Accuracy: 0.87890625\n",
            "Step: 512400, Loss: 401.7791442871094, Accuracy: 0.8828125\n",
            "Step: 512500, Loss: 397.15576171875, Accuracy: 0.890625\n",
            "Step: 512600, Loss: 397.634765625, Accuracy: 0.890625\n",
            "Step: 512700, Loss: 398.78631591796875, Accuracy: 0.8828125\n",
            "Step: 512800, Loss: 403.6087646484375, Accuracy: 0.86328125\n",
            "Step: 512900, Loss: 395.8017883300781, Accuracy: 0.90625\n",
            "Step: 513000, Loss: 406.09271240234375, Accuracy: 0.8515625\n",
            "Step: 513100, Loss: 400.1593933105469, Accuracy: 0.87890625\n",
            "Step: 513200, Loss: 399.34588623046875, Accuracy: 0.87890625\n",
            "Step: 513300, Loss: 399.59637451171875, Accuracy: 0.88671875\n",
            "Step: 513400, Loss: 402.5124206542969, Accuracy: 0.875\n",
            "Step: 513500, Loss: 404.9937744140625, Accuracy: 0.8671875\n",
            "Step: 513600, Loss: 401.89337158203125, Accuracy: 0.87109375\n",
            "Step: 513700, Loss: 395.28631591796875, Accuracy: 0.90625\n",
            "Step: 513800, Loss: 404.47113037109375, Accuracy: 0.85546875\n",
            "Step: 513900, Loss: 399.6723937988281, Accuracy: 0.88671875\n",
            "Step: 514000, Loss: 397.4996032714844, Accuracy: 0.89453125\n",
            "Step: 514100, Loss: 403.95819091796875, Accuracy: 0.859375\n",
            "Step: 514200, Loss: 394.04730224609375, Accuracy: 0.91015625\n",
            "Step: 514300, Loss: 404.7464599609375, Accuracy: 0.859375\n",
            "Step: 514400, Loss: 409.4897155761719, Accuracy: 0.84375\n",
            "Step: 514500, Loss: 407.19305419921875, Accuracy: 0.859375\n",
            "Step: 514600, Loss: 399.2388916015625, Accuracy: 0.87890625\n",
            "Step: 514700, Loss: 392.0365295410156, Accuracy: 0.91015625\n",
            "Step: 514800, Loss: 406.26776123046875, Accuracy: 0.84765625\n",
            "Step: 514900, Loss: 393.4110107421875, Accuracy: 0.921875\n",
            "Step: 515000, Loss: 398.7540588378906, Accuracy: 0.8828125\n",
            "Step: 515100, Loss: 402.85089111328125, Accuracy: 0.86328125\n",
            "Step: 515200, Loss: 397.0014953613281, Accuracy: 0.890625\n",
            "Step: 515300, Loss: 399.4024963378906, Accuracy: 0.88671875\n",
            "Step: 515400, Loss: 398.67022705078125, Accuracy: 0.88671875\n",
            "Step: 515500, Loss: 394.49444580078125, Accuracy: 0.8984375\n",
            "Step: 515600, Loss: 399.24072265625, Accuracy: 0.890625\n",
            "Step: 515700, Loss: 397.7630920410156, Accuracy: 0.88671875\n",
            "Step: 515800, Loss: 399.20147705078125, Accuracy: 0.88671875\n",
            "Step: 515900, Loss: 401.76153564453125, Accuracy: 0.8671875\n",
            "Step: 516000, Loss: 397.144287109375, Accuracy: 0.90625\n",
            "Step: 516100, Loss: 402.64056396484375, Accuracy: 0.875\n",
            "Step: 516200, Loss: 395.39862060546875, Accuracy: 0.90234375\n",
            "Step: 516300, Loss: 402.9627685546875, Accuracy: 0.8671875\n",
            "Step: 516400, Loss: 400.0798645019531, Accuracy: 0.890625\n",
            "Step: 516500, Loss: 404.9087219238281, Accuracy: 0.86328125\n",
            "Step: 516600, Loss: 401.09765625, Accuracy: 0.87890625\n",
            "Step: 516700, Loss: 404.18743896484375, Accuracy: 0.859375\n",
            "Step: 516800, Loss: 391.0458068847656, Accuracy: 0.91796875\n",
            "Step: 516900, Loss: 404.61962890625, Accuracy: 0.85546875\n",
            "Step: 517000, Loss: 400.54803466796875, Accuracy: 0.890625\n",
            "Step: 517100, Loss: 394.07464599609375, Accuracy: 0.91796875\n",
            "Step: 517200, Loss: 403.3480529785156, Accuracy: 0.8671875\n",
            "Step: 517300, Loss: 399.34710693359375, Accuracy: 0.87890625\n",
            "Step: 517400, Loss: 398.03106689453125, Accuracy: 0.89453125\n",
            "Step: 517500, Loss: 395.30853271484375, Accuracy: 0.8984375\n",
            "Step: 517600, Loss: 403.4588623046875, Accuracy: 0.87109375\n",
            "Step: 517700, Loss: 400.692138671875, Accuracy: 0.8828125\n",
            "Step: 517800, Loss: 396.93548583984375, Accuracy: 0.90625\n",
            "Step: 517900, Loss: 407.1361083984375, Accuracy: 0.859375\n",
            "Step: 518000, Loss: 401.7520446777344, Accuracy: 0.87890625\n",
            "Step: 518100, Loss: 393.51318359375, Accuracy: 0.90234375\n",
            "Step: 518200, Loss: 399.96746826171875, Accuracy: 0.88671875\n",
            "Step: 518300, Loss: 396.96783447265625, Accuracy: 0.89453125\n",
            "Step: 518400, Loss: 399.5780029296875, Accuracy: 0.87890625\n",
            "Step: 518500, Loss: 393.6888427734375, Accuracy: 0.91015625\n",
            "Step: 518600, Loss: 396.73052978515625, Accuracy: 0.8984375\n",
            "Step: 518700, Loss: 405.0403137207031, Accuracy: 0.8671875\n",
            "Step: 518800, Loss: 400.30084228515625, Accuracy: 0.8671875\n",
            "Step: 518900, Loss: 393.453125, Accuracy: 0.91796875\n",
            "Step: 519000, Loss: 391.80316162109375, Accuracy: 0.9140625\n",
            "Step: 519100, Loss: 400.98394775390625, Accuracy: 0.8828125\n",
            "Step: 519200, Loss: 392.51666259765625, Accuracy: 0.921875\n",
            "Step: 519300, Loss: 397.670654296875, Accuracy: 0.89453125\n",
            "Step: 519400, Loss: 400.03411865234375, Accuracy: 0.87890625\n",
            "Step: 519500, Loss: 399.2353210449219, Accuracy: 0.8828125\n",
            "Step: 519600, Loss: 397.24951171875, Accuracy: 0.8984375\n",
            "Step: 519700, Loss: 392.69232177734375, Accuracy: 0.9140625\n",
            "Step: 519800, Loss: 399.2291259765625, Accuracy: 0.8984375\n",
            "Step: 519900, Loss: 394.9603576660156, Accuracy: 0.90234375\n",
            "Step: 520000, Loss: 403.380126953125, Accuracy: 0.87890625\n",
            "Step: 520100, Loss: 405.12603759765625, Accuracy: 0.859375\n",
            "Step: 520200, Loss: 403.60009765625, Accuracy: 0.859375\n",
            "Step: 520300, Loss: 393.67547607421875, Accuracy: 0.90234375\n",
            "Step: 520400, Loss: 398.1433410644531, Accuracy: 0.890625\n",
            "Step: 520500, Loss: 405.098876953125, Accuracy: 0.8671875\n",
            "Step: 520600, Loss: 393.93414306640625, Accuracy: 0.91015625\n",
            "Step: 520700, Loss: 396.9498596191406, Accuracy: 0.890625\n",
            "Step: 520800, Loss: 401.24041748046875, Accuracy: 0.87890625\n",
            "Step: 520900, Loss: 400.89666748046875, Accuracy: 0.8828125\n",
            "Step: 521000, Loss: 400.61126708984375, Accuracy: 0.8828125\n",
            "Step: 521100, Loss: 397.4659118652344, Accuracy: 0.890625\n",
            "Step: 521200, Loss: 400.50604248046875, Accuracy: 0.8828125\n",
            "Step: 521300, Loss: 402.8758544921875, Accuracy: 0.8828125\n",
            "Step: 521400, Loss: 395.93121337890625, Accuracy: 0.89453125\n",
            "Step: 521500, Loss: 390.7279968261719, Accuracy: 0.921875\n",
            "Step: 521600, Loss: 407.0556640625, Accuracy: 0.859375\n",
            "Step: 521700, Loss: 401.6236267089844, Accuracy: 0.875\n",
            "Step: 521800, Loss: 392.70147705078125, Accuracy: 0.90625\n",
            "Step: 521900, Loss: 402.26934814453125, Accuracy: 0.87109375\n",
            "Step: 522000, Loss: 407.71002197265625, Accuracy: 0.84375\n",
            "Step: 522100, Loss: 397.9847717285156, Accuracy: 0.890625\n",
            "Step: 522200, Loss: 408.6634826660156, Accuracy: 0.8359375\n",
            "Step: 522300, Loss: 398.9417724609375, Accuracy: 0.88671875\n",
            "Step: 522400, Loss: 408.36456298828125, Accuracy: 0.85546875\n",
            "Step: 522500, Loss: 399.6659240722656, Accuracy: 0.87890625\n",
            "Step: 522600, Loss: 404.01824951171875, Accuracy: 0.8671875\n",
            "Step: 522700, Loss: 398.1211853027344, Accuracy: 0.890625\n",
            "Step: 522800, Loss: 398.30755615234375, Accuracy: 0.890625\n",
            "Step: 522900, Loss: 395.1736755371094, Accuracy: 0.90234375\n",
            "Step: 523000, Loss: 393.7712097167969, Accuracy: 0.91796875\n",
            "Step: 523100, Loss: 395.0982360839844, Accuracy: 0.90625\n",
            "Step: 523200, Loss: 400.90728759765625, Accuracy: 0.875\n",
            "Step: 523300, Loss: 399.002685546875, Accuracy: 0.90234375\n",
            "Step: 523400, Loss: 398.34771728515625, Accuracy: 0.88671875\n",
            "Step: 523500, Loss: 400.6234130859375, Accuracy: 0.88671875\n",
            "Step: 523600, Loss: 402.68035888671875, Accuracy: 0.87109375\n",
            "Step: 523700, Loss: 394.49505615234375, Accuracy: 0.9140625\n",
            "Step: 523800, Loss: 393.17340087890625, Accuracy: 0.91796875\n",
            "Step: 523900, Loss: 394.83343505859375, Accuracy: 0.90625\n",
            "Step: 524000, Loss: 393.4858093261719, Accuracy: 0.90625\n",
            "Step: 524100, Loss: 396.1386413574219, Accuracy: 0.89453125\n",
            "Step: 524200, Loss: 398.602783203125, Accuracy: 0.8828125\n",
            "Step: 524300, Loss: 397.366455078125, Accuracy: 0.89453125\n",
            "Step: 524400, Loss: 396.1107482910156, Accuracy: 0.90234375\n",
            "Step: 524500, Loss: 402.5096435546875, Accuracy: 0.87109375\n",
            "Step: 524600, Loss: 400.86053466796875, Accuracy: 0.8828125\n",
            "Step: 524700, Loss: 402.1513671875, Accuracy: 0.875\n",
            "Step: 524800, Loss: 402.212646484375, Accuracy: 0.87890625\n",
            "Step: 524900, Loss: 396.2646789550781, Accuracy: 0.89453125\n",
            "Step: 525000, Loss: 401.1513977050781, Accuracy: 0.875\n",
            "Step: 525100, Loss: 397.5570068359375, Accuracy: 0.88671875\n",
            "Step: 525200, Loss: 397.9659423828125, Accuracy: 0.90234375\n",
            "Step: 525300, Loss: 403.69903564453125, Accuracy: 0.86328125\n",
            "Step: 525400, Loss: 396.07403564453125, Accuracy: 0.90234375\n",
            "Step: 525500, Loss: 401.4005126953125, Accuracy: 0.8828125\n",
            "Step: 525600, Loss: 397.56964111328125, Accuracy: 0.890625\n",
            "Step: 525700, Loss: 398.32159423828125, Accuracy: 0.890625\n",
            "Step: 525800, Loss: 404.1875305175781, Accuracy: 0.87890625\n",
            "Step: 525900, Loss: 396.6751708984375, Accuracy: 0.91015625\n",
            "Step: 526000, Loss: 392.2269287109375, Accuracy: 0.91796875\n",
            "Step: 526100, Loss: 404.47454833984375, Accuracy: 0.86328125\n",
            "Step: 526200, Loss: 402.74407958984375, Accuracy: 0.8671875\n",
            "Step: 526300, Loss: 398.9793395996094, Accuracy: 0.88671875\n",
            "Step: 526400, Loss: 391.6973571777344, Accuracy: 0.91796875\n",
            "Step: 526500, Loss: 394.32373046875, Accuracy: 0.9140625\n",
            "Step: 526600, Loss: 405.459716796875, Accuracy: 0.85546875\n",
            "Step: 526700, Loss: 399.8206481933594, Accuracy: 0.890625\n",
            "Step: 526800, Loss: 403.5691223144531, Accuracy: 0.8671875\n",
            "Step: 526900, Loss: 401.34088134765625, Accuracy: 0.875\n",
            "Step: 527000, Loss: 404.87774658203125, Accuracy: 0.875\n",
            "Step: 527100, Loss: 399.59344482421875, Accuracy: 0.8828125\n",
            "Step: 527200, Loss: 392.3691101074219, Accuracy: 0.9140625\n",
            "Step: 527300, Loss: 398.2841796875, Accuracy: 0.890625\n",
            "Step: 527400, Loss: 394.06610107421875, Accuracy: 0.9140625\n",
            "Step: 527500, Loss: 397.91241455078125, Accuracy: 0.90234375\n",
            "Step: 527600, Loss: 394.4689025878906, Accuracy: 0.90234375\n",
            "Step: 527700, Loss: 403.1273193359375, Accuracy: 0.8671875\n",
            "Step: 527800, Loss: 401.08685302734375, Accuracy: 0.88671875\n",
            "Step: 527900, Loss: 398.84307861328125, Accuracy: 0.89453125\n",
            "Step: 528000, Loss: 399.63092041015625, Accuracy: 0.89453125\n",
            "Step: 528100, Loss: 399.9920654296875, Accuracy: 0.8828125\n",
            "Step: 528200, Loss: 405.2401123046875, Accuracy: 0.85546875\n",
            "Step: 528300, Loss: 397.0157165527344, Accuracy: 0.89453125\n",
            "Step: 528400, Loss: 394.22308349609375, Accuracy: 0.90625\n",
            "Step: 528500, Loss: 396.60882568359375, Accuracy: 0.890625\n",
            "Step: 528600, Loss: 392.96875, Accuracy: 0.91796875\n",
            "Step: 528700, Loss: 407.0306396484375, Accuracy: 0.8515625\n",
            "Step: 528800, Loss: 399.5466613769531, Accuracy: 0.88671875\n",
            "Step: 528900, Loss: 395.77850341796875, Accuracy: 0.91015625\n",
            "Step: 529000, Loss: 397.1766357421875, Accuracy: 0.90234375\n",
            "Step: 529100, Loss: 404.19219970703125, Accuracy: 0.859375\n",
            "Step: 529200, Loss: 395.86285400390625, Accuracy: 0.91015625\n",
            "Step: 529300, Loss: 408.014404296875, Accuracy: 0.84765625\n",
            "Step: 529400, Loss: 406.50830078125, Accuracy: 0.85546875\n",
            "Step: 529500, Loss: 394.2388916015625, Accuracy: 0.91015625\n",
            "Step: 529600, Loss: 400.17010498046875, Accuracy: 0.890625\n",
            "Step: 529700, Loss: 396.9293212890625, Accuracy: 0.90234375\n",
            "Step: 529800, Loss: 397.43231201171875, Accuracy: 0.89453125\n",
            "Step: 529900, Loss: 400.143798828125, Accuracy: 0.88671875\n",
            "Step: 530000, Loss: 396.30816650390625, Accuracy: 0.90234375\n",
            "Step: 530100, Loss: 398.01568603515625, Accuracy: 0.90234375\n",
            "Step: 530200, Loss: 410.17059326171875, Accuracy: 0.8359375\n",
            "Step: 530300, Loss: 401.95452880859375, Accuracy: 0.8671875\n",
            "Step: 530400, Loss: 400.816162109375, Accuracy: 0.87890625\n",
            "Step: 530500, Loss: 390.52630615234375, Accuracy: 0.9375\n",
            "Step: 530600, Loss: 402.2318115234375, Accuracy: 0.8671875\n",
            "Step: 530700, Loss: 399.9744567871094, Accuracy: 0.8828125\n",
            "Step: 530800, Loss: 404.3337707519531, Accuracy: 0.85546875\n",
            "Step: 530900, Loss: 402.48626708984375, Accuracy: 0.87890625\n",
            "Step: 531000, Loss: 399.149169921875, Accuracy: 0.87890625\n",
            "Step: 531100, Loss: 402.54132080078125, Accuracy: 0.875\n",
            "Step: 531200, Loss: 401.83673095703125, Accuracy: 0.86328125\n",
            "Step: 531300, Loss: 396.49481201171875, Accuracy: 0.90234375\n",
            "Step: 531400, Loss: 399.28082275390625, Accuracy: 0.88671875\n",
            "Step: 531500, Loss: 395.8357849121094, Accuracy: 0.91015625\n",
            "Step: 531600, Loss: 403.68975830078125, Accuracy: 0.859375\n",
            "Step: 531700, Loss: 407.953369140625, Accuracy: 0.859375\n",
            "Step: 531800, Loss: 406.841552734375, Accuracy: 0.8515625\n",
            "Step: 531900, Loss: 397.6537170410156, Accuracy: 0.89453125\n",
            "Step: 532000, Loss: 399.3260803222656, Accuracy: 0.875\n",
            "Step: 532100, Loss: 399.1448974609375, Accuracy: 0.890625\n",
            "Step: 532200, Loss: 397.97589111328125, Accuracy: 0.89453125\n",
            "Step: 532300, Loss: 397.57855224609375, Accuracy: 0.890625\n",
            "Step: 532400, Loss: 397.8135986328125, Accuracy: 0.890625\n",
            "Step: 532500, Loss: 403.80621337890625, Accuracy: 0.859375\n",
            "Step: 532600, Loss: 405.33465576171875, Accuracy: 0.8671875\n",
            "Step: 532700, Loss: 402.86358642578125, Accuracy: 0.8671875\n",
            "Step: 532800, Loss: 401.1728210449219, Accuracy: 0.890625\n",
            "Step: 532900, Loss: 395.2477722167969, Accuracy: 0.90625\n",
            "Step: 533000, Loss: 398.82183837890625, Accuracy: 0.890625\n",
            "Step: 533100, Loss: 399.0118408203125, Accuracy: 0.890625\n",
            "Step: 533200, Loss: 401.43115234375, Accuracy: 0.8828125\n",
            "Step: 533300, Loss: 401.0216979980469, Accuracy: 0.8828125\n",
            "Step: 533400, Loss: 398.65924072265625, Accuracy: 0.89453125\n",
            "Step: 533500, Loss: 403.38275146484375, Accuracy: 0.86328125\n",
            "Step: 533600, Loss: 399.3684997558594, Accuracy: 0.88671875\n",
            "Step: 533700, Loss: 398.28240966796875, Accuracy: 0.8984375\n",
            "Step: 533800, Loss: 396.62408447265625, Accuracy: 0.8984375\n",
            "Step: 533900, Loss: 387.45062255859375, Accuracy: 0.94140625\n",
            "Step: 534000, Loss: 405.9976501464844, Accuracy: 0.859375\n",
            "Step: 534100, Loss: 398.8310546875, Accuracy: 0.89453125\n",
            "Step: 534200, Loss: 403.27117919921875, Accuracy: 0.8671875\n",
            "Step: 534300, Loss: 399.8641052246094, Accuracy: 0.8828125\n",
            "Step: 534400, Loss: 402.2451171875, Accuracy: 0.875\n",
            "Step: 534500, Loss: 397.4758605957031, Accuracy: 0.89453125\n",
            "Step: 534600, Loss: 397.4239501953125, Accuracy: 0.89453125\n",
            "Step: 534700, Loss: 398.62677001953125, Accuracy: 0.89453125\n",
            "Step: 534800, Loss: 400.955078125, Accuracy: 0.88671875\n",
            "Step: 534900, Loss: 392.91400146484375, Accuracy: 0.91796875\n",
            "Step: 535000, Loss: 396.04156494140625, Accuracy: 0.89453125\n",
            "Step: 535100, Loss: 402.90069580078125, Accuracy: 0.87890625\n",
            "Step: 535200, Loss: 404.68853759765625, Accuracy: 0.87109375\n",
            "Step: 535300, Loss: 404.8393859863281, Accuracy: 0.859375\n",
            "Step: 535400, Loss: 398.58233642578125, Accuracy: 0.890625\n",
            "Step: 535500, Loss: 411.7714538574219, Accuracy: 0.8359375\n",
            "Step: 535600, Loss: 396.00701904296875, Accuracy: 0.90234375\n",
            "Step: 535700, Loss: 398.6730651855469, Accuracy: 0.88671875\n",
            "Step: 535800, Loss: 401.0291748046875, Accuracy: 0.890625\n",
            "Step: 535900, Loss: 397.7158203125, Accuracy: 0.8984375\n",
            "Step: 536000, Loss: 395.6195068359375, Accuracy: 0.8828125\n",
            "Step: 536100, Loss: 402.05889892578125, Accuracy: 0.875\n",
            "Step: 536200, Loss: 394.3725280761719, Accuracy: 0.90625\n",
            "Step: 536300, Loss: 404.1611328125, Accuracy: 0.875\n",
            "Step: 536400, Loss: 406.2625427246094, Accuracy: 0.859375\n",
            "Step: 536500, Loss: 398.761474609375, Accuracy: 0.88671875\n",
            "Step: 536600, Loss: 398.85577392578125, Accuracy: 0.8828125\n",
            "Step: 536700, Loss: 400.7376708984375, Accuracy: 0.87890625\n",
            "Step: 536800, Loss: 389.13641357421875, Accuracy: 0.921875\n",
            "Step: 536900, Loss: 403.6715087890625, Accuracy: 0.86328125\n",
            "Step: 537000, Loss: 404.0815124511719, Accuracy: 0.87109375\n",
            "Step: 537100, Loss: 403.51226806640625, Accuracy: 0.8671875\n",
            "Step: 537200, Loss: 403.5921630859375, Accuracy: 0.8671875\n",
            "Step: 537300, Loss: 400.7297058105469, Accuracy: 0.875\n",
            "Step: 537400, Loss: 390.2918395996094, Accuracy: 0.9375\n",
            "Step: 537500, Loss: 399.05450439453125, Accuracy: 0.88671875\n",
            "Step: 537600, Loss: 398.6163330078125, Accuracy: 0.890625\n",
            "Step: 537700, Loss: 401.06317138671875, Accuracy: 0.87890625\n",
            "Step: 537800, Loss: 398.3802490234375, Accuracy: 0.89453125\n",
            "Step: 537900, Loss: 400.73590087890625, Accuracy: 0.8828125\n",
            "Step: 538000, Loss: 394.9971618652344, Accuracy: 0.90625\n",
            "Step: 538100, Loss: 400.0804748535156, Accuracy: 0.89453125\n",
            "Step: 538200, Loss: 396.14019775390625, Accuracy: 0.90234375\n",
            "Step: 538300, Loss: 397.7743835449219, Accuracy: 0.890625\n",
            "Step: 538400, Loss: 399.62353515625, Accuracy: 0.88671875\n",
            "Step: 538500, Loss: 397.0330505371094, Accuracy: 0.88671875\n",
            "Step: 538600, Loss: 401.1356201171875, Accuracy: 0.875\n",
            "Step: 538700, Loss: 399.68743896484375, Accuracy: 0.8828125\n",
            "Step: 538800, Loss: 397.7669982910156, Accuracy: 0.8984375\n",
            "Step: 538900, Loss: 401.9659118652344, Accuracy: 0.875\n",
            "Step: 539000, Loss: 405.67431640625, Accuracy: 0.87109375\n",
            "Step: 539100, Loss: 406.2620849609375, Accuracy: 0.859375\n",
            "Step: 539200, Loss: 399.18682861328125, Accuracy: 0.8828125\n",
            "Step: 539300, Loss: 397.63525390625, Accuracy: 0.8984375\n",
            "Step: 539400, Loss: 400.5463562011719, Accuracy: 0.88671875\n",
            "Step: 539500, Loss: 407.1610412597656, Accuracy: 0.8515625\n",
            "Step: 539600, Loss: 399.2138671875, Accuracy: 0.87890625\n",
            "Step: 539700, Loss: 406.14788818359375, Accuracy: 0.8671875\n",
            "Step: 539800, Loss: 401.6736145019531, Accuracy: 0.87890625\n",
            "Step: 539900, Loss: 398.06365966796875, Accuracy: 0.8984375\n",
            "Step: 540000, Loss: 395.1351013183594, Accuracy: 0.90234375\n",
            "Step: 540100, Loss: 404.845947265625, Accuracy: 0.859375\n",
            "Step: 540200, Loss: 404.0870056152344, Accuracy: 0.87109375\n",
            "Step: 540300, Loss: 398.1182861328125, Accuracy: 0.89453125\n",
            "Step: 540400, Loss: 390.4949645996094, Accuracy: 0.92578125\n",
            "Step: 540500, Loss: 397.53167724609375, Accuracy: 0.90234375\n",
            "Step: 540600, Loss: 400.75653076171875, Accuracy: 0.890625\n",
            "Step: 540700, Loss: 394.95220947265625, Accuracy: 0.90625\n",
            "Step: 540800, Loss: 397.51373291015625, Accuracy: 0.90625\n",
            "Step: 540900, Loss: 401.3551025390625, Accuracy: 0.88671875\n",
            "Step: 541000, Loss: 395.682861328125, Accuracy: 0.90234375\n",
            "Step: 541100, Loss: 403.46160888671875, Accuracy: 0.8671875\n",
            "Step: 541200, Loss: 405.8729248046875, Accuracy: 0.859375\n",
            "Step: 541300, Loss: 406.35101318359375, Accuracy: 0.859375\n",
            "Step: 541400, Loss: 399.1419982910156, Accuracy: 0.890625\n",
            "Step: 541500, Loss: 399.17449951171875, Accuracy: 0.8984375\n",
            "Step: 541600, Loss: 398.8319396972656, Accuracy: 0.88671875\n",
            "Step: 541700, Loss: 400.76983642578125, Accuracy: 0.87890625\n",
            "Step: 541800, Loss: 397.82073974609375, Accuracy: 0.8984375\n",
            "Step: 541900, Loss: 394.5897216796875, Accuracy: 0.9140625\n",
            "Step: 542000, Loss: 402.70550537109375, Accuracy: 0.87109375\n",
            "Step: 542100, Loss: 397.50616455078125, Accuracy: 0.89453125\n",
            "Step: 542200, Loss: 397.497314453125, Accuracy: 0.90234375\n",
            "Step: 542300, Loss: 396.6042785644531, Accuracy: 0.8984375\n",
            "Step: 542400, Loss: 395.78594970703125, Accuracy: 0.90234375\n",
            "Step: 542500, Loss: 399.5240478515625, Accuracy: 0.88671875\n",
            "Step: 542600, Loss: 393.70404052734375, Accuracy: 0.921875\n",
            "Step: 542700, Loss: 397.74993896484375, Accuracy: 0.8984375\n",
            "Step: 542800, Loss: 396.4497375488281, Accuracy: 0.89453125\n",
            "Step: 542900, Loss: 395.8896484375, Accuracy: 0.90234375\n",
            "Step: 543000, Loss: 391.5719299316406, Accuracy: 0.91796875\n",
            "Step: 543100, Loss: 402.0615234375, Accuracy: 0.87890625\n",
            "Step: 543200, Loss: 394.97967529296875, Accuracy: 0.8984375\n",
            "Step: 543300, Loss: 399.26593017578125, Accuracy: 0.89453125\n",
            "Step: 543400, Loss: 401.8014831542969, Accuracy: 0.875\n",
            "Step: 543500, Loss: 400.892578125, Accuracy: 0.875\n",
            "Step: 543600, Loss: 404.05950927734375, Accuracy: 0.859375\n",
            "Step: 543700, Loss: 398.33538818359375, Accuracy: 0.890625\n",
            "Step: 543800, Loss: 398.2822570800781, Accuracy: 0.89453125\n",
            "Step: 543900, Loss: 391.18890380859375, Accuracy: 0.921875\n",
            "Step: 544000, Loss: 404.23406982421875, Accuracy: 0.86328125\n",
            "Step: 544100, Loss: 403.4969482421875, Accuracy: 0.875\n",
            "Step: 544200, Loss: 395.6511535644531, Accuracy: 0.90234375\n",
            "Step: 544300, Loss: 398.54022216796875, Accuracy: 0.890625\n",
            "Step: 544400, Loss: 401.218505859375, Accuracy: 0.875\n",
            "Step: 544500, Loss: 395.85650634765625, Accuracy: 0.9140625\n",
            "Step: 544600, Loss: 399.7886962890625, Accuracy: 0.875\n",
            "Step: 544700, Loss: 399.6356201171875, Accuracy: 0.8828125\n",
            "Step: 544800, Loss: 402.109130859375, Accuracy: 0.8671875\n",
            "Step: 544900, Loss: 389.45135498046875, Accuracy: 0.9296875\n",
            "Step: 545000, Loss: 398.4321594238281, Accuracy: 0.890625\n",
            "Step: 545100, Loss: 403.36322021484375, Accuracy: 0.87109375\n",
            "Step: 545200, Loss: 401.8523864746094, Accuracy: 0.88671875\n",
            "Step: 545300, Loss: 405.04541015625, Accuracy: 0.86328125\n",
            "Step: 545400, Loss: 403.62188720703125, Accuracy: 0.87109375\n",
            "Step: 545500, Loss: 395.410400390625, Accuracy: 0.90234375\n",
            "Step: 545600, Loss: 402.5507507324219, Accuracy: 0.86328125\n",
            "Step: 545700, Loss: 407.7325439453125, Accuracy: 0.8515625\n",
            "Step: 545800, Loss: 394.01947021484375, Accuracy: 0.91796875\n",
            "Step: 545900, Loss: 403.55596923828125, Accuracy: 0.8671875\n",
            "Step: 546000, Loss: 396.9527587890625, Accuracy: 0.88671875\n",
            "Step: 546100, Loss: 399.2294921875, Accuracy: 0.890625\n",
            "Step: 546200, Loss: 401.7241516113281, Accuracy: 0.87109375\n",
            "Step: 546300, Loss: 398.089111328125, Accuracy: 0.8984375\n",
            "Step: 546400, Loss: 395.8742370605469, Accuracy: 0.91015625\n",
            "Step: 546500, Loss: 405.17156982421875, Accuracy: 0.86328125\n",
            "Step: 546600, Loss: 403.858642578125, Accuracy: 0.86328125\n",
            "Step: 546700, Loss: 392.7169494628906, Accuracy: 0.91015625\n",
            "Step: 546800, Loss: 405.0234375, Accuracy: 0.859375\n",
            "Step: 546900, Loss: 403.4359130859375, Accuracy: 0.86328125\n",
            "Step: 547000, Loss: 403.38714599609375, Accuracy: 0.87109375\n",
            "Step: 547100, Loss: 398.589111328125, Accuracy: 0.8828125\n",
            "Step: 547200, Loss: 400.194580078125, Accuracy: 0.87890625\n",
            "Step: 547300, Loss: 399.61456298828125, Accuracy: 0.89453125\n",
            "Step: 547400, Loss: 403.284912109375, Accuracy: 0.875\n",
            "Step: 547500, Loss: 399.82501220703125, Accuracy: 0.8828125\n",
            "Step: 547600, Loss: 403.03082275390625, Accuracy: 0.86328125\n",
            "Step: 547700, Loss: 400.6487731933594, Accuracy: 0.8828125\n",
            "Step: 547800, Loss: 403.32867431640625, Accuracy: 0.87109375\n",
            "Step: 547900, Loss: 394.8250732421875, Accuracy: 0.90625\n",
            "Step: 548000, Loss: 404.38037109375, Accuracy: 0.875\n",
            "Step: 548100, Loss: 400.7943420410156, Accuracy: 0.8828125\n",
            "Step: 548200, Loss: 402.86517333984375, Accuracy: 0.875\n",
            "Step: 548300, Loss: 404.37701416015625, Accuracy: 0.8671875\n",
            "Step: 548400, Loss: 401.8090515136719, Accuracy: 0.8828125\n",
            "Step: 548500, Loss: 394.0914306640625, Accuracy: 0.91796875\n",
            "Step: 548600, Loss: 392.64141845703125, Accuracy: 0.91796875\n",
            "Step: 548700, Loss: 400.6805419921875, Accuracy: 0.87890625\n",
            "Step: 548800, Loss: 400.979736328125, Accuracy: 0.8828125\n",
            "Step: 548900, Loss: 389.1745910644531, Accuracy: 0.91796875\n",
            "Step: 549000, Loss: 400.36859130859375, Accuracy: 0.8828125\n",
            "Step: 549100, Loss: 400.1431884765625, Accuracy: 0.87890625\n",
            "Step: 549200, Loss: 398.4801330566406, Accuracy: 0.890625\n",
            "Step: 549300, Loss: 390.70062255859375, Accuracy: 0.92578125\n",
            "Step: 549400, Loss: 396.02581787109375, Accuracy: 0.8984375\n",
            "Step: 549500, Loss: 397.1342468261719, Accuracy: 0.90234375\n",
            "Step: 549600, Loss: 399.7999572753906, Accuracy: 0.88671875\n",
            "Step: 549700, Loss: 390.4659118652344, Accuracy: 0.92578125\n",
            "Step: 549800, Loss: 402.3105773925781, Accuracy: 0.87109375\n",
            "Step: 549900, Loss: 396.19232177734375, Accuracy: 0.90234375\n",
            "Step: 550000, Loss: 399.3534240722656, Accuracy: 0.88671875\n",
            "Step: 550100, Loss: 405.88885498046875, Accuracy: 0.859375\n",
            "Step: 550200, Loss: 397.1142883300781, Accuracy: 0.8984375\n",
            "Step: 550300, Loss: 392.3707580566406, Accuracy: 0.9140625\n",
            "Step: 550400, Loss: 398.379150390625, Accuracy: 0.8984375\n",
            "Step: 550500, Loss: 400.58770751953125, Accuracy: 0.875\n",
            "Step: 550600, Loss: 398.05499267578125, Accuracy: 0.8828125\n",
            "Step: 550700, Loss: 394.0855712890625, Accuracy: 0.91796875\n",
            "Step: 550800, Loss: 398.6868896484375, Accuracy: 0.88671875\n",
            "Step: 550900, Loss: 395.5294189453125, Accuracy: 0.90234375\n",
            "Step: 551000, Loss: 409.58795166015625, Accuracy: 0.84375\n",
            "Step: 551100, Loss: 395.8525390625, Accuracy: 0.90625\n",
            "Step: 551200, Loss: 393.32598876953125, Accuracy: 0.91796875\n",
            "Step: 551300, Loss: 407.22344970703125, Accuracy: 0.84765625\n",
            "Step: 551400, Loss: 406.39642333984375, Accuracy: 0.83984375\n",
            "Step: 551500, Loss: 395.1158447265625, Accuracy: 0.91015625\n",
            "Step: 551600, Loss: 399.886474609375, Accuracy: 0.875\n",
            "Step: 551700, Loss: 400.53546142578125, Accuracy: 0.875\n",
            "Step: 551800, Loss: 397.57476806640625, Accuracy: 0.88671875\n",
            "Step: 551900, Loss: 407.3594970703125, Accuracy: 0.8515625\n",
            "Step: 552000, Loss: 396.534423828125, Accuracy: 0.90625\n",
            "Step: 552100, Loss: 399.0422058105469, Accuracy: 0.88671875\n",
            "Step: 552200, Loss: 393.78155517578125, Accuracy: 0.9140625\n",
            "Step: 552300, Loss: 402.95025634765625, Accuracy: 0.88671875\n",
            "Step: 552400, Loss: 398.99713134765625, Accuracy: 0.890625\n",
            "Step: 552500, Loss: 408.512451171875, Accuracy: 0.84375\n",
            "Step: 552600, Loss: 406.2502746582031, Accuracy: 0.859375\n",
            "Step: 552700, Loss: 393.28912353515625, Accuracy: 0.91796875\n",
            "Step: 552800, Loss: 400.5558776855469, Accuracy: 0.87890625\n",
            "Step: 552900, Loss: 393.2105712890625, Accuracy: 0.91796875\n",
            "Step: 553000, Loss: 403.1137390136719, Accuracy: 0.86328125\n",
            "Step: 553100, Loss: 400.40283203125, Accuracy: 0.88671875\n",
            "Step: 553200, Loss: 400.42620849609375, Accuracy: 0.88671875\n",
            "Step: 553300, Loss: 403.4658203125, Accuracy: 0.8671875\n",
            "Step: 553400, Loss: 404.5793762207031, Accuracy: 0.87890625\n",
            "Step: 553500, Loss: 399.03472900390625, Accuracy: 0.89453125\n",
            "Step: 553600, Loss: 394.5771179199219, Accuracy: 0.90625\n",
            "Step: 553700, Loss: 397.62139892578125, Accuracy: 0.89453125\n",
            "Step: 553800, Loss: 401.7674560546875, Accuracy: 0.875\n",
            "Step: 553900, Loss: 394.7459411621094, Accuracy: 0.91015625\n",
            "Step: 554000, Loss: 396.82904052734375, Accuracy: 0.90234375\n",
            "Step: 554100, Loss: 395.6189270019531, Accuracy: 0.90234375\n",
            "Step: 554200, Loss: 401.88555908203125, Accuracy: 0.87890625\n",
            "Step: 554300, Loss: 401.4613037109375, Accuracy: 0.859375\n",
            "Step: 554400, Loss: 398.55902099609375, Accuracy: 0.8828125\n",
            "Step: 554500, Loss: 402.52117919921875, Accuracy: 0.875\n",
            "Step: 554600, Loss: 394.5575256347656, Accuracy: 0.90625\n",
            "Step: 554700, Loss: 408.87939453125, Accuracy: 0.84375\n",
            "Step: 554800, Loss: 403.74273681640625, Accuracy: 0.87890625\n",
            "Step: 554900, Loss: 402.09857177734375, Accuracy: 0.87890625\n",
            "Step: 555000, Loss: 390.56640625, Accuracy: 0.9296875\n",
            "Step: 555100, Loss: 406.40740966796875, Accuracy: 0.859375\n",
            "Step: 555200, Loss: 399.4903564453125, Accuracy: 0.88671875\n",
            "Step: 555300, Loss: 399.06298828125, Accuracy: 0.88671875\n",
            "Step: 555400, Loss: 401.84326171875, Accuracy: 0.875\n",
            "Step: 555500, Loss: 402.78118896484375, Accuracy: 0.8671875\n",
            "Step: 555600, Loss: 398.6407775878906, Accuracy: 0.890625\n",
            "Step: 555700, Loss: 396.5435485839844, Accuracy: 0.90625\n",
            "Step: 555800, Loss: 397.17425537109375, Accuracy: 0.90234375\n",
            "Step: 555900, Loss: 401.51300048828125, Accuracy: 0.88671875\n",
            "Step: 556000, Loss: 397.08294677734375, Accuracy: 0.90234375\n",
            "Step: 556100, Loss: 399.22125244140625, Accuracy: 0.89453125\n",
            "Step: 556200, Loss: 395.2545166015625, Accuracy: 0.91015625\n",
            "Step: 556300, Loss: 396.90875244140625, Accuracy: 0.90625\n",
            "Step: 556400, Loss: 405.68084716796875, Accuracy: 0.8515625\n",
            "Step: 556500, Loss: 404.075439453125, Accuracy: 0.87109375\n",
            "Step: 556600, Loss: 394.20208740234375, Accuracy: 0.90625\n",
            "Step: 556700, Loss: 394.2628173828125, Accuracy: 0.90625\n",
            "Step: 556800, Loss: 397.45123291015625, Accuracy: 0.890625\n",
            "Step: 556900, Loss: 397.0802307128906, Accuracy: 0.89453125\n",
            "Step: 557000, Loss: 393.7899475097656, Accuracy: 0.9140625\n",
            "Step: 557100, Loss: 393.3770751953125, Accuracy: 0.9140625\n",
            "Step: 557200, Loss: 400.1151123046875, Accuracy: 0.8828125\n",
            "Step: 557300, Loss: 395.1678771972656, Accuracy: 0.90625\n",
            "Step: 557400, Loss: 398.30487060546875, Accuracy: 0.90234375\n",
            "Step: 557500, Loss: 395.39996337890625, Accuracy: 0.91015625\n",
            "Step: 557600, Loss: 398.29290771484375, Accuracy: 0.90625\n",
            "Step: 557700, Loss: 400.32269287109375, Accuracy: 0.87890625\n",
            "Step: 557800, Loss: 400.86041259765625, Accuracy: 0.87890625\n",
            "Step: 557900, Loss: 388.90447998046875, Accuracy: 0.9375\n",
            "Step: 558000, Loss: 397.8966064453125, Accuracy: 0.88671875\n",
            "Step: 558100, Loss: 398.4151611328125, Accuracy: 0.90234375\n",
            "Step: 558200, Loss: 397.6851501464844, Accuracy: 0.890625\n",
            "Step: 558300, Loss: 403.52252197265625, Accuracy: 0.8671875\n",
            "Step: 558400, Loss: 397.2506408691406, Accuracy: 0.90234375\n",
            "Step: 558500, Loss: 398.4595947265625, Accuracy: 0.89453125\n",
            "Step: 558600, Loss: 405.9998779296875, Accuracy: 0.8515625\n",
            "Step: 558700, Loss: 397.67413330078125, Accuracy: 0.8984375\n",
            "Step: 558800, Loss: 399.8115234375, Accuracy: 0.8984375\n",
            "Step: 558900, Loss: 405.36163330078125, Accuracy: 0.86328125\n",
            "Step: 559000, Loss: 405.7149658203125, Accuracy: 0.85546875\n",
            "Step: 559100, Loss: 395.0870056152344, Accuracy: 0.91015625\n",
            "Step: 559200, Loss: 392.62200927734375, Accuracy: 0.91796875\n",
            "Step: 559300, Loss: 407.79412841796875, Accuracy: 0.84765625\n",
            "Step: 559400, Loss: 400.808837890625, Accuracy: 0.87890625\n",
            "Step: 559500, Loss: 406.7275695800781, Accuracy: 0.8671875\n",
            "Step: 559600, Loss: 398.3714904785156, Accuracy: 0.87109375\n",
            "Step: 559700, Loss: 399.14215087890625, Accuracy: 0.890625\n",
            "Step: 559800, Loss: 399.3389892578125, Accuracy: 0.88671875\n",
            "Step: 559900, Loss: 396.98834228515625, Accuracy: 0.90234375\n",
            "Step: 560000, Loss: 407.42633056640625, Accuracy: 0.8515625\n",
            "Step: 560100, Loss: 403.01629638671875, Accuracy: 0.87109375\n",
            "Step: 560200, Loss: 400.6073913574219, Accuracy: 0.875\n",
            "Step: 560300, Loss: 405.5094299316406, Accuracy: 0.8671875\n",
            "Step: 560400, Loss: 405.1604309082031, Accuracy: 0.86328125\n",
            "Step: 560500, Loss: 403.86102294921875, Accuracy: 0.86328125\n",
            "Step: 560600, Loss: 398.7037048339844, Accuracy: 0.89453125\n",
            "Step: 560700, Loss: 398.3355712890625, Accuracy: 0.890625\n",
            "Step: 560800, Loss: 403.6020812988281, Accuracy: 0.875\n",
            "Step: 560900, Loss: 400.4459228515625, Accuracy: 0.875\n",
            "Step: 561000, Loss: 403.16845703125, Accuracy: 0.8671875\n",
            "Step: 561100, Loss: 396.7806091308594, Accuracy: 0.8984375\n",
            "Step: 561200, Loss: 403.26165771484375, Accuracy: 0.875\n",
            "Step: 561300, Loss: 396.65673828125, Accuracy: 0.8984375\n",
            "Step: 561400, Loss: 404.2337646484375, Accuracy: 0.87109375\n",
            "Step: 561500, Loss: 394.9456481933594, Accuracy: 0.9140625\n",
            "Step: 561600, Loss: 401.5657043457031, Accuracy: 0.8828125\n",
            "Step: 561700, Loss: 398.26202392578125, Accuracy: 0.8828125\n",
            "Step: 561800, Loss: 398.25115966796875, Accuracy: 0.890625\n",
            "Step: 561900, Loss: 407.35552978515625, Accuracy: 0.87109375\n",
            "Step: 562000, Loss: 390.1697998046875, Accuracy: 0.9296875\n",
            "Step: 562100, Loss: 398.2298278808594, Accuracy: 0.8984375\n",
            "Step: 562200, Loss: 405.246337890625, Accuracy: 0.86328125\n",
            "Step: 562300, Loss: 394.0775146484375, Accuracy: 0.91015625\n",
            "Step: 562400, Loss: 398.34869384765625, Accuracy: 0.890625\n",
            "Step: 562500, Loss: 404.3355712890625, Accuracy: 0.875\n",
            "Step: 562600, Loss: 404.23541259765625, Accuracy: 0.8671875\n",
            "Step: 562700, Loss: 400.17401123046875, Accuracy: 0.88671875\n",
            "Step: 562800, Loss: 395.8035888671875, Accuracy: 0.90625\n",
            "Step: 562900, Loss: 401.6829833984375, Accuracy: 0.875\n",
            "Step: 563000, Loss: 404.52874755859375, Accuracy: 0.859375\n",
            "Step: 563100, Loss: 395.19390869140625, Accuracy: 0.90625\n",
            "Step: 563200, Loss: 396.79412841796875, Accuracy: 0.89453125\n",
            "Step: 563300, Loss: 399.192138671875, Accuracy: 0.89453125\n",
            "Step: 563400, Loss: 402.0447998046875, Accuracy: 0.86328125\n",
            "Step: 563500, Loss: 405.28570556640625, Accuracy: 0.8515625\n",
            "Step: 563600, Loss: 400.62030029296875, Accuracy: 0.87890625\n",
            "Step: 563700, Loss: 400.65789794921875, Accuracy: 0.88671875\n",
            "Step: 563800, Loss: 404.96575927734375, Accuracy: 0.8671875\n",
            "Step: 563900, Loss: 390.65899658203125, Accuracy: 0.9296875\n",
            "Step: 564000, Loss: 400.3702697753906, Accuracy: 0.8828125\n",
            "Step: 564100, Loss: 391.3402099609375, Accuracy: 0.9140625\n",
            "Step: 564200, Loss: 396.8334045410156, Accuracy: 0.8984375\n",
            "Step: 564300, Loss: 396.52557373046875, Accuracy: 0.90234375\n",
            "Step: 564400, Loss: 392.3702392578125, Accuracy: 0.92578125\n",
            "Step: 564500, Loss: 406.8515319824219, Accuracy: 0.859375\n",
            "Step: 564600, Loss: 401.55596923828125, Accuracy: 0.87890625\n",
            "Step: 564700, Loss: 407.2558288574219, Accuracy: 0.84765625\n",
            "Step: 564800, Loss: 405.4945068359375, Accuracy: 0.85546875\n",
            "Step: 564900, Loss: 401.6429443359375, Accuracy: 0.89453125\n",
            "Step: 565000, Loss: 398.3069152832031, Accuracy: 0.890625\n",
            "Step: 565100, Loss: 395.7740173339844, Accuracy: 0.9140625\n",
            "Step: 565200, Loss: 397.60546875, Accuracy: 0.89453125\n",
            "Step: 565300, Loss: 402.6278076171875, Accuracy: 0.88671875\n",
            "Step: 565400, Loss: 404.7096862792969, Accuracy: 0.859375\n",
            "Step: 565500, Loss: 394.224609375, Accuracy: 0.89453125\n",
            "Step: 565600, Loss: 390.616455078125, Accuracy: 0.921875\n",
            "Step: 565700, Loss: 402.8719787597656, Accuracy: 0.87109375\n",
            "Step: 565800, Loss: 399.48077392578125, Accuracy: 0.8828125\n",
            "Step: 565900, Loss: 399.9573974609375, Accuracy: 0.890625\n",
            "Step: 566000, Loss: 399.8126220703125, Accuracy: 0.88671875\n",
            "Step: 566100, Loss: 397.59454345703125, Accuracy: 0.87890625\n",
            "Step: 566200, Loss: 398.619384765625, Accuracy: 0.890625\n",
            "Step: 566300, Loss: 400.6827392578125, Accuracy: 0.8828125\n",
            "Step: 566400, Loss: 406.5986022949219, Accuracy: 0.859375\n",
            "Step: 566500, Loss: 397.5963134765625, Accuracy: 0.90234375\n",
            "Step: 566600, Loss: 392.38763427734375, Accuracy: 0.9140625\n",
            "Step: 566700, Loss: 397.11749267578125, Accuracy: 0.8984375\n",
            "Step: 566800, Loss: 401.5876770019531, Accuracy: 0.87890625\n",
            "Step: 566900, Loss: 399.23687744140625, Accuracy: 0.8828125\n",
            "Step: 567000, Loss: 398.61346435546875, Accuracy: 0.89453125\n",
            "Step: 567100, Loss: 387.26483154296875, Accuracy: 0.93359375\n",
            "Step: 567200, Loss: 398.520263671875, Accuracy: 0.88671875\n",
            "Step: 567300, Loss: 394.38580322265625, Accuracy: 0.90625\n",
            "Step: 567400, Loss: 400.909423828125, Accuracy: 0.890625\n",
            "Step: 567500, Loss: 392.20928955078125, Accuracy: 0.9140625\n",
            "Step: 567600, Loss: 405.5213928222656, Accuracy: 0.859375\n",
            "Step: 567700, Loss: 401.3980407714844, Accuracy: 0.87890625\n",
            "Step: 567800, Loss: 392.35302734375, Accuracy: 0.9140625\n",
            "Step: 567900, Loss: 401.1402893066406, Accuracy: 0.87109375\n",
            "Step: 568000, Loss: 396.28656005859375, Accuracy: 0.89453125\n",
            "Step: 568100, Loss: 397.82049560546875, Accuracy: 0.89453125\n",
            "Step: 568200, Loss: 397.40264892578125, Accuracy: 0.89453125\n",
            "Step: 568300, Loss: 407.2127990722656, Accuracy: 0.8515625\n",
            "Step: 568400, Loss: 392.619384765625, Accuracy: 0.9140625\n",
            "Step: 568500, Loss: 393.955810546875, Accuracy: 0.91015625\n",
            "Step: 568600, Loss: 398.4390869140625, Accuracy: 0.890625\n",
            "Step: 568700, Loss: 396.34100341796875, Accuracy: 0.89453125\n",
            "Step: 568800, Loss: 393.728271484375, Accuracy: 0.90625\n",
            "Step: 568900, Loss: 399.9346923828125, Accuracy: 0.89453125\n",
            "Step: 569000, Loss: 397.8442687988281, Accuracy: 0.890625\n",
            "Step: 569100, Loss: 408.2894287109375, Accuracy: 0.8359375\n",
            "Step: 569200, Loss: 400.0074157714844, Accuracy: 0.89453125\n",
            "Step: 569300, Loss: 389.93896484375, Accuracy: 0.9375\n",
            "Step: 569400, Loss: 395.0870056152344, Accuracy: 0.8984375\n",
            "Step: 569500, Loss: 399.4148254394531, Accuracy: 0.890625\n",
            "Step: 569600, Loss: 398.16400146484375, Accuracy: 0.890625\n",
            "Step: 569700, Loss: 397.7601623535156, Accuracy: 0.87890625\n",
            "Step: 569800, Loss: 391.9137268066406, Accuracy: 0.91796875\n",
            "Step: 569900, Loss: 400.2628173828125, Accuracy: 0.8828125\n",
            "Step: 570000, Loss: 403.04522705078125, Accuracy: 0.8671875\n",
            "Step: 570100, Loss: 398.69439697265625, Accuracy: 0.90234375\n",
            "Step: 570200, Loss: 415.4736328125, Accuracy: 0.828125\n",
            "Step: 570300, Loss: 395.9110107421875, Accuracy: 0.890625\n",
            "Step: 570400, Loss: 413.87347412109375, Accuracy: 0.8203125\n",
            "Step: 570500, Loss: 402.689697265625, Accuracy: 0.87109375\n",
            "Step: 570600, Loss: 396.885986328125, Accuracy: 0.89453125\n",
            "Step: 570700, Loss: 391.19281005859375, Accuracy: 0.93359375\n",
            "Step: 570800, Loss: 400.100830078125, Accuracy: 0.890625\n",
            "Step: 570900, Loss: 397.10308837890625, Accuracy: 0.9140625\n",
            "Step: 571000, Loss: 391.86773681640625, Accuracy: 0.9140625\n",
            "Step: 571100, Loss: 401.10296630859375, Accuracy: 0.87890625\n",
            "Step: 571200, Loss: 400.0469665527344, Accuracy: 0.8828125\n",
            "Step: 571300, Loss: 406.7982482910156, Accuracy: 0.84765625\n",
            "Step: 571400, Loss: 393.14434814453125, Accuracy: 0.921875\n",
            "Step: 571500, Loss: 403.3662414550781, Accuracy: 0.87109375\n",
            "Step: 571600, Loss: 399.24566650390625, Accuracy: 0.88671875\n",
            "Step: 571700, Loss: 390.8438720703125, Accuracy: 0.92578125\n",
            "Step: 571800, Loss: 398.3890075683594, Accuracy: 0.8984375\n",
            "Step: 571900, Loss: 404.3320007324219, Accuracy: 0.87109375\n",
            "Step: 572000, Loss: 398.83197021484375, Accuracy: 0.890625\n",
            "Step: 572100, Loss: 395.62994384765625, Accuracy: 0.8984375\n",
            "Step: 572200, Loss: 391.01666259765625, Accuracy: 0.9140625\n",
            "Step: 572300, Loss: 404.3486328125, Accuracy: 0.875\n",
            "Step: 572400, Loss: 401.4602966308594, Accuracy: 0.87890625\n",
            "Step: 572500, Loss: 391.0126037597656, Accuracy: 0.91796875\n",
            "Step: 572600, Loss: 396.6107482910156, Accuracy: 0.8984375\n",
            "Step: 572700, Loss: 400.63385009765625, Accuracy: 0.87109375\n",
            "Step: 572800, Loss: 399.4127197265625, Accuracy: 0.89453125\n",
            "Step: 572900, Loss: 397.0760498046875, Accuracy: 0.90234375\n",
            "Step: 573000, Loss: 401.15533447265625, Accuracy: 0.88671875\n",
            "Step: 573100, Loss: 395.1541748046875, Accuracy: 0.91015625\n",
            "Step: 573200, Loss: 397.5799560546875, Accuracy: 0.89453125\n",
            "Step: 573300, Loss: 393.70208740234375, Accuracy: 0.9140625\n",
            "Step: 573400, Loss: 395.60443115234375, Accuracy: 0.91015625\n",
            "Step: 573500, Loss: 398.4265441894531, Accuracy: 0.890625\n",
            "Step: 573600, Loss: 391.85577392578125, Accuracy: 0.91015625\n",
            "Step: 573700, Loss: 394.96136474609375, Accuracy: 0.90234375\n",
            "Step: 573800, Loss: 404.4866027832031, Accuracy: 0.87109375\n",
            "Step: 573900, Loss: 406.968994140625, Accuracy: 0.84765625\n",
            "Step: 574000, Loss: 401.1135559082031, Accuracy: 0.87109375\n",
            "Step: 574100, Loss: 392.7296447753906, Accuracy: 0.91015625\n",
            "Step: 574200, Loss: 399.74658203125, Accuracy: 0.88671875\n",
            "Step: 574300, Loss: 393.23443603515625, Accuracy: 0.91015625\n",
            "Step: 574400, Loss: 400.3806457519531, Accuracy: 0.875\n",
            "Step: 574500, Loss: 405.6158752441406, Accuracy: 0.86328125\n",
            "Step: 574600, Loss: 402.6260986328125, Accuracy: 0.86328125\n",
            "Step: 574700, Loss: 394.8130187988281, Accuracy: 0.90234375\n",
            "Step: 574800, Loss: 401.1557922363281, Accuracy: 0.89453125\n",
            "Step: 574900, Loss: 398.6939697265625, Accuracy: 0.88671875\n",
            "Step: 575000, Loss: 398.91009521484375, Accuracy: 0.875\n",
            "Step: 575100, Loss: 396.9679260253906, Accuracy: 0.89453125\n",
            "Step: 575200, Loss: 399.586669921875, Accuracy: 0.8828125\n",
            "Step: 575300, Loss: 401.6361083984375, Accuracy: 0.87890625\n",
            "Step: 575400, Loss: 401.8058776855469, Accuracy: 0.87890625\n",
            "Step: 575500, Loss: 391.541015625, Accuracy: 0.92578125\n",
            "Step: 575600, Loss: 398.99847412109375, Accuracy: 0.890625\n",
            "Step: 575700, Loss: 400.9322814941406, Accuracy: 0.87890625\n",
            "Step: 575800, Loss: 400.98126220703125, Accuracy: 0.87890625\n",
            "Step: 575900, Loss: 401.0919189453125, Accuracy: 0.890625\n",
            "Step: 576000, Loss: 403.3466796875, Accuracy: 0.875\n",
            "Step: 576100, Loss: 405.85400390625, Accuracy: 0.859375\n",
            "Step: 576200, Loss: 391.891357421875, Accuracy: 0.92578125\n",
            "Step: 576300, Loss: 402.7974853515625, Accuracy: 0.875\n",
            "Step: 576400, Loss: 403.77215576171875, Accuracy: 0.859375\n",
            "Step: 576500, Loss: 406.0258483886719, Accuracy: 0.84765625\n",
            "Step: 576600, Loss: 399.891357421875, Accuracy: 0.88671875\n",
            "Step: 576700, Loss: 392.5136413574219, Accuracy: 0.921875\n",
            "Step: 576800, Loss: 398.3299560546875, Accuracy: 0.890625\n",
            "Step: 576900, Loss: 405.6170349121094, Accuracy: 0.85546875\n",
            "Step: 577000, Loss: 399.6085205078125, Accuracy: 0.890625\n",
            "Step: 577100, Loss: 395.7812805175781, Accuracy: 0.91015625\n",
            "Step: 577200, Loss: 396.53399658203125, Accuracy: 0.90234375\n",
            "Step: 577300, Loss: 395.13067626953125, Accuracy: 0.89453125\n",
            "Step: 577400, Loss: 398.498779296875, Accuracy: 0.89453125\n",
            "Step: 577500, Loss: 404.4259948730469, Accuracy: 0.85546875\n",
            "Step: 577600, Loss: 392.24884033203125, Accuracy: 0.90625\n",
            "Step: 577700, Loss: 402.9850158691406, Accuracy: 0.87109375\n",
            "Step: 577800, Loss: 397.5116882324219, Accuracy: 0.89453125\n",
            "Step: 577900, Loss: 398.07635498046875, Accuracy: 0.90234375\n",
            "Step: 578000, Loss: 400.671142578125, Accuracy: 0.8828125\n",
            "Step: 578100, Loss: 391.83172607421875, Accuracy: 0.90625\n",
            "Step: 578200, Loss: 407.1954040527344, Accuracy: 0.86328125\n",
            "Step: 578300, Loss: 408.645751953125, Accuracy: 0.859375\n",
            "Step: 578400, Loss: 397.84429931640625, Accuracy: 0.90234375\n",
            "Step: 578500, Loss: 400.94482421875, Accuracy: 0.890625\n",
            "Step: 578600, Loss: 401.2545471191406, Accuracy: 0.87890625\n",
            "Step: 578700, Loss: 400.64019775390625, Accuracy: 0.8828125\n",
            "Step: 578800, Loss: 396.4140319824219, Accuracy: 0.90625\n",
            "Step: 578900, Loss: 389.7298583984375, Accuracy: 0.92578125\n",
            "Step: 579000, Loss: 400.4412841796875, Accuracy: 0.87890625\n",
            "Step: 579100, Loss: 407.8935546875, Accuracy: 0.8671875\n",
            "Step: 579200, Loss: 402.2236328125, Accuracy: 0.87109375\n",
            "Step: 579300, Loss: 397.7783203125, Accuracy: 0.890625\n",
            "Step: 579400, Loss: 410.3057861328125, Accuracy: 0.84375\n",
            "Step: 579500, Loss: 392.5684814453125, Accuracy: 0.9140625\n",
            "Step: 579600, Loss: 395.06036376953125, Accuracy: 0.91015625\n",
            "Step: 579700, Loss: 396.56494140625, Accuracy: 0.90234375\n",
            "Step: 579800, Loss: 398.93328857421875, Accuracy: 0.88671875\n",
            "Step: 579900, Loss: 395.89984130859375, Accuracy: 0.90234375\n",
            "Step: 580000, Loss: 399.78204345703125, Accuracy: 0.890625\n",
            "Step: 580100, Loss: 393.569580078125, Accuracy: 0.92578125\n",
            "Step: 580200, Loss: 395.5308837890625, Accuracy: 0.90625\n",
            "Step: 580300, Loss: 398.9725036621094, Accuracy: 0.8984375\n",
            "Step: 580400, Loss: 400.8234558105469, Accuracy: 0.88671875\n",
            "Step: 580500, Loss: 398.62896728515625, Accuracy: 0.88671875\n",
            "Step: 580600, Loss: 398.32049560546875, Accuracy: 0.890625\n",
            "Step: 580700, Loss: 399.4635009765625, Accuracy: 0.89453125\n",
            "Step: 580800, Loss: 400.91943359375, Accuracy: 0.87890625\n",
            "Step: 580900, Loss: 396.0479736328125, Accuracy: 0.890625\n",
            "Step: 581000, Loss: 403.0451965332031, Accuracy: 0.87109375\n",
            "Step: 581100, Loss: 400.90679931640625, Accuracy: 0.890625\n",
            "Step: 581200, Loss: 399.9975280761719, Accuracy: 0.87109375\n",
            "Step: 581300, Loss: 395.8697509765625, Accuracy: 0.91015625\n",
            "Step: 581400, Loss: 393.40673828125, Accuracy: 0.91015625\n",
            "Step: 581500, Loss: 399.93951416015625, Accuracy: 0.88671875\n",
            "Step: 581600, Loss: 401.5005798339844, Accuracy: 0.88671875\n",
            "Step: 581700, Loss: 395.25482177734375, Accuracy: 0.90234375\n",
            "Step: 581800, Loss: 394.64996337890625, Accuracy: 0.91015625\n",
            "Step: 581900, Loss: 400.0350341796875, Accuracy: 0.87109375\n",
            "Step: 582000, Loss: 402.087890625, Accuracy: 0.8671875\n",
            "Step: 582100, Loss: 395.9276123046875, Accuracy: 0.8984375\n",
            "Step: 582200, Loss: 399.7460632324219, Accuracy: 0.8984375\n",
            "Step: 582300, Loss: 401.9891662597656, Accuracy: 0.87890625\n",
            "Step: 582400, Loss: 403.1134338378906, Accuracy: 0.875\n",
            "Step: 582500, Loss: 394.70501708984375, Accuracy: 0.90234375\n",
            "Step: 582600, Loss: 398.48504638671875, Accuracy: 0.8828125\n",
            "Step: 582700, Loss: 405.8365173339844, Accuracy: 0.86328125\n",
            "Step: 582800, Loss: 406.7455749511719, Accuracy: 0.86328125\n",
            "Step: 582900, Loss: 397.9610290527344, Accuracy: 0.89453125\n",
            "Step: 583000, Loss: 395.5972900390625, Accuracy: 0.90234375\n",
            "Step: 583100, Loss: 395.87158203125, Accuracy: 0.90234375\n",
            "Step: 583200, Loss: 398.204345703125, Accuracy: 0.88671875\n",
            "Step: 583300, Loss: 391.26019287109375, Accuracy: 0.921875\n",
            "Step: 583400, Loss: 405.973876953125, Accuracy: 0.875\n",
            "Step: 583500, Loss: 397.20654296875, Accuracy: 0.88671875\n",
            "Step: 583600, Loss: 396.96124267578125, Accuracy: 0.89453125\n",
            "Step: 583700, Loss: 395.00628662109375, Accuracy: 0.90234375\n",
            "Step: 583800, Loss: 395.84832763671875, Accuracy: 0.89453125\n",
            "Step: 583900, Loss: 406.95697021484375, Accuracy: 0.85546875\n",
            "Step: 584000, Loss: 398.3432922363281, Accuracy: 0.8984375\n",
            "Step: 584100, Loss: 399.34991455078125, Accuracy: 0.890625\n",
            "Step: 584200, Loss: 402.59326171875, Accuracy: 0.87890625\n",
            "Step: 584300, Loss: 398.7825927734375, Accuracy: 0.890625\n",
            "Step: 584400, Loss: 403.000244140625, Accuracy: 0.86328125\n",
            "Step: 584500, Loss: 398.46954345703125, Accuracy: 0.91015625\n",
            "Step: 584600, Loss: 405.3307189941406, Accuracy: 0.86328125\n",
            "Step: 584700, Loss: 398.266357421875, Accuracy: 0.89453125\n",
            "Step: 584800, Loss: 400.3306884765625, Accuracy: 0.87109375\n",
            "Step: 584900, Loss: 395.412353515625, Accuracy: 0.91015625\n",
            "Step: 585000, Loss: 399.13623046875, Accuracy: 0.88671875\n",
            "Step: 585100, Loss: 402.1453552246094, Accuracy: 0.87109375\n",
            "Step: 585200, Loss: 405.3960876464844, Accuracy: 0.86328125\n",
            "Step: 585300, Loss: 400.4353332519531, Accuracy: 0.890625\n",
            "Step: 585400, Loss: 404.75445556640625, Accuracy: 0.85546875\n",
            "Step: 585500, Loss: 398.5421142578125, Accuracy: 0.90234375\n",
            "Step: 585600, Loss: 397.9131164550781, Accuracy: 0.88671875\n",
            "Step: 585700, Loss: 393.16937255859375, Accuracy: 0.91015625\n",
            "Step: 585800, Loss: 390.8115234375, Accuracy: 0.94140625\n",
            "Step: 585900, Loss: 404.7861633300781, Accuracy: 0.859375\n",
            "Step: 586000, Loss: 399.3924560546875, Accuracy: 0.875\n",
            "Step: 586100, Loss: 395.88531494140625, Accuracy: 0.90234375\n",
            "Step: 586200, Loss: 400.76409912109375, Accuracy: 0.890625\n",
            "Step: 586300, Loss: 396.5286865234375, Accuracy: 0.9140625\n",
            "Step: 586400, Loss: 398.048095703125, Accuracy: 0.90234375\n",
            "Step: 586500, Loss: 391.6683349609375, Accuracy: 0.92578125\n",
            "Step: 586600, Loss: 399.7146301269531, Accuracy: 0.890625\n",
            "Step: 586700, Loss: 400.4881591796875, Accuracy: 0.8828125\n",
            "Step: 586800, Loss: 400.6805114746094, Accuracy: 0.8828125\n",
            "Step: 586900, Loss: 398.0648498535156, Accuracy: 0.890625\n",
            "Step: 587000, Loss: 402.6964111328125, Accuracy: 0.86328125\n",
            "Step: 587100, Loss: 395.11767578125, Accuracy: 0.91015625\n",
            "Step: 587200, Loss: 410.9768981933594, Accuracy: 0.828125\n",
            "Step: 587300, Loss: 396.71539306640625, Accuracy: 0.89453125\n",
            "Step: 587400, Loss: 394.26849365234375, Accuracy: 0.8984375\n",
            "Step: 587500, Loss: 401.1080017089844, Accuracy: 0.875\n",
            "Step: 587600, Loss: 401.74102783203125, Accuracy: 0.89453125\n",
            "Step: 587700, Loss: 393.9234619140625, Accuracy: 0.9140625\n",
            "Step: 587800, Loss: 393.407958984375, Accuracy: 0.921875\n",
            "Step: 587900, Loss: 397.98748779296875, Accuracy: 0.89453125\n",
            "Step: 588000, Loss: 401.663818359375, Accuracy: 0.890625\n",
            "Step: 588100, Loss: 397.1597595214844, Accuracy: 0.89453125\n",
            "Step: 588200, Loss: 402.67059326171875, Accuracy: 0.8671875\n",
            "Step: 588300, Loss: 402.3404541015625, Accuracy: 0.87890625\n",
            "Step: 588400, Loss: 392.5799865722656, Accuracy: 0.91796875\n",
            "Step: 588500, Loss: 400.4718017578125, Accuracy: 0.890625\n",
            "Step: 588600, Loss: 401.9457092285156, Accuracy: 0.8828125\n",
            "Step: 588700, Loss: 400.78839111328125, Accuracy: 0.89453125\n",
            "Step: 588800, Loss: 405.1787414550781, Accuracy: 0.86328125\n",
            "Step: 588900, Loss: 397.19134521484375, Accuracy: 0.90625\n",
            "Step: 589000, Loss: 400.75433349609375, Accuracy: 0.875\n",
            "Step: 589100, Loss: 392.6075134277344, Accuracy: 0.93359375\n",
            "Step: 589200, Loss: 400.8603515625, Accuracy: 0.88671875\n",
            "Step: 589300, Loss: 403.20989990234375, Accuracy: 0.8671875\n",
            "Step: 589400, Loss: 396.901611328125, Accuracy: 0.8984375\n",
            "Step: 589500, Loss: 402.8236999511719, Accuracy: 0.87109375\n",
            "Step: 589600, Loss: 397.4033203125, Accuracy: 0.890625\n",
            "Step: 589700, Loss: 402.26361083984375, Accuracy: 0.875\n",
            "Step: 589800, Loss: 400.6273193359375, Accuracy: 0.8828125\n",
            "Step: 589900, Loss: 395.74774169921875, Accuracy: 0.9140625\n",
            "Step: 590000, Loss: 399.9111633300781, Accuracy: 0.890625\n",
            "Step: 590100, Loss: 394.37481689453125, Accuracy: 0.90625\n",
            "Step: 590200, Loss: 391.0550537109375, Accuracy: 0.9296875\n",
            "Step: 590300, Loss: 410.66571044921875, Accuracy: 0.8359375\n",
            "Step: 590400, Loss: 400.207763671875, Accuracy: 0.88671875\n",
            "Step: 590500, Loss: 401.2189636230469, Accuracy: 0.875\n",
            "Step: 590600, Loss: 402.682861328125, Accuracy: 0.90234375\n",
            "Step: 590700, Loss: 394.280029296875, Accuracy: 0.8984375\n",
            "Step: 590800, Loss: 394.9613037109375, Accuracy: 0.90625\n",
            "Step: 590900, Loss: 410.02899169921875, Accuracy: 0.83984375\n",
            "Step: 591000, Loss: 403.41339111328125, Accuracy: 0.87109375\n",
            "Step: 591100, Loss: 394.498779296875, Accuracy: 0.91015625\n",
            "Step: 591200, Loss: 397.50927734375, Accuracy: 0.8984375\n",
            "Step: 591300, Loss: 401.6112976074219, Accuracy: 0.87890625\n",
            "Step: 591400, Loss: 401.97802734375, Accuracy: 0.87109375\n",
            "Step: 591500, Loss: 397.7332458496094, Accuracy: 0.8984375\n",
            "Step: 591600, Loss: 400.03192138671875, Accuracy: 0.8828125\n",
            "Step: 591700, Loss: 405.153564453125, Accuracy: 0.85546875\n",
            "Step: 591800, Loss: 396.55010986328125, Accuracy: 0.890625\n",
            "Step: 591900, Loss: 395.2688903808594, Accuracy: 0.91015625\n",
            "Step: 592000, Loss: 396.376708984375, Accuracy: 0.8984375\n",
            "Step: 592100, Loss: 399.487060546875, Accuracy: 0.875\n",
            "Step: 592200, Loss: 394.289794921875, Accuracy: 0.91015625\n",
            "Step: 592300, Loss: 399.9501953125, Accuracy: 0.8828125\n",
            "Step: 592400, Loss: 396.4090576171875, Accuracy: 0.90234375\n",
            "Step: 592500, Loss: 400.74139404296875, Accuracy: 0.87890625\n",
            "Step: 592600, Loss: 399.2247009277344, Accuracy: 0.875\n",
            "Step: 592700, Loss: 393.3568420410156, Accuracy: 0.9140625\n",
            "Step: 592800, Loss: 397.6580810546875, Accuracy: 0.8984375\n",
            "Step: 592900, Loss: 400.215576171875, Accuracy: 0.88671875\n",
            "Step: 593000, Loss: 404.6331787109375, Accuracy: 0.85546875\n",
            "Step: 593100, Loss: 401.5633850097656, Accuracy: 0.88671875\n",
            "Step: 593200, Loss: 396.65264892578125, Accuracy: 0.8984375\n",
            "Step: 593300, Loss: 400.15936279296875, Accuracy: 0.890625\n",
            "Step: 593400, Loss: 397.10723876953125, Accuracy: 0.89453125\n",
            "Step: 593500, Loss: 408.38714599609375, Accuracy: 0.859375\n",
            "Step: 593600, Loss: 396.55694580078125, Accuracy: 0.90625\n",
            "Step: 593700, Loss: 392.88006591796875, Accuracy: 0.93359375\n",
            "Step: 593800, Loss: 396.1042785644531, Accuracy: 0.90625\n",
            "Step: 593900, Loss: 392.4701232910156, Accuracy: 0.9140625\n",
            "Step: 594000, Loss: 395.28546142578125, Accuracy: 0.90234375\n",
            "Step: 594100, Loss: 395.33880615234375, Accuracy: 0.90234375\n",
            "Step: 594200, Loss: 403.46661376953125, Accuracy: 0.87109375\n",
            "Step: 594300, Loss: 392.5406494140625, Accuracy: 0.91796875\n",
            "Step: 594400, Loss: 401.3277893066406, Accuracy: 0.87890625\n",
            "Step: 594500, Loss: 398.93115234375, Accuracy: 0.89453125\n",
            "Step: 594600, Loss: 397.22589111328125, Accuracy: 0.8984375\n",
            "Step: 594700, Loss: 400.54010009765625, Accuracy: 0.87890625\n",
            "Step: 594800, Loss: 409.92352294921875, Accuracy: 0.84375\n",
            "Step: 594900, Loss: 399.930419921875, Accuracy: 0.8828125\n",
            "Step: 595000, Loss: 394.07781982421875, Accuracy: 0.9140625\n",
            "Step: 595100, Loss: 396.9952392578125, Accuracy: 0.88671875\n",
            "Step: 595200, Loss: 398.57171630859375, Accuracy: 0.8828125\n",
            "Step: 595300, Loss: 391.2294006347656, Accuracy: 0.92578125\n",
            "Step: 595400, Loss: 398.2747497558594, Accuracy: 0.8984375\n",
            "Step: 595500, Loss: 394.0521240234375, Accuracy: 0.8984375\n",
            "Step: 595600, Loss: 390.6764831542969, Accuracy: 0.9296875\n",
            "Step: 595700, Loss: 397.1445007324219, Accuracy: 0.8984375\n",
            "Step: 595800, Loss: 406.158203125, Accuracy: 0.86328125\n",
            "Step: 595900, Loss: 398.9183349609375, Accuracy: 0.8984375\n",
            "Step: 596000, Loss: 398.19140625, Accuracy: 0.890625\n",
            "Step: 596100, Loss: 396.75274658203125, Accuracy: 0.89453125\n",
            "Step: 596200, Loss: 405.314208984375, Accuracy: 0.859375\n",
            "Step: 596300, Loss: 395.5502014160156, Accuracy: 0.91015625\n",
            "Step: 596400, Loss: 400.81744384765625, Accuracy: 0.87890625\n",
            "Step: 596500, Loss: 391.74725341796875, Accuracy: 0.921875\n",
            "Step: 596600, Loss: 397.9028015136719, Accuracy: 0.890625\n",
            "Step: 596700, Loss: 404.9724426269531, Accuracy: 0.859375\n",
            "Step: 596800, Loss: 398.06341552734375, Accuracy: 0.89453125\n",
            "Step: 596900, Loss: 402.5255126953125, Accuracy: 0.87109375\n",
            "Step: 597000, Loss: 397.6831970214844, Accuracy: 0.89453125\n",
            "Step: 597100, Loss: 399.1651611328125, Accuracy: 0.88671875\n",
            "Step: 597200, Loss: 400.20233154296875, Accuracy: 0.890625\n",
            "Step: 597300, Loss: 404.1452331542969, Accuracy: 0.86328125\n",
            "Step: 597400, Loss: 400.169677734375, Accuracy: 0.890625\n",
            "Step: 597500, Loss: 396.5836181640625, Accuracy: 0.90234375\n",
            "Step: 597600, Loss: 402.4403076171875, Accuracy: 0.87109375\n",
            "Step: 597700, Loss: 401.2845458984375, Accuracy: 0.88671875\n",
            "Step: 597800, Loss: 401.88531494140625, Accuracy: 0.87109375\n",
            "Step: 597900, Loss: 397.354248046875, Accuracy: 0.890625\n",
            "Step: 598000, Loss: 394.712646484375, Accuracy: 0.91015625\n",
            "Step: 598100, Loss: 399.1154479980469, Accuracy: 0.890625\n",
            "Step: 598200, Loss: 396.95623779296875, Accuracy: 0.90625\n",
            "Step: 598300, Loss: 405.72393798828125, Accuracy: 0.8515625\n",
            "Step: 598400, Loss: 403.5536193847656, Accuracy: 0.85546875\n",
            "Step: 598500, Loss: 395.41522216796875, Accuracy: 0.90234375\n",
            "Step: 598600, Loss: 396.11285400390625, Accuracy: 0.90625\n",
            "Step: 598700, Loss: 405.50872802734375, Accuracy: 0.8515625\n",
            "Step: 598800, Loss: 398.54730224609375, Accuracy: 0.8984375\n",
            "Step: 598900, Loss: 403.04510498046875, Accuracy: 0.875\n",
            "Step: 599000, Loss: 393.8057861328125, Accuracy: 0.92578125\n",
            "Step: 599100, Loss: 402.870849609375, Accuracy: 0.8828125\n",
            "Step: 599200, Loss: 398.67584228515625, Accuracy: 0.8828125\n",
            "Step: 599300, Loss: 398.0494384765625, Accuracy: 0.890625\n",
            "Step: 599400, Loss: 396.23309326171875, Accuracy: 0.90234375\n",
            "Step: 599500, Loss: 396.00848388671875, Accuracy: 0.9140625\n",
            "Step: 599600, Loss: 398.03778076171875, Accuracy: 0.8984375\n",
            "Step: 599700, Loss: 399.060791015625, Accuracy: 0.88671875\n",
            "Step: 599800, Loss: 398.297119140625, Accuracy: 0.90625\n",
            "Step: 599900, Loss: 401.16973876953125, Accuracy: 0.875\n",
            "Step: 600000, Loss: 400.36773681640625, Accuracy: 0.87890625\n",
            "Step: 600100, Loss: 399.685791015625, Accuracy: 0.8828125\n",
            "Step: 600200, Loss: 410.35174560546875, Accuracy: 0.84375\n",
            "Step: 600300, Loss: 402.1486511230469, Accuracy: 0.88671875\n",
            "Step: 600400, Loss: 395.0636901855469, Accuracy: 0.9140625\n",
            "Step: 600500, Loss: 399.6341247558594, Accuracy: 0.87890625\n",
            "Step: 600600, Loss: 397.7310485839844, Accuracy: 0.890625\n",
            "Step: 600700, Loss: 397.0319519042969, Accuracy: 0.8984375\n",
            "Step: 600800, Loss: 408.9173278808594, Accuracy: 0.84375\n",
            "Step: 600900, Loss: 406.6317443847656, Accuracy: 0.8828125\n",
            "Step: 601000, Loss: 397.9796142578125, Accuracy: 0.890625\n",
            "Step: 601100, Loss: 396.2206115722656, Accuracy: 0.9140625\n",
            "Step: 601200, Loss: 400.6348876953125, Accuracy: 0.8828125\n",
            "Step: 601300, Loss: 399.6011962890625, Accuracy: 0.88671875\n",
            "Step: 601400, Loss: 400.38397216796875, Accuracy: 0.8828125\n",
            "Step: 601500, Loss: 400.7745361328125, Accuracy: 0.87890625\n",
            "Step: 601600, Loss: 397.2941589355469, Accuracy: 0.91015625\n",
            "Step: 601700, Loss: 403.25384521484375, Accuracy: 0.87109375\n",
            "Step: 601800, Loss: 395.83013916015625, Accuracy: 0.90625\n",
            "Step: 601900, Loss: 393.63482666015625, Accuracy: 0.91796875\n",
            "Step: 602000, Loss: 396.86370849609375, Accuracy: 0.90234375\n",
            "Step: 602100, Loss: 400.7554931640625, Accuracy: 0.890625\n",
            "Step: 602200, Loss: 407.36981201171875, Accuracy: 0.8515625\n",
            "Step: 602300, Loss: 400.1279602050781, Accuracy: 0.875\n",
            "Step: 602400, Loss: 399.2503356933594, Accuracy: 0.875\n",
            "Step: 602500, Loss: 399.3360595703125, Accuracy: 0.890625\n",
            "Step: 602600, Loss: 398.35406494140625, Accuracy: 0.8984375\n",
            "Step: 602700, Loss: 393.89556884765625, Accuracy: 0.9140625\n",
            "Step: 602800, Loss: 396.6365661621094, Accuracy: 0.89453125\n",
            "Step: 602900, Loss: 395.8428955078125, Accuracy: 0.90625\n",
            "Step: 603000, Loss: 399.5579528808594, Accuracy: 0.890625\n",
            "Step: 603100, Loss: 396.72265625, Accuracy: 0.90234375\n",
            "Step: 603200, Loss: 389.22015380859375, Accuracy: 0.9296875\n",
            "Step: 603300, Loss: 400.0208740234375, Accuracy: 0.89453125\n",
            "Step: 603400, Loss: 399.38397216796875, Accuracy: 0.88671875\n",
            "Step: 603500, Loss: 400.8365173339844, Accuracy: 0.8828125\n",
            "Step: 603600, Loss: 400.2818908691406, Accuracy: 0.89453125\n",
            "Step: 603700, Loss: 395.8989562988281, Accuracy: 0.89453125\n",
            "Step: 603800, Loss: 400.13983154296875, Accuracy: 0.890625\n",
            "Step: 603900, Loss: 393.354736328125, Accuracy: 0.9140625\n",
            "Step: 604000, Loss: 396.8150634765625, Accuracy: 0.90234375\n",
            "Step: 604100, Loss: 404.419921875, Accuracy: 0.8671875\n",
            "Step: 604200, Loss: 395.61688232421875, Accuracy: 0.90234375\n",
            "Step: 604300, Loss: 399.8967590332031, Accuracy: 0.89453125\n",
            "Step: 604400, Loss: 398.885009765625, Accuracy: 0.88671875\n",
            "Step: 604500, Loss: 404.0758361816406, Accuracy: 0.88671875\n",
            "Step: 604600, Loss: 401.33331298828125, Accuracy: 0.88671875\n",
            "Step: 604700, Loss: 398.007568359375, Accuracy: 0.90625\n",
            "Step: 604800, Loss: 400.2575988769531, Accuracy: 0.87890625\n",
            "Step: 604900, Loss: 404.3115234375, Accuracy: 0.8671875\n",
            "Step: 605000, Loss: 400.0444030761719, Accuracy: 0.87890625\n",
            "Step: 605100, Loss: 403.7050476074219, Accuracy: 0.85546875\n",
            "Step: 605200, Loss: 405.6309814453125, Accuracy: 0.87890625\n",
            "Step: 605300, Loss: 400.878173828125, Accuracy: 0.87890625\n",
            "Step: 605400, Loss: 395.7164306640625, Accuracy: 0.90625\n",
            "Step: 605500, Loss: 400.1470031738281, Accuracy: 0.8828125\n",
            "Step: 605600, Loss: 400.1985168457031, Accuracy: 0.88671875\n",
            "Step: 605700, Loss: 395.33111572265625, Accuracy: 0.8984375\n",
            "Step: 605800, Loss: 394.19683837890625, Accuracy: 0.9140625\n",
            "Step: 605900, Loss: 396.23187255859375, Accuracy: 0.89453125\n",
            "Step: 606000, Loss: 401.63226318359375, Accuracy: 0.87109375\n",
            "Step: 606100, Loss: 404.170654296875, Accuracy: 0.87890625\n",
            "Step: 606200, Loss: 397.3614501953125, Accuracy: 0.890625\n",
            "Step: 606300, Loss: 404.5377197265625, Accuracy: 0.87109375\n",
            "Step: 606400, Loss: 397.3204650878906, Accuracy: 0.90234375\n",
            "Step: 606500, Loss: 404.376708984375, Accuracy: 0.87109375\n",
            "Step: 606600, Loss: 401.7919006347656, Accuracy: 0.8671875\n",
            "Step: 606700, Loss: 404.73858642578125, Accuracy: 0.85546875\n",
            "Step: 606800, Loss: 409.4461364746094, Accuracy: 0.84765625\n",
            "Step: 606900, Loss: 396.5974426269531, Accuracy: 0.90234375\n",
            "Step: 607000, Loss: 401.27691650390625, Accuracy: 0.890625\n",
            "Step: 607100, Loss: 406.17156982421875, Accuracy: 0.85546875\n",
            "Step: 607200, Loss: 398.05303955078125, Accuracy: 0.89453125\n",
            "Step: 607300, Loss: 403.6274719238281, Accuracy: 0.87109375\n",
            "Step: 607400, Loss: 397.99322509765625, Accuracy: 0.88671875\n",
            "Step: 607500, Loss: 400.7407531738281, Accuracy: 0.890625\n",
            "Step: 607600, Loss: 400.5457763671875, Accuracy: 0.8828125\n",
            "Step: 607700, Loss: 394.44146728515625, Accuracy: 0.91015625\n",
            "Step: 607800, Loss: 400.7518310546875, Accuracy: 0.89453125\n",
            "Step: 607900, Loss: 398.8470458984375, Accuracy: 0.89453125\n",
            "Step: 608000, Loss: 399.9697265625, Accuracy: 0.890625\n",
            "Step: 608100, Loss: 405.9809265136719, Accuracy: 0.85546875\n",
            "Step: 608200, Loss: 399.3286437988281, Accuracy: 0.88671875\n",
            "Step: 608300, Loss: 393.589111328125, Accuracy: 0.9140625\n",
            "Step: 608400, Loss: 404.98992919921875, Accuracy: 0.87109375\n",
            "Step: 608500, Loss: 397.5018310546875, Accuracy: 0.90625\n",
            "Step: 608600, Loss: 403.5130920410156, Accuracy: 0.87890625\n",
            "Step: 608700, Loss: 404.23974609375, Accuracy: 0.8671875\n",
            "Step: 608800, Loss: 399.9194641113281, Accuracy: 0.8828125\n",
            "Step: 608900, Loss: 394.06640625, Accuracy: 0.90625\n",
            "Step: 609000, Loss: 393.0770568847656, Accuracy: 0.921875\n",
            "Step: 609100, Loss: 402.72784423828125, Accuracy: 0.8828125\n",
            "Step: 609200, Loss: 401.332275390625, Accuracy: 0.88671875\n",
            "Step: 609300, Loss: 396.53271484375, Accuracy: 0.90234375\n",
            "Step: 609400, Loss: 406.7964172363281, Accuracy: 0.85546875\n",
            "Step: 609500, Loss: 397.2743225097656, Accuracy: 0.90625\n",
            "Step: 609600, Loss: 406.12176513671875, Accuracy: 0.8671875\n",
            "Step: 609700, Loss: 402.6365661621094, Accuracy: 0.8828125\n",
            "Step: 609800, Loss: 404.44134521484375, Accuracy: 0.8671875\n",
            "Step: 609900, Loss: 403.19903564453125, Accuracy: 0.875\n",
            "Step: 610000, Loss: 401.7950439453125, Accuracy: 0.87890625\n",
            "Step: 610100, Loss: 400.1961669921875, Accuracy: 0.88671875\n",
            "Step: 610200, Loss: 393.08648681640625, Accuracy: 0.9140625\n",
            "Step: 610300, Loss: 398.3013916015625, Accuracy: 0.88671875\n",
            "Step: 610400, Loss: 398.2620849609375, Accuracy: 0.90625\n",
            "Step: 610500, Loss: 398.36572265625, Accuracy: 0.8984375\n",
            "Step: 610600, Loss: 395.3489990234375, Accuracy: 0.8984375\n",
            "Step: 610700, Loss: 396.2586364746094, Accuracy: 0.90625\n",
            "Step: 610800, Loss: 395.97491455078125, Accuracy: 0.8984375\n",
            "Step: 610900, Loss: 404.4760437011719, Accuracy: 0.8671875\n",
            "Step: 611000, Loss: 394.32568359375, Accuracy: 0.9140625\n",
            "Step: 611100, Loss: 395.9256286621094, Accuracy: 0.90234375\n",
            "Step: 611200, Loss: 396.0753173828125, Accuracy: 0.90234375\n",
            "Step: 611300, Loss: 403.70849609375, Accuracy: 0.875\n",
            "Step: 611400, Loss: 408.0746765136719, Accuracy: 0.8515625\n",
            "Step: 611500, Loss: 406.0751953125, Accuracy: 0.87109375\n",
            "Step: 611600, Loss: 399.8763122558594, Accuracy: 0.89453125\n",
            "Step: 611700, Loss: 401.1094970703125, Accuracy: 0.8828125\n",
            "Step: 611800, Loss: 403.76141357421875, Accuracy: 0.86328125\n",
            "Step: 611900, Loss: 397.28399658203125, Accuracy: 0.90234375\n",
            "Step: 612000, Loss: 399.06903076171875, Accuracy: 0.890625\n",
            "Step: 612100, Loss: 399.225341796875, Accuracy: 0.875\n",
            "Step: 612200, Loss: 405.8283996582031, Accuracy: 0.87109375\n",
            "Step: 612300, Loss: 391.95599365234375, Accuracy: 0.8984375\n",
            "Step: 612400, Loss: 397.7637939453125, Accuracy: 0.89453125\n",
            "Step: 612500, Loss: 405.8326110839844, Accuracy: 0.8515625\n",
            "Step: 612600, Loss: 411.0863342285156, Accuracy: 0.83203125\n",
            "Step: 612700, Loss: 395.6156921386719, Accuracy: 0.9140625\n",
            "Step: 612800, Loss: 403.1560363769531, Accuracy: 0.8828125\n",
            "Step: 612900, Loss: 389.96820068359375, Accuracy: 0.92578125\n",
            "Step: 613000, Loss: 402.58935546875, Accuracy: 0.87890625\n",
            "Step: 613100, Loss: 396.68414306640625, Accuracy: 0.91015625\n",
            "Step: 613200, Loss: 404.00579833984375, Accuracy: 0.87109375\n",
            "Step: 613300, Loss: 397.73638916015625, Accuracy: 0.8984375\n",
            "Step: 613400, Loss: 398.4270935058594, Accuracy: 0.90625\n",
            "Step: 613500, Loss: 403.86712646484375, Accuracy: 0.875\n",
            "Step: 613600, Loss: 399.054443359375, Accuracy: 0.8828125\n",
            "Step: 613700, Loss: 408.5214538574219, Accuracy: 0.8515625\n",
            "Step: 613800, Loss: 400.6849365234375, Accuracy: 0.88671875\n",
            "Step: 613900, Loss: 392.159912109375, Accuracy: 0.93359375\n",
            "Step: 614000, Loss: 396.8841857910156, Accuracy: 0.89453125\n",
            "Step: 614100, Loss: 399.5771789550781, Accuracy: 0.90234375\n",
            "Step: 614200, Loss: 395.10418701171875, Accuracy: 0.90625\n",
            "Step: 614300, Loss: 398.0927734375, Accuracy: 0.890625\n",
            "Step: 614400, Loss: 398.5503234863281, Accuracy: 0.890625\n",
            "Step: 614500, Loss: 393.4385986328125, Accuracy: 0.91015625\n",
            "Step: 614600, Loss: 396.6999816894531, Accuracy: 0.91015625\n",
            "Step: 614700, Loss: 398.4336853027344, Accuracy: 0.88671875\n",
            "Step: 614800, Loss: 393.846435546875, Accuracy: 0.9140625\n",
            "Step: 614900, Loss: 397.13348388671875, Accuracy: 0.8984375\n",
            "Step: 615000, Loss: 399.9393310546875, Accuracy: 0.87890625\n",
            "Step: 615100, Loss: 399.2469482421875, Accuracy: 0.89453125\n",
            "Step: 615200, Loss: 394.033447265625, Accuracy: 0.90625\n",
            "Step: 615300, Loss: 400.3673400878906, Accuracy: 0.890625\n",
            "Step: 615400, Loss: 391.80474853515625, Accuracy: 0.9140625\n",
            "Step: 615500, Loss: 404.4398193359375, Accuracy: 0.86328125\n",
            "Step: 615600, Loss: 402.19940185546875, Accuracy: 0.8828125\n",
            "Step: 615700, Loss: 403.238525390625, Accuracy: 0.8671875\n",
            "Step: 615800, Loss: 400.68853759765625, Accuracy: 0.89453125\n",
            "Step: 615900, Loss: 404.57977294921875, Accuracy: 0.8828125\n",
            "Step: 616000, Loss: 397.1996765136719, Accuracy: 0.8984375\n",
            "Step: 616100, Loss: 388.2415466308594, Accuracy: 0.9296875\n",
            "Step: 616200, Loss: 395.54058837890625, Accuracy: 0.90625\n",
            "Step: 616300, Loss: 402.8303527832031, Accuracy: 0.87109375\n",
            "Step: 616400, Loss: 392.5264587402344, Accuracy: 0.9296875\n",
            "Step: 616500, Loss: 401.30859375, Accuracy: 0.8828125\n",
            "Step: 616600, Loss: 401.7962646484375, Accuracy: 0.890625\n",
            "Step: 616700, Loss: 400.60821533203125, Accuracy: 0.890625\n",
            "Step: 616800, Loss: 396.7254333496094, Accuracy: 0.90234375\n",
            "Step: 616900, Loss: 402.23486328125, Accuracy: 0.87890625\n",
            "Step: 617000, Loss: 396.245849609375, Accuracy: 0.91015625\n",
            "Step: 617100, Loss: 397.619384765625, Accuracy: 0.890625\n",
            "Step: 617200, Loss: 405.7567443847656, Accuracy: 0.8515625\n",
            "Step: 617300, Loss: 405.0414733886719, Accuracy: 0.859375\n",
            "Step: 617400, Loss: 392.1856384277344, Accuracy: 0.9296875\n",
            "Step: 617500, Loss: 403.50921630859375, Accuracy: 0.8828125\n",
            "Step: 617600, Loss: 405.04803466796875, Accuracy: 0.86328125\n",
            "Step: 617700, Loss: 401.41717529296875, Accuracy: 0.87890625\n",
            "Step: 617800, Loss: 409.0872802734375, Accuracy: 0.85546875\n",
            "Step: 617900, Loss: 401.5993957519531, Accuracy: 0.87890625\n",
            "Step: 618000, Loss: 405.04449462890625, Accuracy: 0.8515625\n",
            "Step: 618100, Loss: 407.4769287109375, Accuracy: 0.85546875\n",
            "Step: 618200, Loss: 406.1424560546875, Accuracy: 0.86328125\n",
            "Step: 618300, Loss: 397.52117919921875, Accuracy: 0.89453125\n",
            "Step: 618400, Loss: 397.48443603515625, Accuracy: 0.8984375\n",
            "Step: 618500, Loss: 400.3033447265625, Accuracy: 0.88671875\n",
            "Step: 618600, Loss: 397.3553466796875, Accuracy: 0.8984375\n",
            "Step: 618700, Loss: 397.9925231933594, Accuracy: 0.8984375\n",
            "Step: 618800, Loss: 399.31390380859375, Accuracy: 0.87890625\n",
            "Step: 618900, Loss: 401.17388916015625, Accuracy: 0.88671875\n",
            "Step: 619000, Loss: 403.6592712402344, Accuracy: 0.86328125\n",
            "Step: 619100, Loss: 390.86810302734375, Accuracy: 0.92578125\n",
            "Step: 619200, Loss: 402.5966796875, Accuracy: 0.88671875\n",
            "Step: 619300, Loss: 405.6766357421875, Accuracy: 0.83984375\n",
            "Step: 619400, Loss: 398.7944030761719, Accuracy: 0.890625\n",
            "Step: 619500, Loss: 404.8521728515625, Accuracy: 0.8828125\n",
            "Step: 619600, Loss: 400.4242858886719, Accuracy: 0.89453125\n",
            "Step: 619700, Loss: 398.0260925292969, Accuracy: 0.90625\n",
            "Step: 619800, Loss: 404.1075439453125, Accuracy: 0.875\n",
            "Step: 619900, Loss: 401.89556884765625, Accuracy: 0.875\n",
            "Step: 620000, Loss: 399.79461669921875, Accuracy: 0.890625\n",
            "Step: 620100, Loss: 395.72747802734375, Accuracy: 0.90234375\n",
            "Step: 620200, Loss: 402.5826416015625, Accuracy: 0.86328125\n",
            "Step: 620300, Loss: 398.80352783203125, Accuracy: 0.89453125\n",
            "Step: 620400, Loss: 395.695068359375, Accuracy: 0.90234375\n",
            "Step: 620500, Loss: 399.17034912109375, Accuracy: 0.8984375\n",
            "Step: 620600, Loss: 401.32659912109375, Accuracy: 0.88671875\n",
            "Step: 620700, Loss: 400.5542297363281, Accuracy: 0.88671875\n",
            "Step: 620800, Loss: 398.27850341796875, Accuracy: 0.8984375\n",
            "Step: 620900, Loss: 400.0909423828125, Accuracy: 0.88671875\n",
            "Step: 621000, Loss: 398.1014404296875, Accuracy: 0.89453125\n",
            "Step: 621100, Loss: 405.76873779296875, Accuracy: 0.85546875\n",
            "Step: 621200, Loss: 399.9638671875, Accuracy: 0.890625\n",
            "Step: 621300, Loss: 400.764892578125, Accuracy: 0.89453125\n",
            "Step: 621400, Loss: 398.693115234375, Accuracy: 0.890625\n",
            "Step: 621500, Loss: 399.8885192871094, Accuracy: 0.87890625\n",
            "Step: 621600, Loss: 401.93243408203125, Accuracy: 0.87890625\n",
            "Step: 621700, Loss: 397.68707275390625, Accuracy: 0.890625\n",
            "Step: 621800, Loss: 403.21435546875, Accuracy: 0.89453125\n",
            "Step: 621900, Loss: 404.1758117675781, Accuracy: 0.87109375\n",
            "Step: 622000, Loss: 405.97467041015625, Accuracy: 0.87109375\n",
            "Step: 622100, Loss: 396.2188720703125, Accuracy: 0.90234375\n",
            "Step: 622200, Loss: 396.4919128417969, Accuracy: 0.8984375\n",
            "Step: 622300, Loss: 393.2412414550781, Accuracy: 0.921875\n",
            "Step: 622400, Loss: 399.2676696777344, Accuracy: 0.89453125\n",
            "Step: 622500, Loss: 401.93536376953125, Accuracy: 0.86328125\n",
            "Step: 622600, Loss: 404.566162109375, Accuracy: 0.8671875\n",
            "Step: 622700, Loss: 402.28765869140625, Accuracy: 0.8671875\n",
            "Step: 622800, Loss: 396.0455017089844, Accuracy: 0.89453125\n",
            "Step: 622900, Loss: 400.50030517578125, Accuracy: 0.88671875\n",
            "Step: 623000, Loss: 398.2199401855469, Accuracy: 0.88671875\n",
            "Step: 623100, Loss: 397.60772705078125, Accuracy: 0.8984375\n",
            "Step: 623200, Loss: 403.04095458984375, Accuracy: 0.8671875\n",
            "Step: 623300, Loss: 404.7568359375, Accuracy: 0.87109375\n",
            "Step: 623400, Loss: 402.70037841796875, Accuracy: 0.8828125\n",
            "Step: 623500, Loss: 396.69976806640625, Accuracy: 0.90625\n",
            "Step: 623600, Loss: 396.61090087890625, Accuracy: 0.90234375\n",
            "Step: 623700, Loss: 394.7823486328125, Accuracy: 0.91015625\n",
            "Step: 623800, Loss: 402.06640625, Accuracy: 0.875\n",
            "Step: 623900, Loss: 388.1269836425781, Accuracy: 0.9375\n",
            "Step: 624000, Loss: 395.7743835449219, Accuracy: 0.9140625\n",
            "Step: 624100, Loss: 394.19818115234375, Accuracy: 0.90625\n",
            "Step: 624200, Loss: 405.6972961425781, Accuracy: 0.859375\n",
            "Step: 624300, Loss: 401.2913818359375, Accuracy: 0.875\n",
            "Step: 624400, Loss: 395.7022705078125, Accuracy: 0.8984375\n",
            "Step: 624500, Loss: 398.4474182128906, Accuracy: 0.87890625\n",
            "Step: 624600, Loss: 396.69195556640625, Accuracy: 0.8984375\n",
            "Step: 624700, Loss: 400.88189697265625, Accuracy: 0.8828125\n",
            "Step: 624800, Loss: 399.7569885253906, Accuracy: 0.890625\n",
            "Step: 624900, Loss: 399.76934814453125, Accuracy: 0.89453125\n",
            "Step: 625000, Loss: 403.54095458984375, Accuracy: 0.86328125\n",
            "Step: 625100, Loss: 398.9380187988281, Accuracy: 0.89453125\n",
            "Step: 625200, Loss: 395.75677490234375, Accuracy: 0.90625\n",
            "Step: 625300, Loss: 398.5167541503906, Accuracy: 0.890625\n",
            "Step: 625400, Loss: 393.14801025390625, Accuracy: 0.89453125\n",
            "Step: 625500, Loss: 399.87646484375, Accuracy: 0.8828125\n",
            "Step: 625600, Loss: 401.7748107910156, Accuracy: 0.8984375\n",
            "Step: 625700, Loss: 401.642578125, Accuracy: 0.87109375\n",
            "Step: 625800, Loss: 393.168701171875, Accuracy: 0.921875\n",
            "Step: 625900, Loss: 396.00677490234375, Accuracy: 0.90234375\n",
            "Step: 626000, Loss: 402.6189270019531, Accuracy: 0.87109375\n",
            "Step: 626100, Loss: 395.06671142578125, Accuracy: 0.91015625\n",
            "Step: 626200, Loss: 396.502685546875, Accuracy: 0.90625\n",
            "Step: 626300, Loss: 403.35931396484375, Accuracy: 0.87109375\n",
            "Step: 626400, Loss: 393.63751220703125, Accuracy: 0.9140625\n",
            "Step: 626500, Loss: 401.8943786621094, Accuracy: 0.8828125\n",
            "Step: 626600, Loss: 389.59759521484375, Accuracy: 0.93359375\n",
            "Step: 626700, Loss: 397.0551452636719, Accuracy: 0.90625\n",
            "Step: 626800, Loss: 401.9355163574219, Accuracy: 0.875\n",
            "Step: 626900, Loss: 410.678466796875, Accuracy: 0.83984375\n",
            "Step: 627000, Loss: 400.03472900390625, Accuracy: 0.89453125\n",
            "Step: 627100, Loss: 400.61090087890625, Accuracy: 0.87890625\n",
            "Step: 627200, Loss: 397.23333740234375, Accuracy: 0.90234375\n",
            "Step: 627300, Loss: 398.4412841796875, Accuracy: 0.890625\n",
            "Step: 627400, Loss: 398.9953918457031, Accuracy: 0.8984375\n",
            "Step: 627500, Loss: 392.31707763671875, Accuracy: 0.92578125\n",
            "Step: 627600, Loss: 395.7811584472656, Accuracy: 0.91796875\n",
            "Step: 627700, Loss: 392.95025634765625, Accuracy: 0.921875\n",
            "Step: 627800, Loss: 395.0716552734375, Accuracy: 0.91796875\n",
            "Step: 627900, Loss: 404.3406677246094, Accuracy: 0.86328125\n",
            "Step: 628000, Loss: 398.725341796875, Accuracy: 0.890625\n",
            "Step: 628100, Loss: 400.06207275390625, Accuracy: 0.8828125\n",
            "Step: 628200, Loss: 400.38031005859375, Accuracy: 0.88671875\n",
            "Step: 628300, Loss: 402.0059814453125, Accuracy: 0.875\n",
            "Step: 628400, Loss: 396.6012878417969, Accuracy: 0.89453125\n",
            "Step: 628500, Loss: 393.3411560058594, Accuracy: 0.9140625\n",
            "Step: 628600, Loss: 402.8251953125, Accuracy: 0.8671875\n",
            "Step: 628700, Loss: 400.2215881347656, Accuracy: 0.8828125\n",
            "Step: 628800, Loss: 395.028564453125, Accuracy: 0.921875\n",
            "Step: 628900, Loss: 399.7974548339844, Accuracy: 0.89453125\n",
            "Step: 629000, Loss: 394.16168212890625, Accuracy: 0.91796875\n",
            "Step: 629100, Loss: 390.880615234375, Accuracy: 0.921875\n",
            "Step: 629200, Loss: 398.1671142578125, Accuracy: 0.89453125\n",
            "Step: 629300, Loss: 400.65863037109375, Accuracy: 0.89453125\n",
            "Step: 629400, Loss: 402.4277648925781, Accuracy: 0.859375\n",
            "Step: 629500, Loss: 397.70062255859375, Accuracy: 0.88671875\n",
            "Step: 629600, Loss: 399.175537109375, Accuracy: 0.8828125\n",
            "Step: 629700, Loss: 402.237060546875, Accuracy: 0.87109375\n",
            "Step: 629800, Loss: 394.86285400390625, Accuracy: 0.90625\n",
            "Step: 629900, Loss: 392.80682373046875, Accuracy: 0.91015625\n",
            "Step: 630000, Loss: 398.7391357421875, Accuracy: 0.88671875\n",
            "Step: 630100, Loss: 402.49603271484375, Accuracy: 0.86328125\n",
            "Step: 630200, Loss: 396.64300537109375, Accuracy: 0.91015625\n",
            "Step: 630300, Loss: 396.1570739746094, Accuracy: 0.890625\n",
            "Step: 630400, Loss: 395.54962158203125, Accuracy: 0.90625\n",
            "Step: 630500, Loss: 403.0222473144531, Accuracy: 0.8671875\n",
            "Step: 630600, Loss: 393.2660827636719, Accuracy: 0.91015625\n",
            "Step: 630700, Loss: 403.73980712890625, Accuracy: 0.87109375\n",
            "Step: 630800, Loss: 394.89752197265625, Accuracy: 0.9140625\n",
            "Step: 630900, Loss: 404.21697998046875, Accuracy: 0.8828125\n",
            "Step: 631000, Loss: 392.0140380859375, Accuracy: 0.921875\n",
            "Step: 631100, Loss: 400.6589050292969, Accuracy: 0.87890625\n",
            "Step: 631200, Loss: 404.0592346191406, Accuracy: 0.8828125\n",
            "Step: 631300, Loss: 399.1468505859375, Accuracy: 0.875\n",
            "Step: 631400, Loss: 392.1649169921875, Accuracy: 0.9296875\n",
            "Step: 631500, Loss: 397.27593994140625, Accuracy: 0.89453125\n",
            "Step: 631600, Loss: 398.1640625, Accuracy: 0.890625\n",
            "Step: 631700, Loss: 400.9207763671875, Accuracy: 0.87890625\n",
            "Step: 631800, Loss: 395.8374938964844, Accuracy: 0.91015625\n",
            "Step: 631900, Loss: 401.47576904296875, Accuracy: 0.875\n",
            "Step: 632000, Loss: 398.294677734375, Accuracy: 0.89453125\n",
            "Step: 632100, Loss: 389.1482238769531, Accuracy: 0.9453125\n",
            "Step: 632200, Loss: 398.71539306640625, Accuracy: 0.89453125\n",
            "Step: 632300, Loss: 396.2239990234375, Accuracy: 0.90625\n",
            "Step: 632400, Loss: 389.79876708984375, Accuracy: 0.92578125\n",
            "Step: 632500, Loss: 398.76239013671875, Accuracy: 0.8828125\n",
            "Step: 632600, Loss: 402.7026672363281, Accuracy: 0.87109375\n",
            "Step: 632700, Loss: 395.14996337890625, Accuracy: 0.90234375\n",
            "Step: 632800, Loss: 391.3070373535156, Accuracy: 0.9375\n",
            "Step: 632900, Loss: 397.55474853515625, Accuracy: 0.90625\n",
            "Step: 633000, Loss: 400.7996826171875, Accuracy: 0.875\n",
            "Step: 633100, Loss: 407.4296875, Accuracy: 0.859375\n",
            "Step: 633200, Loss: 395.44805908203125, Accuracy: 0.90234375\n",
            "Step: 633300, Loss: 403.8529052734375, Accuracy: 0.8671875\n",
            "Step: 633400, Loss: 404.27587890625, Accuracy: 0.859375\n",
            "Step: 633500, Loss: 400.8939208984375, Accuracy: 0.87890625\n",
            "Step: 633600, Loss: 398.792236328125, Accuracy: 0.88671875\n",
            "Step: 633700, Loss: 397.71661376953125, Accuracy: 0.8984375\n",
            "Step: 633800, Loss: 404.0232849121094, Accuracy: 0.87890625\n",
            "Step: 633900, Loss: 395.79296875, Accuracy: 0.90234375\n",
            "Step: 634000, Loss: 401.5672912597656, Accuracy: 0.86328125\n",
            "Step: 634100, Loss: 398.47998046875, Accuracy: 0.8828125\n",
            "Step: 634200, Loss: 403.96856689453125, Accuracy: 0.87890625\n",
            "Step: 634300, Loss: 399.36480712890625, Accuracy: 0.87890625\n",
            "Step: 634400, Loss: 405.10394287109375, Accuracy: 0.86328125\n",
            "Step: 634500, Loss: 400.1591796875, Accuracy: 0.89453125\n",
            "Step: 634600, Loss: 402.2039489746094, Accuracy: 0.8828125\n",
            "Step: 634700, Loss: 401.9070739746094, Accuracy: 0.88671875\n",
            "Step: 634800, Loss: 400.867431640625, Accuracy: 0.890625\n",
            "Step: 634900, Loss: 400.4851379394531, Accuracy: 0.88671875\n",
            "Step: 635000, Loss: 402.8509826660156, Accuracy: 0.8671875\n",
            "Step: 635100, Loss: 400.062744140625, Accuracy: 0.88671875\n",
            "Step: 635200, Loss: 402.2166748046875, Accuracy: 0.88671875\n",
            "Step: 635300, Loss: 408.8642578125, Accuracy: 0.84375\n",
            "Step: 635400, Loss: 399.11260986328125, Accuracy: 0.8984375\n",
            "Step: 635500, Loss: 398.927978515625, Accuracy: 0.90234375\n",
            "Step: 635600, Loss: 401.01348876953125, Accuracy: 0.890625\n",
            "Step: 635700, Loss: 392.00885009765625, Accuracy: 0.92578125\n",
            "Step: 635800, Loss: 403.9071044921875, Accuracy: 0.87890625\n",
            "Step: 635900, Loss: 398.5230712890625, Accuracy: 0.90234375\n",
            "Step: 636000, Loss: 406.77374267578125, Accuracy: 0.84765625\n",
            "Step: 636100, Loss: 393.241943359375, Accuracy: 0.921875\n",
            "Step: 636200, Loss: 397.6338806152344, Accuracy: 0.89453125\n",
            "Step: 636300, Loss: 401.74810791015625, Accuracy: 0.875\n",
            "Step: 636400, Loss: 400.24310302734375, Accuracy: 0.890625\n",
            "Step: 636500, Loss: 394.69927978515625, Accuracy: 0.9140625\n",
            "Step: 636600, Loss: 398.2585754394531, Accuracy: 0.89453125\n",
            "Step: 636700, Loss: 401.0025329589844, Accuracy: 0.87890625\n",
            "Step: 636800, Loss: 394.4924011230469, Accuracy: 0.91015625\n",
            "Step: 636900, Loss: 398.4422607421875, Accuracy: 0.89453125\n",
            "Step: 637000, Loss: 402.81231689453125, Accuracy: 0.875\n",
            "Step: 637100, Loss: 398.0694885253906, Accuracy: 0.890625\n",
            "Step: 637200, Loss: 405.766845703125, Accuracy: 0.859375\n",
            "Step: 637300, Loss: 399.3924560546875, Accuracy: 0.88671875\n",
            "Step: 637400, Loss: 402.8193359375, Accuracy: 0.875\n",
            "Step: 637500, Loss: 399.4940185546875, Accuracy: 0.88671875\n",
            "Step: 637600, Loss: 400.37042236328125, Accuracy: 0.89453125\n",
            "Step: 637700, Loss: 397.565185546875, Accuracy: 0.8984375\n",
            "Step: 637800, Loss: 394.4631042480469, Accuracy: 0.90625\n",
            "Step: 637900, Loss: 399.451416015625, Accuracy: 0.890625\n",
            "Step: 638000, Loss: 397.4150390625, Accuracy: 0.90234375\n",
            "Step: 638100, Loss: 396.6697082519531, Accuracy: 0.90625\n",
            "Step: 638200, Loss: 395.5426025390625, Accuracy: 0.91796875\n",
            "Step: 638300, Loss: 400.9632263183594, Accuracy: 0.859375\n",
            "Step: 638400, Loss: 396.64093017578125, Accuracy: 0.90234375\n",
            "Step: 638500, Loss: 400.24566650390625, Accuracy: 0.88671875\n",
            "Step: 638600, Loss: 399.6703186035156, Accuracy: 0.87890625\n",
            "Step: 638700, Loss: 402.37750244140625, Accuracy: 0.87890625\n",
            "Step: 638800, Loss: 408.82958984375, Accuracy: 0.8515625\n",
            "Step: 638900, Loss: 394.217529296875, Accuracy: 0.921875\n",
            "Step: 639000, Loss: 398.71026611328125, Accuracy: 0.8984375\n",
            "Step: 639100, Loss: 398.66290283203125, Accuracy: 0.88671875\n",
            "Step: 639200, Loss: 397.44525146484375, Accuracy: 0.8984375\n",
            "Step: 639300, Loss: 395.860595703125, Accuracy: 0.91015625\n",
            "Step: 639400, Loss: 396.4644775390625, Accuracy: 0.90625\n",
            "Step: 639500, Loss: 399.4505615234375, Accuracy: 0.88671875\n",
            "Step: 639600, Loss: 405.649169921875, Accuracy: 0.87890625\n",
            "Step: 639700, Loss: 392.35516357421875, Accuracy: 0.91796875\n",
            "Step: 639800, Loss: 403.0181884765625, Accuracy: 0.8671875\n",
            "Step: 639900, Loss: 393.90771484375, Accuracy: 0.91796875\n",
            "Step: 640000, Loss: 396.86871337890625, Accuracy: 0.89453125\n",
            "Step: 640100, Loss: 399.58880615234375, Accuracy: 0.88671875\n",
            "Step: 640200, Loss: 397.40216064453125, Accuracy: 0.90234375\n",
            "Step: 640300, Loss: 401.2939453125, Accuracy: 0.8828125\n",
            "Step: 640400, Loss: 394.76446533203125, Accuracy: 0.91015625\n",
            "Step: 640500, Loss: 405.5742492675781, Accuracy: 0.85546875\n",
            "Step: 640600, Loss: 399.6020202636719, Accuracy: 0.88671875\n",
            "Step: 640700, Loss: 400.2062683105469, Accuracy: 0.8828125\n",
            "Step: 640800, Loss: 403.90667724609375, Accuracy: 0.88671875\n",
            "Step: 640900, Loss: 393.97021484375, Accuracy: 0.91015625\n",
            "Step: 641000, Loss: 403.86248779296875, Accuracy: 0.87109375\n",
            "Step: 641100, Loss: 400.88427734375, Accuracy: 0.87890625\n",
            "Step: 641200, Loss: 400.1570739746094, Accuracy: 0.87109375\n",
            "Step: 641300, Loss: 396.5662536621094, Accuracy: 0.89453125\n",
            "Step: 641400, Loss: 396.7084655761719, Accuracy: 0.90625\n",
            "Step: 641500, Loss: 404.144287109375, Accuracy: 0.8828125\n",
            "Step: 641600, Loss: 403.41290283203125, Accuracy: 0.87109375\n",
            "Step: 641700, Loss: 399.33056640625, Accuracy: 0.90234375\n",
            "Step: 641800, Loss: 393.5969543457031, Accuracy: 0.90625\n",
            "Step: 641900, Loss: 403.3238525390625, Accuracy: 0.87890625\n",
            "Step: 642000, Loss: 392.8785705566406, Accuracy: 0.91796875\n",
            "Step: 642100, Loss: 394.77227783203125, Accuracy: 0.90234375\n",
            "Step: 642200, Loss: 400.4612731933594, Accuracy: 0.87890625\n",
            "Step: 642300, Loss: 391.0281677246094, Accuracy: 0.91796875\n",
            "Step: 642400, Loss: 389.963623046875, Accuracy: 0.94140625\n",
            "Step: 642500, Loss: 398.2648010253906, Accuracy: 0.88671875\n",
            "Step: 642600, Loss: 397.6077880859375, Accuracy: 0.890625\n",
            "Step: 642700, Loss: 403.23992919921875, Accuracy: 0.875\n",
            "Step: 642800, Loss: 397.997802734375, Accuracy: 0.890625\n",
            "Step: 642900, Loss: 390.77001953125, Accuracy: 0.92578125\n",
            "Step: 643000, Loss: 405.1611328125, Accuracy: 0.8671875\n",
            "Step: 643100, Loss: 396.523193359375, Accuracy: 0.90234375\n",
            "Step: 643200, Loss: 394.4281005859375, Accuracy: 0.91015625\n",
            "Step: 643300, Loss: 417.2250061035156, Accuracy: 0.82421875\n",
            "Step: 643400, Loss: 401.6374816894531, Accuracy: 0.875\n",
            "Step: 643500, Loss: 398.3900146484375, Accuracy: 0.90234375\n",
            "Step: 643600, Loss: 390.34039306640625, Accuracy: 0.92578125\n",
            "Step: 643700, Loss: 399.88629150390625, Accuracy: 0.89453125\n",
            "Step: 643800, Loss: 399.2309875488281, Accuracy: 0.890625\n",
            "Step: 643900, Loss: 396.8854675292969, Accuracy: 0.91796875\n",
            "Step: 644000, Loss: 393.0071105957031, Accuracy: 0.9140625\n",
            "Step: 644100, Loss: 400.49969482421875, Accuracy: 0.88671875\n",
            "Step: 644200, Loss: 401.1796875, Accuracy: 0.875\n",
            "Step: 644300, Loss: 396.1582336425781, Accuracy: 0.90625\n",
            "Step: 644400, Loss: 394.3890686035156, Accuracy: 0.90625\n",
            "Step: 644500, Loss: 403.69769287109375, Accuracy: 0.87109375\n",
            "Step: 644600, Loss: 396.4150695800781, Accuracy: 0.90234375\n",
            "Step: 644700, Loss: 396.6195068359375, Accuracy: 0.8984375\n",
            "Step: 644800, Loss: 399.0675964355469, Accuracy: 0.8984375\n",
            "Step: 644900, Loss: 399.0916748046875, Accuracy: 0.88671875\n",
            "Step: 645000, Loss: 397.41162109375, Accuracy: 0.89453125\n",
            "Step: 645100, Loss: 400.503662109375, Accuracy: 0.8828125\n",
            "Step: 645200, Loss: 399.1878356933594, Accuracy: 0.875\n",
            "Step: 645300, Loss: 405.9454345703125, Accuracy: 0.8671875\n",
            "Step: 645400, Loss: 399.2931213378906, Accuracy: 0.890625\n",
            "Step: 645500, Loss: 400.14404296875, Accuracy: 0.890625\n",
            "Step: 645600, Loss: 394.6787109375, Accuracy: 0.90625\n",
            "Step: 645700, Loss: 392.29449462890625, Accuracy: 0.91796875\n",
            "Step: 645800, Loss: 393.2724304199219, Accuracy: 0.9140625\n",
            "Step: 645900, Loss: 402.88330078125, Accuracy: 0.8671875\n",
            "Step: 646000, Loss: 395.67657470703125, Accuracy: 0.91015625\n",
            "Step: 646100, Loss: 395.203125, Accuracy: 0.9140625\n",
            "Step: 646200, Loss: 401.8198547363281, Accuracy: 0.88671875\n",
            "Step: 646300, Loss: 394.1568298339844, Accuracy: 0.91015625\n",
            "Step: 646400, Loss: 403.2547607421875, Accuracy: 0.87109375\n",
            "Step: 646500, Loss: 396.858154296875, Accuracy: 0.90234375\n",
            "Step: 646600, Loss: 395.8798828125, Accuracy: 0.91015625\n",
            "Step: 646700, Loss: 397.95184326171875, Accuracy: 0.890625\n",
            "Step: 646800, Loss: 396.41455078125, Accuracy: 0.90625\n",
            "Step: 646900, Loss: 394.9245300292969, Accuracy: 0.921875\n",
            "Step: 647000, Loss: 397.9111633300781, Accuracy: 0.890625\n",
            "Step: 647100, Loss: 399.95001220703125, Accuracy: 0.890625\n",
            "Step: 647200, Loss: 402.347412109375, Accuracy: 0.87890625\n",
            "Step: 647300, Loss: 405.191162109375, Accuracy: 0.875\n",
            "Step: 647400, Loss: 398.6925048828125, Accuracy: 0.90234375\n",
            "Step: 647500, Loss: 398.18170166015625, Accuracy: 0.89453125\n",
            "Step: 647600, Loss: 410.11895751953125, Accuracy: 0.84375\n",
            "Step: 647700, Loss: 398.35943603515625, Accuracy: 0.88671875\n",
            "Step: 647800, Loss: 403.444091796875, Accuracy: 0.87890625\n",
            "Step: 647900, Loss: 399.2737731933594, Accuracy: 0.89453125\n",
            "Step: 648000, Loss: 407.15313720703125, Accuracy: 0.8359375\n",
            "Step: 648100, Loss: 393.19049072265625, Accuracy: 0.921875\n",
            "Step: 648200, Loss: 399.0173034667969, Accuracy: 0.890625\n",
            "Step: 648300, Loss: 402.44317626953125, Accuracy: 0.87109375\n",
            "Step: 648400, Loss: 396.5323791503906, Accuracy: 0.89453125\n",
            "Step: 648500, Loss: 394.04730224609375, Accuracy: 0.90625\n",
            "Step: 648600, Loss: 401.0462951660156, Accuracy: 0.8828125\n",
            "Step: 648700, Loss: 402.650146484375, Accuracy: 0.8828125\n",
            "Step: 648800, Loss: 397.77655029296875, Accuracy: 0.90625\n",
            "Step: 648900, Loss: 402.62896728515625, Accuracy: 0.8671875\n",
            "Step: 649000, Loss: 395.6776428222656, Accuracy: 0.89453125\n",
            "Step: 649100, Loss: 396.9512634277344, Accuracy: 0.89453125\n",
            "Step: 649200, Loss: 409.180419921875, Accuracy: 0.8515625\n",
            "Step: 649300, Loss: 397.00244140625, Accuracy: 0.89453125\n",
            "Step: 649400, Loss: 399.1563720703125, Accuracy: 0.8984375\n",
            "Step: 649500, Loss: 402.98345947265625, Accuracy: 0.875\n",
            "Step: 649600, Loss: 405.4532775878906, Accuracy: 0.85546875\n",
            "Step: 649700, Loss: 399.926025390625, Accuracy: 0.88671875\n",
            "Step: 649800, Loss: 393.07489013671875, Accuracy: 0.91796875\n",
            "Step: 649900, Loss: 396.1484375, Accuracy: 0.91015625\n",
            "Step: 650000, Loss: 400.91949462890625, Accuracy: 0.89453125\n",
            "Step: 650100, Loss: 402.2791748046875, Accuracy: 0.875\n",
            "Step: 650200, Loss: 406.0887756347656, Accuracy: 0.86328125\n",
            "Step: 650300, Loss: 397.045654296875, Accuracy: 0.890625\n",
            "Step: 650400, Loss: 396.23797607421875, Accuracy: 0.90234375\n",
            "Step: 650500, Loss: 395.3137512207031, Accuracy: 0.90234375\n",
            "Step: 650600, Loss: 397.777099609375, Accuracy: 0.91015625\n",
            "Step: 650700, Loss: 392.75592041015625, Accuracy: 0.90234375\n",
            "Step: 650800, Loss: 406.00665283203125, Accuracy: 0.859375\n",
            "Step: 650900, Loss: 401.56427001953125, Accuracy: 0.890625\n",
            "Step: 651000, Loss: 398.5505065917969, Accuracy: 0.87890625\n",
            "Step: 651100, Loss: 405.3545837402344, Accuracy: 0.875\n",
            "Step: 651200, Loss: 393.5045471191406, Accuracy: 0.921875\n",
            "Step: 651300, Loss: 401.6188049316406, Accuracy: 0.8828125\n",
            "Step: 651400, Loss: 400.93206787109375, Accuracy: 0.8828125\n",
            "Step: 651500, Loss: 401.72650146484375, Accuracy: 0.8828125\n",
            "Step: 651600, Loss: 397.20745849609375, Accuracy: 0.8984375\n",
            "Step: 651700, Loss: 406.5224609375, Accuracy: 0.8671875\n",
            "Step: 651800, Loss: 401.5783996582031, Accuracy: 0.875\n",
            "Step: 651900, Loss: 400.0621032714844, Accuracy: 0.88671875\n",
            "Step: 652000, Loss: 401.6070556640625, Accuracy: 0.8828125\n",
            "Step: 652100, Loss: 397.2192687988281, Accuracy: 0.90625\n",
            "Step: 652200, Loss: 402.52227783203125, Accuracy: 0.87890625\n",
            "Step: 652300, Loss: 401.75311279296875, Accuracy: 0.87890625\n",
            "Step: 652400, Loss: 396.2510681152344, Accuracy: 0.8984375\n",
            "Step: 652500, Loss: 395.77850341796875, Accuracy: 0.90234375\n",
            "Step: 652600, Loss: 404.1453552246094, Accuracy: 0.8828125\n",
            "Step: 652700, Loss: 398.84356689453125, Accuracy: 0.8828125\n",
            "Step: 652800, Loss: 394.7267761230469, Accuracy: 0.9140625\n",
            "Step: 652900, Loss: 391.934814453125, Accuracy: 0.9296875\n",
            "Step: 653000, Loss: 396.6370544433594, Accuracy: 0.90234375\n",
            "Step: 653100, Loss: 401.8044128417969, Accuracy: 0.86328125\n",
            "Step: 653200, Loss: 391.5665588378906, Accuracy: 0.9140625\n",
            "Step: 653300, Loss: 395.2240295410156, Accuracy: 0.90625\n",
            "Step: 653400, Loss: 398.111328125, Accuracy: 0.90234375\n",
            "Step: 653500, Loss: 397.1474609375, Accuracy: 0.90625\n",
            "Step: 653600, Loss: 393.5384521484375, Accuracy: 0.91015625\n",
            "Step: 653700, Loss: 402.5128173828125, Accuracy: 0.8671875\n",
            "Step: 653800, Loss: 399.58355712890625, Accuracy: 0.89453125\n",
            "Step: 653900, Loss: 399.95050048828125, Accuracy: 0.89453125\n",
            "Step: 654000, Loss: 399.14251708984375, Accuracy: 0.890625\n",
            "Step: 654100, Loss: 402.30426025390625, Accuracy: 0.875\n",
            "Step: 654200, Loss: 403.575439453125, Accuracy: 0.8671875\n",
            "Step: 654300, Loss: 398.7223205566406, Accuracy: 0.890625\n",
            "Step: 654400, Loss: 403.2377624511719, Accuracy: 0.88671875\n",
            "Step: 654500, Loss: 390.5445556640625, Accuracy: 0.93359375\n",
            "Step: 654600, Loss: 397.8202209472656, Accuracy: 0.89453125\n",
            "Step: 654700, Loss: 391.8858642578125, Accuracy: 0.9296875\n",
            "Step: 654800, Loss: 398.842529296875, Accuracy: 0.88671875\n",
            "Step: 654900, Loss: 397.29852294921875, Accuracy: 0.89453125\n",
            "Step: 655000, Loss: 402.7668762207031, Accuracy: 0.8671875\n",
            "Step: 655100, Loss: 401.05615234375, Accuracy: 0.875\n",
            "Step: 655200, Loss: 393.2715148925781, Accuracy: 0.91796875\n",
            "Step: 655300, Loss: 394.90570068359375, Accuracy: 0.90625\n",
            "Step: 655400, Loss: 404.9559020996094, Accuracy: 0.859375\n",
            "Step: 655500, Loss: 399.436767578125, Accuracy: 0.88671875\n",
            "Step: 655600, Loss: 394.221923828125, Accuracy: 0.91796875\n",
            "Step: 655700, Loss: 405.40570068359375, Accuracy: 0.875\n",
            "Step: 655800, Loss: 400.6287841796875, Accuracy: 0.890625\n",
            "Step: 655900, Loss: 402.956298828125, Accuracy: 0.87109375\n",
            "Step: 656000, Loss: 399.1014099121094, Accuracy: 0.890625\n",
            "Step: 656100, Loss: 396.21112060546875, Accuracy: 0.8984375\n",
            "Step: 656200, Loss: 397.47900390625, Accuracy: 0.90234375\n",
            "Step: 656300, Loss: 400.69549560546875, Accuracy: 0.90234375\n",
            "Step: 656400, Loss: 401.543212890625, Accuracy: 0.87890625\n",
            "Step: 656500, Loss: 399.9027099609375, Accuracy: 0.87890625\n",
            "Step: 656600, Loss: 400.1731262207031, Accuracy: 0.87890625\n",
            "Step: 656700, Loss: 398.8984680175781, Accuracy: 0.88671875\n",
            "Step: 656800, Loss: 394.7171630859375, Accuracy: 0.90234375\n",
            "Step: 656900, Loss: 404.6032409667969, Accuracy: 0.87109375\n",
            "Step: 657000, Loss: 402.5564880371094, Accuracy: 0.87109375\n",
            "Step: 657100, Loss: 396.5162658691406, Accuracy: 0.8984375\n",
            "Step: 657200, Loss: 398.0411376953125, Accuracy: 0.8984375\n",
            "Step: 657300, Loss: 399.1669921875, Accuracy: 0.88671875\n",
            "Step: 657400, Loss: 393.3159484863281, Accuracy: 0.91015625\n",
            "Step: 657500, Loss: 395.1737060546875, Accuracy: 0.90625\n",
            "Step: 657600, Loss: 393.1683654785156, Accuracy: 0.92578125\n",
            "Step: 657700, Loss: 392.3200378417969, Accuracy: 0.91796875\n",
            "Step: 657800, Loss: 396.92803955078125, Accuracy: 0.8828125\n",
            "Step: 657900, Loss: 394.6424560546875, Accuracy: 0.9140625\n",
            "Step: 658000, Loss: 401.41259765625, Accuracy: 0.8671875\n",
            "Step: 658100, Loss: 400.86810302734375, Accuracy: 0.88671875\n",
            "Step: 658200, Loss: 399.12530517578125, Accuracy: 0.8828125\n",
            "Step: 658300, Loss: 402.5814208984375, Accuracy: 0.8828125\n",
            "Step: 658400, Loss: 397.4022521972656, Accuracy: 0.90234375\n",
            "Step: 658500, Loss: 394.1973876953125, Accuracy: 0.921875\n",
            "Step: 658600, Loss: 394.3989562988281, Accuracy: 0.90625\n",
            "Step: 658700, Loss: 399.1448974609375, Accuracy: 0.88671875\n",
            "Step: 658800, Loss: 404.0240783691406, Accuracy: 0.875\n",
            "Step: 658900, Loss: 401.5309753417969, Accuracy: 0.8828125\n",
            "Step: 659000, Loss: 398.8251953125, Accuracy: 0.89453125\n",
            "Step: 659100, Loss: 408.8988952636719, Accuracy: 0.86328125\n",
            "Step: 659200, Loss: 398.6170349121094, Accuracy: 0.890625\n",
            "Step: 659300, Loss: 406.3799133300781, Accuracy: 0.86328125\n",
            "Step: 659400, Loss: 397.6180725097656, Accuracy: 0.890625\n",
            "Step: 659500, Loss: 400.06512451171875, Accuracy: 0.88671875\n",
            "Step: 659600, Loss: 392.7156677246094, Accuracy: 0.91796875\n",
            "Step: 659700, Loss: 394.66436767578125, Accuracy: 0.91796875\n",
            "Step: 659800, Loss: 397.62799072265625, Accuracy: 0.90234375\n",
            "Step: 659900, Loss: 399.1361999511719, Accuracy: 0.890625\n",
            "Step: 660000, Loss: 395.36981201171875, Accuracy: 0.90625\n",
            "Step: 660100, Loss: 400.96331787109375, Accuracy: 0.87890625\n",
            "Step: 660200, Loss: 399.162109375, Accuracy: 0.89453125\n",
            "Step: 660300, Loss: 398.3830261230469, Accuracy: 0.890625\n",
            "Step: 660400, Loss: 398.28106689453125, Accuracy: 0.8984375\n",
            "Step: 660500, Loss: 395.59637451171875, Accuracy: 0.91015625\n",
            "Step: 660600, Loss: 397.4875793457031, Accuracy: 0.890625\n",
            "Step: 660700, Loss: 400.8468017578125, Accuracy: 0.89453125\n",
            "Step: 660800, Loss: 397.3639221191406, Accuracy: 0.89453125\n",
            "Step: 660900, Loss: 406.8123779296875, Accuracy: 0.86328125\n",
            "Step: 661000, Loss: 406.50750732421875, Accuracy: 0.86328125\n",
            "Step: 661100, Loss: 402.2208251953125, Accuracy: 0.88671875\n",
            "Step: 661200, Loss: 403.21368408203125, Accuracy: 0.87109375\n",
            "Step: 661300, Loss: 404.3446960449219, Accuracy: 0.86328125\n",
            "Step: 661400, Loss: 395.69476318359375, Accuracy: 0.90625\n",
            "Step: 661500, Loss: 399.1822509765625, Accuracy: 0.8828125\n",
            "Step: 661600, Loss: 398.614990234375, Accuracy: 0.8984375\n",
            "Step: 661700, Loss: 394.6891174316406, Accuracy: 0.91015625\n",
            "Step: 661800, Loss: 391.72479248046875, Accuracy: 0.91796875\n",
            "Step: 661900, Loss: 401.895751953125, Accuracy: 0.890625\n",
            "Step: 662000, Loss: 403.4808349609375, Accuracy: 0.87109375\n",
            "Step: 662100, Loss: 390.6328125, Accuracy: 0.9296875\n",
            "Step: 662200, Loss: 397.8706359863281, Accuracy: 0.890625\n",
            "Step: 662300, Loss: 394.888427734375, Accuracy: 0.9140625\n",
            "Step: 662400, Loss: 401.0028076171875, Accuracy: 0.8828125\n",
            "Step: 662500, Loss: 401.89654541015625, Accuracy: 0.87890625\n",
            "Step: 662600, Loss: 395.47021484375, Accuracy: 0.921875\n",
            "Step: 662700, Loss: 406.7083740234375, Accuracy: 0.87890625\n",
            "Step: 662800, Loss: 394.849853515625, Accuracy: 0.90625\n",
            "Step: 662900, Loss: 393.1656494140625, Accuracy: 0.9140625\n",
            "Step: 663000, Loss: 405.70367431640625, Accuracy: 0.87890625\n",
            "Step: 663100, Loss: 391.7117614746094, Accuracy: 0.921875\n",
            "Step: 663200, Loss: 399.9598388671875, Accuracy: 0.88671875\n",
            "Step: 663300, Loss: 402.83758544921875, Accuracy: 0.875\n",
            "Step: 663400, Loss: 400.17620849609375, Accuracy: 0.8828125\n",
            "Step: 663500, Loss: 401.9737548828125, Accuracy: 0.88671875\n",
            "Step: 663600, Loss: 392.10858154296875, Accuracy: 0.921875\n",
            "Step: 663700, Loss: 396.5709228515625, Accuracy: 0.90625\n",
            "Step: 663800, Loss: 406.6357116699219, Accuracy: 0.8671875\n",
            "Step: 663900, Loss: 399.8521728515625, Accuracy: 0.88671875\n",
            "Step: 664000, Loss: 390.8681945800781, Accuracy: 0.91796875\n",
            "Step: 664100, Loss: 398.71221923828125, Accuracy: 0.890625\n",
            "Step: 664200, Loss: 393.34771728515625, Accuracy: 0.9140625\n",
            "Step: 664300, Loss: 396.243896484375, Accuracy: 0.8984375\n",
            "Step: 664400, Loss: 403.2989501953125, Accuracy: 0.87890625\n",
            "Step: 664500, Loss: 399.2926025390625, Accuracy: 0.8984375\n",
            "Step: 664600, Loss: 397.04425048828125, Accuracy: 0.90234375\n",
            "Step: 664700, Loss: 394.691650390625, Accuracy: 0.91015625\n",
            "Step: 664800, Loss: 405.04974365234375, Accuracy: 0.86328125\n",
            "Step: 664900, Loss: 402.9716796875, Accuracy: 0.87890625\n",
            "Step: 665000, Loss: 393.5951843261719, Accuracy: 0.90234375\n",
            "Step: 665100, Loss: 399.9722900390625, Accuracy: 0.89453125\n",
            "Step: 665200, Loss: 401.5868225097656, Accuracy: 0.8828125\n",
            "Step: 665300, Loss: 391.2230224609375, Accuracy: 0.921875\n",
            "Step: 665400, Loss: 397.7388610839844, Accuracy: 0.8984375\n",
            "Step: 665500, Loss: 397.1275634765625, Accuracy: 0.89453125\n",
            "Step: 665600, Loss: 406.81829833984375, Accuracy: 0.85546875\n",
            "Step: 665700, Loss: 395.74371337890625, Accuracy: 0.90625\n",
            "Step: 665800, Loss: 398.6014709472656, Accuracy: 0.8828125\n",
            "Step: 665900, Loss: 396.44384765625, Accuracy: 0.8984375\n",
            "Step: 666000, Loss: 397.4787292480469, Accuracy: 0.90234375\n",
            "Step: 666100, Loss: 399.1603698730469, Accuracy: 0.90625\n",
            "Step: 666200, Loss: 397.83441162109375, Accuracy: 0.88671875\n",
            "Step: 666300, Loss: 398.3238525390625, Accuracy: 0.90234375\n",
            "Step: 666400, Loss: 394.0054626464844, Accuracy: 0.90625\n",
            "Step: 666500, Loss: 399.0628967285156, Accuracy: 0.89453125\n",
            "Step: 666600, Loss: 402.16656494140625, Accuracy: 0.87890625\n",
            "Step: 666700, Loss: 401.2103271484375, Accuracy: 0.890625\n",
            "Step: 666800, Loss: 396.425537109375, Accuracy: 0.8984375\n",
            "Step: 666900, Loss: 402.912109375, Accuracy: 0.87109375\n",
            "Step: 667000, Loss: 396.8553466796875, Accuracy: 0.90234375\n",
            "Step: 667100, Loss: 403.5561218261719, Accuracy: 0.8828125\n",
            "Step: 667200, Loss: 394.87396240234375, Accuracy: 0.90625\n",
            "Step: 667300, Loss: 402.7818603515625, Accuracy: 0.875\n",
            "Step: 667400, Loss: 404.636962890625, Accuracy: 0.86328125\n",
            "Step: 667500, Loss: 404.3425598144531, Accuracy: 0.875\n",
            "Step: 667600, Loss: 394.40234375, Accuracy: 0.9140625\n",
            "Step: 667700, Loss: 395.3009033203125, Accuracy: 0.91015625\n",
            "Step: 667800, Loss: 403.15185546875, Accuracy: 0.8828125\n",
            "Step: 667900, Loss: 396.6109313964844, Accuracy: 0.90234375\n",
            "Step: 668000, Loss: 397.0093994140625, Accuracy: 0.91015625\n",
            "Step: 668100, Loss: 395.9097900390625, Accuracy: 0.9140625\n",
            "Step: 668200, Loss: 395.6072998046875, Accuracy: 0.90625\n",
            "Step: 668300, Loss: 399.36260986328125, Accuracy: 0.88671875\n",
            "Step: 668400, Loss: 400.02667236328125, Accuracy: 0.875\n",
            "Step: 668500, Loss: 398.91455078125, Accuracy: 0.890625\n",
            "Step: 668600, Loss: 393.01202392578125, Accuracy: 0.921875\n",
            "Step: 668700, Loss: 393.8502197265625, Accuracy: 0.93359375\n",
            "Step: 668800, Loss: 401.00927734375, Accuracy: 0.8828125\n",
            "Step: 668900, Loss: 406.75537109375, Accuracy: 0.8515625\n",
            "Step: 669000, Loss: 394.4765625, Accuracy: 0.921875\n",
            "Step: 669100, Loss: 397.8248596191406, Accuracy: 0.89453125\n",
            "Step: 669200, Loss: 398.6229553222656, Accuracy: 0.890625\n",
            "Step: 669300, Loss: 395.65191650390625, Accuracy: 0.91015625\n",
            "Step: 669400, Loss: 398.77581787109375, Accuracy: 0.90234375\n",
            "Step: 669500, Loss: 399.6790466308594, Accuracy: 0.88671875\n",
            "Step: 669600, Loss: 400.77655029296875, Accuracy: 0.88671875\n",
            "Step: 669700, Loss: 399.29754638671875, Accuracy: 0.88671875\n",
            "Step: 669800, Loss: 401.4124450683594, Accuracy: 0.890625\n",
            "Step: 669900, Loss: 396.85211181640625, Accuracy: 0.8984375\n",
            "Step: 670000, Loss: 398.2795104980469, Accuracy: 0.88671875\n",
            "Step: 670100, Loss: 406.0481872558594, Accuracy: 0.86328125\n",
            "Step: 670200, Loss: 393.2586669921875, Accuracy: 0.91796875\n",
            "Step: 670300, Loss: 396.4925537109375, Accuracy: 0.92578125\n",
            "Step: 670400, Loss: 396.517578125, Accuracy: 0.9140625\n",
            "Step: 670500, Loss: 400.65594482421875, Accuracy: 0.8828125\n",
            "Step: 670600, Loss: 394.92291259765625, Accuracy: 0.91796875\n",
            "Step: 670700, Loss: 399.474853515625, Accuracy: 0.890625\n",
            "Step: 670800, Loss: 402.2828063964844, Accuracy: 0.875\n",
            "Step: 670900, Loss: 394.6002197265625, Accuracy: 0.90625\n",
            "Step: 671000, Loss: 402.57208251953125, Accuracy: 0.875\n",
            "Step: 671100, Loss: 398.732421875, Accuracy: 0.890625\n",
            "Step: 671200, Loss: 396.58892822265625, Accuracy: 0.90625\n",
            "Step: 671300, Loss: 402.1748962402344, Accuracy: 0.87890625\n",
            "Step: 671400, Loss: 392.8162536621094, Accuracy: 0.9140625\n",
            "Step: 671500, Loss: 397.67047119140625, Accuracy: 0.8828125\n",
            "Step: 671600, Loss: 393.5164794921875, Accuracy: 0.91015625\n",
            "Step: 671700, Loss: 403.4427185058594, Accuracy: 0.87109375\n",
            "Step: 671800, Loss: 398.28570556640625, Accuracy: 0.89453125\n",
            "Step: 671900, Loss: 406.6900634765625, Accuracy: 0.859375\n",
            "Step: 672000, Loss: 398.0538024902344, Accuracy: 0.8984375\n",
            "Step: 672100, Loss: 400.200927734375, Accuracy: 0.8671875\n",
            "Step: 672200, Loss: 398.4881591796875, Accuracy: 0.890625\n",
            "Step: 672300, Loss: 395.0743713378906, Accuracy: 0.90234375\n",
            "Step: 672400, Loss: 396.5082092285156, Accuracy: 0.90625\n",
            "Step: 672500, Loss: 394.4666442871094, Accuracy: 0.91015625\n",
            "Step: 672600, Loss: 397.11846923828125, Accuracy: 0.890625\n",
            "Step: 672700, Loss: 403.2554931640625, Accuracy: 0.87890625\n",
            "Step: 672800, Loss: 408.3119812011719, Accuracy: 0.84765625\n",
            "Step: 672900, Loss: 391.56793212890625, Accuracy: 0.91796875\n",
            "Step: 673000, Loss: 400.732421875, Accuracy: 0.875\n",
            "Step: 673100, Loss: 401.98065185546875, Accuracy: 0.88671875\n",
            "Step: 673200, Loss: 399.1336669921875, Accuracy: 0.890625\n",
            "Step: 673300, Loss: 399.8710632324219, Accuracy: 0.890625\n",
            "Step: 673400, Loss: 408.6475830078125, Accuracy: 0.85546875\n",
            "Step: 673500, Loss: 397.5314025878906, Accuracy: 0.8984375\n",
            "Step: 673600, Loss: 400.2805480957031, Accuracy: 0.8828125\n",
            "Step: 673700, Loss: 395.63116455078125, Accuracy: 0.9140625\n",
            "Step: 673800, Loss: 398.92919921875, Accuracy: 0.8984375\n",
            "Step: 673900, Loss: 398.133056640625, Accuracy: 0.89453125\n",
            "Step: 674000, Loss: 404.82977294921875, Accuracy: 0.859375\n",
            "Step: 674100, Loss: 396.70697021484375, Accuracy: 0.921875\n",
            "Step: 674200, Loss: 400.4488830566406, Accuracy: 0.88671875\n",
            "Step: 674300, Loss: 396.6859130859375, Accuracy: 0.89453125\n",
            "Step: 674400, Loss: 408.28656005859375, Accuracy: 0.859375\n",
            "Step: 674500, Loss: 400.04534912109375, Accuracy: 0.8828125\n",
            "Step: 674600, Loss: 400.2149658203125, Accuracy: 0.88671875\n",
            "Step: 674700, Loss: 398.9227600097656, Accuracy: 0.90625\n",
            "Step: 674800, Loss: 395.9731750488281, Accuracy: 0.8984375\n",
            "Step: 674900, Loss: 394.23699951171875, Accuracy: 0.921875\n",
            "Step: 675000, Loss: 398.4006652832031, Accuracy: 0.88671875\n",
            "Step: 675100, Loss: 403.25433349609375, Accuracy: 0.87890625\n",
            "Step: 675200, Loss: 402.8924255371094, Accuracy: 0.875\n",
            "Step: 675300, Loss: 398.8817138671875, Accuracy: 0.89453125\n",
            "Step: 675400, Loss: 402.40203857421875, Accuracy: 0.87890625\n",
            "Step: 675500, Loss: 400.1391906738281, Accuracy: 0.87890625\n",
            "Step: 675600, Loss: 396.87493896484375, Accuracy: 0.89453125\n",
            "Step: 675700, Loss: 390.6739807128906, Accuracy: 0.921875\n",
            "Step: 675800, Loss: 404.2351989746094, Accuracy: 0.85546875\n",
            "Step: 675900, Loss: 392.8155517578125, Accuracy: 0.91796875\n",
            "Step: 676000, Loss: 399.93817138671875, Accuracy: 0.890625\n",
            "Step: 676100, Loss: 406.5827331542969, Accuracy: 0.85546875\n",
            "Step: 676200, Loss: 408.2912292480469, Accuracy: 0.85546875\n",
            "Step: 676300, Loss: 394.3887939453125, Accuracy: 0.91796875\n",
            "Step: 676400, Loss: 398.1543884277344, Accuracy: 0.91015625\n",
            "Step: 676500, Loss: 408.014892578125, Accuracy: 0.86328125\n",
            "Step: 676600, Loss: 403.7663269042969, Accuracy: 0.88671875\n",
            "Step: 676700, Loss: 393.130859375, Accuracy: 0.921875\n",
            "Step: 676800, Loss: 398.48565673828125, Accuracy: 0.87890625\n",
            "Step: 676900, Loss: 398.16796875, Accuracy: 0.89453125\n",
            "Step: 677000, Loss: 388.8829650878906, Accuracy: 0.94140625\n",
            "Step: 677100, Loss: 402.5002136230469, Accuracy: 0.88671875\n",
            "Step: 677200, Loss: 397.07025146484375, Accuracy: 0.8984375\n",
            "Step: 677300, Loss: 409.0814514160156, Accuracy: 0.84765625\n",
            "Step: 677400, Loss: 397.6962890625, Accuracy: 0.8984375\n",
            "Step: 677500, Loss: 401.7100830078125, Accuracy: 0.87890625\n",
            "Step: 677600, Loss: 400.9503173828125, Accuracy: 0.890625\n",
            "Step: 677700, Loss: 397.09185791015625, Accuracy: 0.90625\n",
            "Step: 677800, Loss: 397.2451171875, Accuracy: 0.90625\n",
            "Step: 677900, Loss: 399.4937438964844, Accuracy: 0.8828125\n",
            "Step: 678000, Loss: 403.04766845703125, Accuracy: 0.859375\n",
            "Step: 678100, Loss: 398.65008544921875, Accuracy: 0.890625\n",
            "Step: 678200, Loss: 392.474609375, Accuracy: 0.9140625\n",
            "Step: 678300, Loss: 399.78350830078125, Accuracy: 0.8828125\n",
            "Step: 678400, Loss: 397.103515625, Accuracy: 0.91015625\n",
            "Step: 678500, Loss: 397.9698181152344, Accuracy: 0.8984375\n",
            "Step: 678600, Loss: 395.11041259765625, Accuracy: 0.9140625\n",
            "Step: 678700, Loss: 408.43499755859375, Accuracy: 0.86328125\n",
            "Step: 678800, Loss: 405.603759765625, Accuracy: 0.86328125\n",
            "Step: 678900, Loss: 393.979248046875, Accuracy: 0.91015625\n",
            "Step: 679000, Loss: 403.29144287109375, Accuracy: 0.87109375\n",
            "Step: 679100, Loss: 394.46484375, Accuracy: 0.9140625\n",
            "Step: 679200, Loss: 391.99725341796875, Accuracy: 0.921875\n",
            "Step: 679300, Loss: 393.94873046875, Accuracy: 0.90625\n",
            "Step: 679400, Loss: 402.7439880371094, Accuracy: 0.8828125\n",
            "Step: 679500, Loss: 399.94732666015625, Accuracy: 0.88671875\n",
            "Step: 679600, Loss: 402.171630859375, Accuracy: 0.87109375\n",
            "Step: 679700, Loss: 391.3769836425781, Accuracy: 0.91796875\n",
            "Step: 679800, Loss: 405.27728271484375, Accuracy: 0.859375\n",
            "Step: 679900, Loss: 390.2339172363281, Accuracy: 0.92578125\n",
            "Step: 680000, Loss: 401.31170654296875, Accuracy: 0.87890625\n",
            "Step: 680100, Loss: 404.25994873046875, Accuracy: 0.87890625\n",
            "Step: 680200, Loss: 405.59619140625, Accuracy: 0.87109375\n",
            "Step: 680300, Loss: 397.12542724609375, Accuracy: 0.8984375\n",
            "Step: 680400, Loss: 400.9439697265625, Accuracy: 0.88671875\n",
            "Step: 680500, Loss: 400.030517578125, Accuracy: 0.87890625\n",
            "Step: 680600, Loss: 399.95306396484375, Accuracy: 0.88671875\n",
            "Step: 680700, Loss: 394.56475830078125, Accuracy: 0.91015625\n",
            "Step: 680800, Loss: 398.5576171875, Accuracy: 0.890625\n",
            "Step: 680900, Loss: 400.43310546875, Accuracy: 0.88671875\n",
            "Step: 681000, Loss: 398.80511474609375, Accuracy: 0.890625\n",
            "Step: 681100, Loss: 398.8918762207031, Accuracy: 0.88671875\n",
            "Step: 681200, Loss: 401.6891784667969, Accuracy: 0.890625\n",
            "Step: 681300, Loss: 393.6977844238281, Accuracy: 0.91015625\n",
            "Step: 681400, Loss: 400.3165283203125, Accuracy: 0.890625\n",
            "Step: 681500, Loss: 398.5294189453125, Accuracy: 0.90234375\n",
            "Step: 681600, Loss: 390.9820556640625, Accuracy: 0.9375\n",
            "Step: 681700, Loss: 399.0421142578125, Accuracy: 0.890625\n",
            "Step: 681800, Loss: 391.63409423828125, Accuracy: 0.9140625\n",
            "Step: 681900, Loss: 401.215576171875, Accuracy: 0.87890625\n",
            "Step: 682000, Loss: 390.7877197265625, Accuracy: 0.921875\n",
            "Step: 682100, Loss: 393.96722412109375, Accuracy: 0.9140625\n",
            "Step: 682200, Loss: 399.9695129394531, Accuracy: 0.875\n",
            "Step: 682300, Loss: 395.1734924316406, Accuracy: 0.90625\n",
            "Step: 682400, Loss: 401.04345703125, Accuracy: 0.87890625\n",
            "Step: 682500, Loss: 402.157958984375, Accuracy: 0.875\n",
            "Step: 682600, Loss: 401.8017883300781, Accuracy: 0.890625\n",
            "Step: 682700, Loss: 397.7010803222656, Accuracy: 0.88671875\n",
            "Step: 682800, Loss: 405.5269775390625, Accuracy: 0.859375\n",
            "Step: 682900, Loss: 410.98834228515625, Accuracy: 0.83984375\n",
            "Step: 683000, Loss: 403.153076171875, Accuracy: 0.88671875\n",
            "Step: 683100, Loss: 401.7992858886719, Accuracy: 0.875\n",
            "Step: 683200, Loss: 396.3392639160156, Accuracy: 0.8984375\n",
            "Step: 683300, Loss: 399.5482482910156, Accuracy: 0.8828125\n",
            "Step: 683400, Loss: 403.7099304199219, Accuracy: 0.87109375\n",
            "Step: 683500, Loss: 400.73162841796875, Accuracy: 0.890625\n",
            "Step: 683600, Loss: 397.9847106933594, Accuracy: 0.89453125\n",
            "Step: 683700, Loss: 397.2916259765625, Accuracy: 0.91015625\n",
            "Step: 683800, Loss: 395.70880126953125, Accuracy: 0.90234375\n",
            "Step: 683900, Loss: 399.85467529296875, Accuracy: 0.890625\n",
            "Step: 684000, Loss: 397.7986755371094, Accuracy: 0.91015625\n",
            "Step: 684100, Loss: 402.58746337890625, Accuracy: 0.87890625\n",
            "Step: 684200, Loss: 397.7818603515625, Accuracy: 0.8984375\n",
            "Step: 684300, Loss: 403.14678955078125, Accuracy: 0.875\n",
            "Step: 684400, Loss: 400.800048828125, Accuracy: 0.890625\n",
            "Step: 684500, Loss: 402.2698974609375, Accuracy: 0.890625\n",
            "Step: 684600, Loss: 404.93707275390625, Accuracy: 0.87109375\n",
            "Step: 684700, Loss: 402.55755615234375, Accuracy: 0.875\n",
            "Step: 684800, Loss: 390.8218994140625, Accuracy: 0.92578125\n",
            "Step: 684900, Loss: 403.57855224609375, Accuracy: 0.875\n",
            "Step: 685000, Loss: 404.1940612792969, Accuracy: 0.86328125\n",
            "Step: 685100, Loss: 396.4824523925781, Accuracy: 0.8984375\n",
            "Step: 685200, Loss: 399.3333740234375, Accuracy: 0.8828125\n",
            "Step: 685300, Loss: 399.07916259765625, Accuracy: 0.89453125\n",
            "Step: 685400, Loss: 400.03692626953125, Accuracy: 0.8828125\n",
            "Step: 685500, Loss: 400.10052490234375, Accuracy: 0.88671875\n",
            "Step: 685600, Loss: 399.4211730957031, Accuracy: 0.89453125\n",
            "Step: 685700, Loss: 392.8275146484375, Accuracy: 0.9140625\n",
            "Step: 685800, Loss: 400.8117370605469, Accuracy: 0.90234375\n",
            "Step: 685900, Loss: 386.8758544921875, Accuracy: 0.94140625\n",
            "Step: 686000, Loss: 393.1365966796875, Accuracy: 0.92578125\n",
            "Step: 686100, Loss: 403.425048828125, Accuracy: 0.87109375\n",
            "Step: 686200, Loss: 398.05718994140625, Accuracy: 0.8984375\n",
            "Step: 686300, Loss: 401.56927490234375, Accuracy: 0.875\n",
            "Step: 686400, Loss: 402.6402587890625, Accuracy: 0.875\n",
            "Step: 686500, Loss: 403.59869384765625, Accuracy: 0.875\n",
            "Step: 686600, Loss: 400.01861572265625, Accuracy: 0.87890625\n",
            "Step: 686700, Loss: 398.21221923828125, Accuracy: 0.90625\n",
            "Step: 686800, Loss: 392.842529296875, Accuracy: 0.91796875\n",
            "Step: 686900, Loss: 407.84478759765625, Accuracy: 0.84765625\n",
            "Step: 687000, Loss: 398.77197265625, Accuracy: 0.90234375\n",
            "Step: 687100, Loss: 395.67327880859375, Accuracy: 0.8984375\n",
            "Step: 687200, Loss: 396.0702209472656, Accuracy: 0.90234375\n",
            "Step: 687300, Loss: 404.8731994628906, Accuracy: 0.8671875\n",
            "Step: 687400, Loss: 399.921875, Accuracy: 0.8828125\n",
            "Step: 687500, Loss: 400.0246887207031, Accuracy: 0.890625\n",
            "Step: 687600, Loss: 403.088134765625, Accuracy: 0.88671875\n",
            "Step: 687700, Loss: 401.5279541015625, Accuracy: 0.8828125\n",
            "Step: 687800, Loss: 397.11260986328125, Accuracy: 0.8984375\n",
            "Step: 687900, Loss: 401.42138671875, Accuracy: 0.890625\n",
            "Step: 688000, Loss: 399.03759765625, Accuracy: 0.89453125\n",
            "Step: 688100, Loss: 396.5538635253906, Accuracy: 0.91015625\n",
            "Step: 688200, Loss: 392.84051513671875, Accuracy: 0.91796875\n",
            "Step: 688300, Loss: 399.13812255859375, Accuracy: 0.88671875\n",
            "Step: 688400, Loss: 394.8458251953125, Accuracy: 0.91796875\n",
            "Step: 688500, Loss: 399.7169189453125, Accuracy: 0.875\n",
            "Step: 688600, Loss: 400.88201904296875, Accuracy: 0.8828125\n",
            "Step: 688700, Loss: 391.8052673339844, Accuracy: 0.91796875\n",
            "Step: 688800, Loss: 402.2947998046875, Accuracy: 0.87890625\n",
            "Step: 688900, Loss: 392.7728271484375, Accuracy: 0.91015625\n",
            "Step: 689000, Loss: 395.00189208984375, Accuracy: 0.90625\n",
            "Step: 689100, Loss: 397.2666015625, Accuracy: 0.90625\n",
            "Step: 689200, Loss: 395.4011535644531, Accuracy: 0.921875\n",
            "Step: 689300, Loss: 396.3861083984375, Accuracy: 0.90234375\n",
            "Step: 689400, Loss: 401.93585205078125, Accuracy: 0.87890625\n",
            "Step: 689500, Loss: 399.33599853515625, Accuracy: 0.88671875\n",
            "Step: 689600, Loss: 387.25933837890625, Accuracy: 0.9453125\n",
            "Step: 689700, Loss: 397.878662109375, Accuracy: 0.90234375\n",
            "Step: 689800, Loss: 397.25701904296875, Accuracy: 0.89453125\n",
            "Step: 689900, Loss: 395.68316650390625, Accuracy: 0.90625\n",
            "Step: 690000, Loss: 398.1734313964844, Accuracy: 0.90625\n",
            "Step: 690100, Loss: 402.53656005859375, Accuracy: 0.87109375\n",
            "Step: 690200, Loss: 403.94989013671875, Accuracy: 0.8671875\n",
            "Step: 690300, Loss: 394.79986572265625, Accuracy: 0.91015625\n",
            "Step: 690400, Loss: 391.4002990722656, Accuracy: 0.93359375\n",
            "Step: 690500, Loss: 400.56744384765625, Accuracy: 0.87890625\n",
            "Step: 690600, Loss: 406.8542785644531, Accuracy: 0.8515625\n",
            "Step: 690700, Loss: 404.61151123046875, Accuracy: 0.859375\n",
            "Step: 690800, Loss: 400.1794128417969, Accuracy: 0.87109375\n",
            "Step: 690900, Loss: 395.9019470214844, Accuracy: 0.921875\n",
            "Step: 691000, Loss: 400.00091552734375, Accuracy: 0.8828125\n",
            "Step: 691100, Loss: 398.59332275390625, Accuracy: 0.88671875\n",
            "Step: 691200, Loss: 394.1015930175781, Accuracy: 0.91796875\n",
            "Step: 691300, Loss: 394.5457763671875, Accuracy: 0.9140625\n",
            "Step: 691400, Loss: 402.23388671875, Accuracy: 0.875\n",
            "Step: 691500, Loss: 396.0110778808594, Accuracy: 0.91015625\n",
            "Step: 691600, Loss: 403.3834228515625, Accuracy: 0.875\n",
            "Step: 691700, Loss: 398.30950927734375, Accuracy: 0.8828125\n",
            "Step: 691800, Loss: 400.7451171875, Accuracy: 0.87109375\n",
            "Step: 691900, Loss: 403.635009765625, Accuracy: 0.87109375\n",
            "Step: 692000, Loss: 395.388427734375, Accuracy: 0.890625\n",
            "Step: 692100, Loss: 399.20281982421875, Accuracy: 0.89453125\n",
            "Step: 692200, Loss: 406.3294372558594, Accuracy: 0.875\n",
            "Step: 692300, Loss: 398.54022216796875, Accuracy: 0.88671875\n",
            "Step: 692400, Loss: 392.2076110839844, Accuracy: 0.921875\n",
            "Step: 692500, Loss: 404.2156677246094, Accuracy: 0.87109375\n",
            "Step: 692600, Loss: 396.5316162109375, Accuracy: 0.90625\n",
            "Step: 692700, Loss: 398.8822021484375, Accuracy: 0.8984375\n",
            "Step: 692800, Loss: 398.8961181640625, Accuracy: 0.890625\n",
            "Step: 692900, Loss: 397.4662170410156, Accuracy: 0.88671875\n",
            "Step: 693000, Loss: 395.01507568359375, Accuracy: 0.91796875\n",
            "Step: 693100, Loss: 394.9979248046875, Accuracy: 0.91796875\n",
            "Step: 693200, Loss: 398.49932861328125, Accuracy: 0.8984375\n",
            "Step: 693300, Loss: 399.5806884765625, Accuracy: 0.89453125\n",
            "Step: 693400, Loss: 395.5665283203125, Accuracy: 0.91015625\n",
            "Step: 693500, Loss: 399.93853759765625, Accuracy: 0.890625\n",
            "Step: 693600, Loss: 395.9913330078125, Accuracy: 0.91015625\n",
            "Step: 693700, Loss: 403.77392578125, Accuracy: 0.890625\n",
            "Step: 693800, Loss: 407.6534423828125, Accuracy: 0.85546875\n",
            "Step: 693900, Loss: 400.3907470703125, Accuracy: 0.890625\n",
            "Step: 694000, Loss: 400.832275390625, Accuracy: 0.875\n",
            "Step: 694100, Loss: 397.17572021484375, Accuracy: 0.91015625\n",
            "Step: 694200, Loss: 393.178955078125, Accuracy: 0.93359375\n",
            "Step: 694300, Loss: 395.863037109375, Accuracy: 0.8984375\n",
            "Step: 694400, Loss: 403.371337890625, Accuracy: 0.87109375\n",
            "Step: 694500, Loss: 394.868408203125, Accuracy: 0.8984375\n",
            "Step: 694600, Loss: 406.366943359375, Accuracy: 0.859375\n",
            "Step: 694700, Loss: 393.1787414550781, Accuracy: 0.90625\n",
            "Step: 694800, Loss: 398.291015625, Accuracy: 0.88671875\n",
            "Step: 694900, Loss: 396.036865234375, Accuracy: 0.89453125\n",
            "Step: 695000, Loss: 394.92401123046875, Accuracy: 0.90625\n",
            "Step: 695100, Loss: 402.63812255859375, Accuracy: 0.87890625\n",
            "Step: 695200, Loss: 403.2796630859375, Accuracy: 0.87109375\n",
            "Step: 695300, Loss: 406.34130859375, Accuracy: 0.85546875\n",
            "Step: 695400, Loss: 400.3785400390625, Accuracy: 0.89453125\n",
            "Step: 695500, Loss: 399.91143798828125, Accuracy: 0.88671875\n",
            "Step: 695600, Loss: 394.23297119140625, Accuracy: 0.9140625\n",
            "Step: 695700, Loss: 403.50201416015625, Accuracy: 0.87109375\n",
            "Step: 695800, Loss: 400.9828186035156, Accuracy: 0.890625\n",
            "Step: 695900, Loss: 396.6575012207031, Accuracy: 0.89453125\n",
            "Step: 696000, Loss: 398.12310791015625, Accuracy: 0.90234375\n",
            "Step: 696100, Loss: 396.99920654296875, Accuracy: 0.90625\n",
            "Step: 696200, Loss: 403.8758544921875, Accuracy: 0.86328125\n",
            "Step: 696300, Loss: 401.71246337890625, Accuracy: 0.88671875\n",
            "Step: 696400, Loss: 403.0865173339844, Accuracy: 0.87109375\n",
            "Step: 696500, Loss: 399.31365966796875, Accuracy: 0.890625\n",
            "Step: 696600, Loss: 396.7179870605469, Accuracy: 0.89453125\n",
            "Step: 696700, Loss: 398.8502197265625, Accuracy: 0.89453125\n",
            "Step: 696800, Loss: 394.77276611328125, Accuracy: 0.91796875\n",
            "Step: 696900, Loss: 397.3529052734375, Accuracy: 0.89453125\n",
            "Step: 697000, Loss: 407.9915771484375, Accuracy: 0.8515625\n",
            "Step: 697100, Loss: 398.0946960449219, Accuracy: 0.90234375\n",
            "Step: 697200, Loss: 401.2505187988281, Accuracy: 0.890625\n",
            "Step: 697300, Loss: 395.64495849609375, Accuracy: 0.9140625\n",
            "Step: 697400, Loss: 396.7769775390625, Accuracy: 0.8984375\n",
            "Step: 697500, Loss: 395.46923828125, Accuracy: 0.91015625\n",
            "Step: 697600, Loss: 402.0677490234375, Accuracy: 0.8828125\n",
            "Step: 697700, Loss: 398.63153076171875, Accuracy: 0.8828125\n",
            "Step: 697800, Loss: 397.5606689453125, Accuracy: 0.890625\n",
            "Step: 697900, Loss: 400.05096435546875, Accuracy: 0.88671875\n",
            "Step: 698000, Loss: 394.2857666015625, Accuracy: 0.9140625\n",
            "Step: 698100, Loss: 396.52105712890625, Accuracy: 0.890625\n",
            "Step: 698200, Loss: 396.01971435546875, Accuracy: 0.90625\n",
            "Step: 698300, Loss: 393.206787109375, Accuracy: 0.92578125\n",
            "Step: 698400, Loss: 393.901611328125, Accuracy: 0.91796875\n",
            "Step: 698500, Loss: 404.0710144042969, Accuracy: 0.87890625\n",
            "Step: 698600, Loss: 400.04302978515625, Accuracy: 0.875\n",
            "Step: 698700, Loss: 411.4593811035156, Accuracy: 0.84765625\n",
            "Step: 698800, Loss: 395.4840087890625, Accuracy: 0.89453125\n",
            "Step: 698900, Loss: 397.89337158203125, Accuracy: 0.88671875\n",
            "Step: 699000, Loss: 397.31451416015625, Accuracy: 0.9140625\n",
            "Step: 699100, Loss: 391.2040710449219, Accuracy: 0.9296875\n",
            "Step: 699200, Loss: 405.3992919921875, Accuracy: 0.85546875\n",
            "Step: 699300, Loss: 401.43743896484375, Accuracy: 0.89453125\n",
            "Step: 699400, Loss: 400.64752197265625, Accuracy: 0.8984375\n",
            "Step: 699500, Loss: 388.6024475097656, Accuracy: 0.9453125\n",
            "Step: 699600, Loss: 399.90496826171875, Accuracy: 0.90234375\n",
            "Step: 699700, Loss: 402.17529296875, Accuracy: 0.87890625\n",
            "Step: 699800, Loss: 404.29266357421875, Accuracy: 0.87109375\n",
            "Step: 699900, Loss: 395.29730224609375, Accuracy: 0.90625\n",
            "Step: 700000, Loss: 396.48748779296875, Accuracy: 0.8984375\n",
            "Step: 700100, Loss: 396.4358215332031, Accuracy: 0.90234375\n",
            "Step: 700200, Loss: 405.3600158691406, Accuracy: 0.86328125\n",
            "Step: 700300, Loss: 391.2567138671875, Accuracy: 0.91796875\n",
            "Step: 700400, Loss: 400.65521240234375, Accuracy: 0.88671875\n",
            "Step: 700500, Loss: 398.2021179199219, Accuracy: 0.88671875\n",
            "Step: 700600, Loss: 401.1676025390625, Accuracy: 0.89453125\n",
            "Step: 700700, Loss: 396.88690185546875, Accuracy: 0.890625\n",
            "Step: 700800, Loss: 397.4608459472656, Accuracy: 0.89453125\n",
            "Step: 700900, Loss: 400.0955505371094, Accuracy: 0.8984375\n",
            "Step: 701000, Loss: 403.283447265625, Accuracy: 0.875\n",
            "Step: 701100, Loss: 403.34747314453125, Accuracy: 0.87109375\n",
            "Step: 701200, Loss: 391.37591552734375, Accuracy: 0.921875\n",
            "Step: 701300, Loss: 394.55029296875, Accuracy: 0.9140625\n",
            "Step: 701400, Loss: 397.478515625, Accuracy: 0.8984375\n",
            "Step: 701500, Loss: 397.07861328125, Accuracy: 0.890625\n",
            "Step: 701600, Loss: 404.1488037109375, Accuracy: 0.87890625\n",
            "Step: 701700, Loss: 398.85662841796875, Accuracy: 0.8984375\n",
            "Step: 701800, Loss: 398.43096923828125, Accuracy: 0.890625\n",
            "Step: 701900, Loss: 400.2320556640625, Accuracy: 0.87890625\n",
            "Step: 702000, Loss: 401.587158203125, Accuracy: 0.87890625\n"
          ]
        }
      ],
      "source": [
        "# Тренування мережі\n",
        "\n",
        "loss_history = []  # кожні display_step кроків зберігай в цьому списку поточну помилку нейромережі\n",
        "accuracy_history = [] # кожні display_step кроків зберігай в цьому списку поточну точність нейромережі\n",
        "\n",
        "# У цьому циклі ми будемо проводити навчання нейронної мережі\n",
        "# із тренувального датасета train_data вилучи випадкову підмножину, на якій\n",
        "# відбудеться тренування. Використовуй метод take, доступний для тренувального датасета.\n",
        "steps_per_epoch = len(x_train) // batch_size\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(steps_per_epoch * training_steps), 1): # Место для вашего кода:\n",
        "    # Обновляем веса нейронной сети\n",
        "    # Место для вашего кода\n",
        "    loss = train(neural_net, batch_x, batch_y)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    if step % display_step == 0:\n",
        "        pred = neural_net(batch_x)\n",
        "        # Место для вашего кода\n",
        "        acc = accuracy(pred, batch_y)\n",
        "        accuracy_history.append(acc)\n",
        "        print(f\"Step: {step}, Loss: {loss}, Accuracy: {acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_yCBfG6MbQB2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "b9b4b8bb-b360-4f57-87c5-b989d822dba5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc2be588c40>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Виведіть графіки залежності зміни точності і втрат від кроку\n",
        "# Якщо все зроблено правильно, то точність повинна зростати, а втрати зменшуватись\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot accuracy\n",
        "axes[0].plot(accuracy_history, label='Accuracy', color='b', marker='o')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_title('Accuracy')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot loss\n",
        "axes[1].plot(loss_history, label='Loss', color='r', marker='x')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_title('Loss')\n",
        "axes[1].legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LE3g4gDyUSNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc30d98c-fdb9-43c6-dda1-9b8f52b91393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8729000091552734\n"
          ]
        }
      ],
      "source": [
        "# Обчисліть точність навченої нейромережі\n",
        "test_accuracy = accuracy(neural_net(x_test), y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_EEHAubOUSNY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c8cb327-ecbb-4247-d85b-6ce81ccc6e25"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaSUlEQVR4nO3ce3jO9/3H8dctiCTUMRFUhcShVGdlWIcwNLQOUZplpYKgm3OvHq1TXEp12k5bdnW0tA7bqknXWa+UtZOuOqeNTtGqU7R1KCG0KXVI8vn94fL+9XYH+d5yUJ6P63Jd8vV93/cnd273M98733x9zjknAAAklSvrBQAArh5EAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBE4ToxZcoU+Xw+HTlypNhuc8iQIYqJiSm227sWvP/++/L5fHr//fdt29X2OBW2xpK2d+9e+Xw+PfPMM8V2m2XxeVwPrsso+Hy+Iv0p6ydb586ddcstt5TpGkrK0aNHNWvWLHXq1EmRkZGqVq2a2rdvr9dff/2Kbrdz585+X8MaNWroJz/5iRYsWKCCgoJiWn3pmDFjht56660yu/9XX31VPp9P//3vf8tsDSXpzTff1C9+8Qs1atRI4eHhatq0qR588EEdP368rJdWpsqX9QLKwuLFi/0+XrRokd59992A7TfffHNpLuu6snbtWj3++OO688479dvf/lbly5dXenq6kpOT9cknn2jq1KlB3/aNN96op556SpKUnZ2tRYsWKTU1VTt27NDMmTOL61Mosvnz5wcVpBkzZmjAgAFKTEws/kVBI0eOVN26dTVo0CDddNNN2rJli+bMmaOMjAxt2rRJYWFhZb3EMnFdRmHQoEF+H69bt07vvvtuwPYLnTx5UuHh4SW5tOtGixYttHPnTjVo0MC2jRo1St26ddPTTz+tRx55RBEREUHddtWqVf2+lvfff7+aNm2qOXPmaNq0aapQoULATEFBgc6cOaNKlSoFdZ+XUtj9oeylpaWpc+fOfttat26tlJQULV26VMOHDy+bhZWx6/Lto6I4/9bNxo0b1alTJ4WHh+s3v/mNpHNvP02ZMiVgJiYmRkOGDPHbdvz4cU2YMEH169dXaGio4uLi9PTTTxfbWxkff/yxhgwZokaNGqlSpUqKjo7WsGHDdPTo0UL3P3LkiJKSknTDDTeoZs2aGj9+vE6dOhWw35IlS9S6dWuFhYWpRo0aSk5O1pdffnnZ9Rw8eFDbt2/X2bNnL7lfw4YN/YIgnXtcExMTdfr0ae3Zs+ey91VU4eHhat++vU6cOKHs7Gy7rzFjxmjp0qVq0aKFQkNDtWLFCknS/v37NWzYMNWuXVuhoaFq0aKFFixYEHC7+/btU2JioiIiIhQVFaUHHnhAp0+fDtivsJ8pFBQU6Pnnn1fLli1VqVIlRUZGqkePHvZWjc/n04kTJ/Taa6/ZW2Hff24V9xqDdebMGT3xxBNq3bq1qlatqoiICHXs2FGZmZkXnfn973+vBg0aKCwsTPHx8dq6dWvAPtu3b9eAAQNUo0YNVapUSW3atNHy5csvu56TJ09q+/btRfrZ2YVBkKR+/fpJkj799NPLzl+rrssjhaI6evSoevbsqeTkZA0aNEi1a9f2NH/y5EnFx8dr//79uv/++3XTTTdpzZo1mjhxog4ePKjZs2df8Rrfffdd7dmzR0OHDlV0dLS2bdumefPmadu2bVq3bp18Pp/f/klJSYqJidFTTz2ldevW6YUXXtCxY8e0aNEi22f69OmaNGmSkpKSNHz4cGVnZ+vFF19Up06d9NFHH6latWoXXc/EiRP12muvKSsrK6gfrn711VeSpFq1anmevZQ9e/YoJCTEb+2rVq3SsmXLNGbMGNWqVUsxMTE6dOiQ2rdvb9GIjIzUO++8o9TUVH3zzTeaMGGCJOm7775T165d9cUXX2jcuHGqW7euFi9erFWrVhVpPampqXr11VfVs2dPDR8+XHl5eVq9erXWrVunNm3aaPHixRo+fLjatm2rkSNHSpJiY2MlqdTWWBTffPONXn75Zf3yl7/UiBEjlJubq1deeUUJCQnasGGDWrVq5bf/okWLlJubq9GjR+vUqVN6/vnn9fOf/1xbtmyx/1/btm3Tz372M9WrV0+PPfaYIiIitGzZMiUmJio9Pd1euAuzYcMGdenSRZMnTy70G7fLKann3w+Kgxs9erS78KGIj493ktxLL70UsL8kN3ny5IDtDRo0cCkpKfbxtGnTXEREhNuxY4fffo899pgLCQlxX3zxxSXXFR8f71q0aHHJfU6ePBmw7c9//rOT5D744APbNnnyZCfJ9enTx2/fUaNGOUlu8+bNzjnn9u7d60JCQtz06dP99tuyZYsrX7683/aUlBTXoEEDv/1SUlKcJJeVlXXJdRfm6NGjLioqynXs2NHz7Hnx8fGuWbNmLjs722VnZ7tPP/3UjRs3zklyvXv3tv0kuXLlyrlt27b5zaempro6deq4I0eO+G1PTk52VatWtcd79uzZTpJbtmyZ7XPixAkXFxfnJLnMzEzbfuHjtGrVKifJjRs3LmD9BQUF9veIiAi/51NJrrEwCxcudJLcf/7zn4vuk5eX506fPu237dixY6527dpu2LBhti0rK8tJcmFhYW7fvn22ff369U6Se+CBB2xb165dXcuWLd2pU6dsW0FBgbv99ttd48aNbVtmZmbA53F+W2H/P4siNTXVhYSEBPyfvZ7w9tElhIaGaujQoUHPv/HGG+rYsaOqV6+uI0eO2J9u3bopPz9fH3zwwRWv8fs/DDt16pSOHDmi9u3bS5I2bdoUsP/o0aP9Ph47dqwkKSMjQ9K5MzIKCgqUlJTkt+bo6Gg1btz4km8LSOfOWHHOeT5KKCgo0MCBA3X8+HG9+OKLnmYvtH37dkVGRioyMlI333yzXnzxRd11110Bb6/Ex8erefPm9rFzTunp6erdu7ecc36ff0JCgr7++mt7TDMyMlSnTh0NGDDA5sPDw+27+ktJT0+Xz+fT5MmTA/7twiO7C5XWGosqJCREFStWlHTua5iTk6O8vDy1adOm0OdfYmKi6tWrZx+3bdtW7dq1s+dfTk6OVq1apaSkJOXm5trndvToUSUkJGjnzp3av3//RdfTuXNnOeeCOkr405/+pFdeeUUPPvigGjdu7Hn+WsHbR5dQr149e8IHY+fOnfr4448VGRlZ6L8fPnw46Ns+LycnR1OnTtVf/vKXgNv7+uuvA/a/8MkeGxurcuXKae/evbZm59xF/1OU1A9Nx44dqxUrVmjRokX60Y9+dEW3FRMTo/nz58vn86lSpUpq3LixoqKiAvZr2LCh38fZ2dk6fvy45s2bp3nz5hV62+cf488//1xxcXEBL+JNmza97Pp2796tunXrqkaNGkX9lEp9jV689tprevbZZwN+lnTh4ysFPv8kqUmTJlq2bJkkadeuXXLOadKkSZo0aVKh93f48GG/sBSH1atXKzU1VQkJCZo+fXqx3vYPDVG4BK+npOXn5/t9XFBQoO7du+uRRx4pdP8mTZoEvbbzkpKStGbNGj388MNq1aqVKleurIKCAvXo0aNIP8y+8AWjoKBAPp9P77zzjkJCQgL2r1y58hWv+UJTp07VH/7wB82cOVP33XffFd9eRESEunXrdtn9Lvz6nn+8Bg0apJSUlEJnbr311ite35W42ta4ZMkSDRkyRImJiXr44YcVFRWlkJAQPfXUU9q9e7fn2zv/+T300ENKSEgodJ+4uLgrWvOFNm/erD59+uiWW25RWlqaype/vl8Wr+/PPkjVq1cP+AWXM2fO6ODBg37bYmNj9e233xbpBSoYx44d0z//+U9NnTpVTzzxhG3fuXPnRWd27tzp9x3crl27VFBQYG/3xMbGyjmnhg0bFku0Lmfu3LmaMmWKJkyYoEcffbTE7+9SIiMjVaVKFeXn51/2a9agQQNt3bpVzjm/sH722WeXvZ/Y2FitXLlSOTk5lzxaKOytpNJaY1GlpaWpUaNGevPNN/3uo7C3xqTCn5s7duyw51+jRo0knTsiLan/N9+3e/du9ejRQ1FRUcrIyCiRb3p+aPiZQhBiY2MDfh4wb968gCOFpKQkrV27VitXrgy4jePHjysvL++K1nH+O3nnnN/2S53VNHfuXL+Pz79/37NnT0nS3XffrZCQEE2dOjXgdp1zFz3V9byinpIqSa+//rrGjRungQMH6rnnnrvs/iUtJCRE/fv3V3p6eqGnSZ4/nVWS7rzzTh04cEBpaWm27eTJkxd9S+f7+vfvL+dcob+g9/3HPCIiIuCbj9JaY1EV9hxcv3691q5dW+j+b731lt/PBDZs2KD169fb8y8qKkqdO3fWH//4x4BvsiT/z68wXk5J/eqrr3THHXeoXLlyWrly5UXf5r3ecKQQhOHDh+tXv/qV+vfvr+7du2vz5s1auXJlwGlsDz/8sJYvX65evXppyJAhat26tU6cOKEtW7YoLS1Ne/fuveypb9nZ2XryyScDtjds2FADBw5Up06d9Lvf/U5nz55VvXr19I9//ENZWVkXvb2srCz16dNHPXr00Nq1a7VkyRLde++99j5+bGysnnzySU2cOFF79+5VYmKiqlSpoqysLP31r3/VyJEj9dBDD1309ot6SuqGDRs0ePBg1axZU127dtXSpUv9/v3222+37xqlc981x8fHl/ilR2bOnKnMzEy1a9dOI0aMUPPmzZWTk6NNmzbpvffeU05OjiRpxIgRmjNnjgYPHqyNGzeqTp06Wrx4cZF+ubFLly6677779MILL2jnzp32Vt/q1avVpUsXjRkzRtK5X6R677339Nxzz6lu3bpq2LCh2rVrVypr/L4FCxbY73B83/jx49WrVy+9+eab6tevn+666y5lZWXppZdeUvPmzfXtt98GzMTFxalDhw769a9/rdOnT2v27NmqWbOm31usc+fOVYcOHdSyZUuNGDFCjRo10qFDh7R27Vrt27dPmzdvvuhavZyS2qNHD+3Zs0ePPPKIPvzwQ3344Yf2b7Vr11b37t2L8Ohcg0r9fKer0MVOSb3Y6aD5+fnu0UcfdbVq1XLh4eEuISHB7dq1K+CUVOecy83NdRMnTnRxcXGuYsWKrlatWu722293zzzzjDtz5swl13X+tNjC/nTt2tU559y+fftcv379XLVq1VzVqlXdPffc4w4cOBBwWt75U1I/+eQTN2DAAFelShVXvXp1N2bMGPfdd98F3Hd6errr0KGDi4iIcBEREa5Zs2Zu9OjR7rPPPrN9ruSU1POnO17sz8KFC/0eQ0kuOTn5krd5/jG73Gm8zp07JXX06NGF/tuhQ4fc6NGjXf369V2FChVcdHS069q1q5s3b57ffp9//rnr06ePCw8Pd7Vq1XLjx493K1asuOwpqc6dO5Vz1qxZrlmzZq5ixYouMjLS9ezZ023cuNH22b59u+vUqZMLCwtzkvyeW8W9xsJc7mv05ZdfuoKCAjdjxgzXoEEDFxoa6n784x+7t99+O+BzPn9K6qxZs9yzzz7r6tev70JDQ13Hjh3tdOjv2717txs8eLCLjo52FSpUcPXq1XO9evVyaWlpts+VnpJ6qc8tPj7+svPXKp9zF7xHAFxlMjIy1KtXL23evFktW7Ys6+UA1zR+poCrXmZmppKTkwkCUAo4UgAAGI4UAACGKAAADFEAABiiAAAwRf7ltctdvREAcHUrynlFHCkAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGDKl/UC8MMUExPjeSYqKiqo+xoyZIjnmZCQEM8zI0aM8Dzj8/k8z+Tk5HiekaQZM2Z4nlm8eLHnmcOHD3uewbWDIwUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAIzPOeeKtGMQF/7CD8OsWbM8zwwdOtTzTPXq1T3P4Mq8+uqrnmdSU1OLfyG4KhTl5Z4jBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADBfEu8b069fP80xaWloJrKT4HDp0yPPMmjVrPM/8+9//9jwTjMcffzyouWAuKJibm+t5pn379p5ntm/f7nkGpY8L4gEAPCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAEz5sl4AilcwF3WbNm2a55lJkyZ5npkxY4bnGUmaM2eO55lgLqJXWrZu3RrU3IoVKzzPVKlSxfNMTEyM5xkuiHft4EgBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhqukXmMOHz7seSaYq6QGc+XSY8eOeZ6RpPz8/KDmrlZxcXFlvQTgojhSAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDA+Jxzrkg7+nwlvRbgunD69Omg5sqX9379yuzsbM8zt956q+eZYC7EiNJXlJd7jhQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADDer7AFwAwePNjzTDAXtgvW/PnzPc9wcbvrG0cKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYn3POFWlHn6+k1wKUqSpVqnie2bBhg+eZJk2aeJ4JVtu2bT3PbNy4sQRWgqtBUV7uOVIAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAKV/WCwCuFgMGDPA8U5pXPH366ac9z/zvf/8r/oXgmsaRAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhgvi4ZpUu3ZtzzN9+/YtgZUUnxUrVnieyc/PL4GV4FrGkQIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYL4uGqV6dOHc8z77zzjueZli1bep4JRkZGRlBzH330UTGvBAjEkQIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAMbnnHNF2tHnK+m1AIVavHix55l77723BFZSPMLCwoKaO3PmTDGvBNeborzcc6QAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIApX9YLwPWjVatWQc117969eBdSjJ5//nnPM3l5eSWwEqB4cKQAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAA43POuSLt6POV9FrwAxIWFuZ5ZuvWrUHdV0xMTFBzXmVkZHieSUxM9DyTn5/veQb/r0qVKp5ncnNzS2AlPzxFebnnSAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAFO+rBeAslepUiXPM4sWLfI8U1oXtpOkv/3tb55n7r777hJYyQ9PdHS055n4+HjPMykpKZ5nJOmNN97wPLNw4cKg7ut6xJECAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGC+JBd9xxh+eZ0rx4XG5urueZ9PT0ElhJ8ahcuXJQc8nJyZ5n+vbt63nmpz/9qeeZ6tWre545e/as5xlJGjVqVFBzKBqOFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMFwQ7xpzzz33eJ6ZMmVK8S+kEHl5eUHN9evXz/NMZmam55kbbrjB80xKSornmbFjx3qekaTY2Nig5krDN99843mmd+/eQd3X3r17g5pD0XCkAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCA4YJ4V6m+ffsGNbdgwQLPM+Hh4UHdl1ebN28Oau62227zPDNz5kzPM9HR0Z5nbrzxRs8zpSmYCwO+9dZbnmfefvttzzNc2O7qxJECAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFdJvUp17NgxqLnSuuJpMFq3bl2qc6Xh8OHDnmd27doV1H39/e9/9zwze/ZszzNnzpzxPINrB0cKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYLoh3lXr55ZeDmgsLC/M807ZtW88zt912m+eZYB04cMDzzPLlyz3P/Otf//I8s2bNGs8z+/bt8zwDlBaOFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMD7nnCvSjj5fSa8FAFCCivJyz5ECAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACmfFF3dM6V5DoAAFcBjhQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAOb/AMTCQ+j7DxLJAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZ6klEQVR4nO3ceXCU9R3H8c8SMCEBITEJp+VIIlejWFKwVAgYHEQOsaaZ1KLcWpsIMiNW6hEZ5WjVFkUrRQU5PIYSZBwHtSg46hhAxeEsEGKCgojhiHKHZH/9g8l3XDZAniWX8H7NZIYsz3f3l2XJe58nTx6fc84JAABJDep6AQCA+oMoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoXCIee+wx+Xw+7d+/v9ruc9SoUWrfvn213d/F4MMPP5TP59OHH35ot9W356myNda0oqIi+Xw+PfXUU9V2n3XxdVwKLsko+Hy+Kn3U9YutX79++uUvf1mna6hJkyZN0q9+9SvFxMQoMjJSXbp00WOPPaYjR46EfJ/9+vUL+DeMiYnRr3/9a82bN09+v78aV1/zpk+fruXLl9fZ47/yyivy+Xz6/PPP62wNNenNN9/UwIED1bp1a4WHh6tt27ZKT0/X5s2b63ppdaphXS+gLixatCjg84ULF2rlypVBt3fp0qU2l3XJ+eyzz9SnTx+NHj1aERER+vLLLzVz5ky9//77+uijj9SgQWjvWdq2basZM2ZIkoqLi7Vw4UKNHTtWO3bs0MyZM6vzS6iSF198MaQgTZ8+Xenp6Ro+fHj1LwratGmToqOjNXHiRMXGxuq7777TvHnz1LNnT+Xl5emaa66p6yXWiUsyCiNGjAj4fM2aNVq5cmXQ7Wc6duyYIiMja3Jpl5RPPvkk6LaEhATdf//9Wrduna677rqQ7rdZs2YB/5Z33323OnXqpOeee06PP/64GjVqFDTj9/tVWlqqiIiIkB7zXCp7PNS9Rx99NOi2cePGqW3btnrhhRc0Z86cOlhV3bskDx9VRcWhmy+++EJ9+/ZVZGSk/vrXv0o6ffjpscceC5pp3769Ro0aFXBbSUmJ7rvvPl155ZUKDw9XYmKi/va3v1XboYyNGzdq1KhR6tixoyIiItSyZUuNGTNGBw4cqHT7/fv3KyMjQ5dffrmuuOIKTZw4USdOnAjabvHixerRo4caN26smJgYZWZm6ptvvjnvevbu3att27bp1KlTIX09FcfeS0pKQpqvTGRkpK677jodPXpUxcXFkk7/G2ZnZ+vVV19Vt27dFB4ernfffVeStGfPHo0ZM0YtWrRQeHi4unXrpnnz5gXd7+7duzV8+HBFRUUpPj5ekyZN0smTJ4O2q+xnCn6/X88884ySk5MVERGhuLg43XTTTXaoxufz6ejRo1qwYIEdCvvpa6u61xiq0tJSPfroo+rRo4eaNWumqKgo9enTR6tXrz7rzD//+U+1a9dOjRs3VmpqaqWHa7Zt26b09HTFxMQoIiJCKSkpeuutt867nmPHjmnbtm0h/+wsPj5ekZGR1fr6+7m5JPcUqurAgQMaNGiQMjMzNWLECLVo0cLT/LFjx5Samqo9e/bo7rvv1i9+8Qt9+umnmjJlivbu3atZs2Zd8BpXrlypr776SqNHj1bLli21ZcsWzZ07V1u2bNGaNWvk8/kCts/IyFD79u01Y8YMrVmzRs8++6wOHTqkhQsX2jbTpk3TI488ooyMDI0bN07FxcWaPXu2+vbtqy+//FLNmzc/63qmTJmiBQsWqLCwsEo/XC0rK1NJSYlKS0u1efNmPfzww2ratKl69uwZ6lNSqa+++kphYWEBa1+1apWWLFmi7OxsxcbGqn379tq3b5+uu+46i0ZcXJzeeecdjR07Vj/++KPuu+8+SdLx48eVlpamr7/+WhMmTFDr1q21aNEirVq1qkrrGTt2rF555RUNGjRI48aNU1lZmT7++GOtWbNGKSkpWrRokcaNG6eePXvqrrvuknR6L0pSra2xKn788Ue99NJL+sMf/qDx48fr8OHDevnllzVw4ECtW7dO3bt3D9h+4cKFOnz4sLKysnTixAk988wzuuGGG7Rp0yb7/7Vlyxb99re/VZs2bfTggw8qKipKS5Ys0fDhw5Wbm6tbb731rOtZt26d+vfvr5ycnErfuFWmpKREp06d0nfffadZs2bpxx9/VFpaWqhPyc+fg8vKynJnPhWpqalOkpszZ07Q9pJcTk5O0O3t2rVzI0eOtM8ff/xxFxUV5Xbs2BGw3YMPPujCwsLc119/fc51paamum7dup1zm2PHjgXd9vrrrztJ7qOPPrLbcnJynCQ3bNiwgG3//Oc/O0luw4YNzjnnioqKXFhYmJs2bVrAdps2bXINGzYMuH3kyJGuXbt2AduNHDnSSXKFhYXnXHeFvLw8J8k+OnXq5FavXl2l2cqkpqa6zp07u+LiYldcXOz+97//uQkTJjhJbujQobadJNegQQO3ZcuWgPmxY8e6Vq1auf379wfcnpmZ6Zo1a2bP96xZs5wkt2TJEtvm6NGjLjEx0UkK+BrOfJ5WrVrlJLkJEyYErd/v99ufo6KiAl5PNbnGysyfP99Jcp999tlZtykrK3MnT54MuO3QoUOuRYsWbsyYMXZbYWGhk+QaN27sdu/ebbevXbvWSXKTJk2y29LS0lxycrI7ceKE3eb3+13v3r1dUlKS3bZ69eqgr6Pitsr+f55Np06d7PXXpEkT9/DDD7vy8vIqz19sOHx0DuHh4Ro9enTI8//5z3/Up08fRUdHa//+/fYxYMAAlZeX66OPPrrgNTZu3Nj+fOLECe3fv9+Oxa9fvz5o+6ysrIDP7733XknSihUrJEnLli2T3+9XRkZGwJpbtmyppKSkcx4WkE6fseKcq/IpmF27dtXKlSu1fPlyPfDAA4qKirqgs4+k04ce4uLiFBcXpy5dumj27NkaPHhw0OGV1NRUde3a1T53zik3N1dDhw6Vcy7g6x84cKB++OEHe05XrFihVq1aKT093eYjIyPtXf255ObmyufzKScnJ+jvztyzO1NtrbGqwsLCdNlll0k6fUjs4MGDKisrU0pKSqWvv+HDh6tNmzb2ec+ePdWrVy97/R08eFCrVq1SRkaGDh8+bF/bgQMHNHDgQOXn52vPnj1nXU+/fv3knKvyXoIkzZ8/X++++67+9a9/qUuXLjp+/LjKy8urPH+x4fDRObRp08Ze8KHIz8/Xxo0bFRcXV+nff//99yHfd4WDBw9q6tSpeuONN4Lu74cffgjaPikpKeDzhIQENWjQQEVFRbZm51zQdhWq+4eml19+uQYMGCBJuuWWW/Taa6/plltu0fr160M++6N9+/Z68cUX5fP5FBERoaSkJMXHxwdt16FDh4DPi4uLVVJSorlz52ru3LmV3nfFc7xr1y4lJiYGfRPv1KnTeddXUFCg1q1bKyYmpqpfUq2v0YsFCxbo6aefDvpZ0pnPrxT8+pOkq666SkuWLJEk7dy5U845PfLII3rkkUcqfbzvv/8+ICwX6je/+Y39OTMz0846rM7fqfg5IQrn8NN34VVx5rsLv9+vG2+8UQ888ECl21911VUhr61CRkaGPv30U02ePFndu3dXkyZN5Pf7ddNNN1Xph9lnfsPw+/3y+Xx65513FBYWFrR9kyZNLnjN5/K73/1Od9xxh954442QoxAVFWWhOZcz/30rnq8RI0Zo5MiRlc5cffXVIa2putS3NS5evFijRo3S8OHDNXnyZMXHxyssLEwzZsxQQUGB5/ur+Pruv/9+DRw4sNJtEhMTL2jN5xIdHa0bbrhBr776KlFA1UVHRwednVBaWqq9e/cG3JaQkKAjR45U6RtUKA4dOqQPPvhAU6dODTi9Lj8//6wz+fn5Ae/gdu7cKb/fb4d7EhIS5JxThw4dqiVaXp08eVJ+v7/SvZyaFhcXp6ZNm6q8vPy8/2bt2rXT5s2b5ZwLCOv27dvP+zgJCQl67733dPDgwXPuLVR2KKm21lhVS5cuVceOHbVs2bKAx6js0JhU+Wtzx44d9vrr2LGjpNN7pDX1/+Z8jh8/Xievv/qCnymEICEhIejnAXPnzg3aU8jIyFBeXp7ee++9oPsoKSlRWVnZBa2j4p28cy7g9nOd1fT8888HfD579mxJ0qBBgySdfqceFhamqVOnBt2vc+6sp7pWqOopqRVnfJzppZdekiSlpKScc74mhIWF6bbbblNubm6lp0lWnM4qSTfffLO+/fZbLV261G47duzYWQ/p/NRtt90m55ymTp0a9Hc/fc6joqKC3nzU1hqrqrLX4Nq1a5WXl1fp9suXLw/4mcC6deu0du1ae/3Fx8erX79++ve//x30JksK/Poq4+WU1MoO3xYVFemDDz6ok9dffcGeQgjGjRunP/3pT7rtttt04403asOGDXrvvfcUGxsbsN3kyZP11ltvaciQIRo1apR69Oiho0ePatOmTVq6dKmKioqCZs5UXFysJ554Iuj2Dh066I9//KP69u2rv//97zp16pTatGmj//73vyosLDzr/RUWFmrYsGG66aablJeXp8WLF+v222+3QzUJCQl64oknNGXKFBUVFWn48OFq2rSpCgsL9eabb+quu+7S/ffff9b7r+opqR9++KEmTJig9PR0JSUlqbS0VB9//LGWLVumlJSUoF8k9Pl8Sk1NrfFLj8ycOVOrV69Wr169NH78eHXt2lUHDx7U+vXr9f777+vgwYOSpPHjx+u5557TnXfeqS+++EKtWrXSokWLqvTLjf3799cdd9yhZ599Vvn5+Xao7+OPP1b//v2VnZ0tSerRo4fef/99/eMf/1Dr1q3VoUMH9erVq1bW+FPz5s2z3+H4qYkTJ2rIkCFatmyZbr31Vg0ePFiFhYWaM2eOunbtWukJA4mJibr++ut1zz336OTJk5o1a5auuOKKgEOszz//vK6//nolJydr/Pjx6tixo/bt26e8vDzt3r1bGzZsOOtavZySmpycrLS0NHXv3l3R0dHKz8/Xyy+/rFOnTtXJb77XG7V+vlM9dLZTUs92Omh5ebn7y1/+4mJjY11kZKQbOHCg27lzZ9Apqc45d/jwYTdlyhSXmJjoLrvsMhcbG+t69+7tnnrqKVdaWnrOdVWcFlvZR1pamnPOud27d7tbb73VNW/e3DVr1sz9/ve/d99++23QaXkVp6Ru3brVpaenu6ZNm7ro6GiXnZ3tjh8/HvTYubm57vrrr3dRUVEuKirKde7c2WVlZbnt27fbNhdySurOnTvdnXfe6Tp27OgaN27sIiIiXLdu3VxOTo47cuRI0HMoyWVmZp7zPiues/Odxuvc6VNSs7KyKv27ffv2uaysLHfllVe6Ro0auZYtW7q0tDQ3d+7cgO127drlhg0b5iIjI11sbKybOHGie/fdd897Sqpzp0/lfPLJJ13nzp3dZZdd5uLi4tygQYPcF198Ydts27bN9e3b1zVu3NhJCnhtVfcaK1NxSurZPr755hvn9/vd9OnTXbt27Vx4eLi79tpr3dtvvx30NVeckvrkk0+6p59+2l155ZUuPDzc9enTx06H/qmCggJ35513upYtW7pGjRq5Nm3auCFDhrilS5faNhd6SmpOTo5LSUlx0dHRrmHDhq5169YuMzPTbdy48byzFzOfc2ccIwDqmRUrVmjIkCHasGGDkpOT63o5wEWNnymg3lu9erUyMzMJAlAL2FMAABj2FAAAhigAAAxRAAAYogAAMFX+5bXzXb0RAFC/VeW8IvYUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAaVjXCwBqQoMG3t/vDB061PPMQw895HmmTZs2nmckafr06Z5nXnvtNc8zhw4d8jyDiwd7CgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGJ9zzlVpQ5+vptcCVJsRI0Z4nlm0aFENrKRuzZ8/3/PMmDFjamAlqA+q8u2ePQUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAwXxMNFaf369Z5nrr322hpYSd3avXu355kBAwZ4ntm+fbvnGdQ+LogHAPCEKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwDet6AUBNKCkpqbePU1ZWFtJjxcTEeJ5p27at55nbb7/d80xOTo7nGdRP7CkAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYogAAMEQBAGB8zjlXpQ19vppeC1BtGjTw/n5n2LBhnmc++OADzzMRERGeZyRp69atnmdiY2M9zxQUFHieSUxM9DyD2leVb/fsKQAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMA0rOsFADXB7/d7nlm+fLnnmcsvv9zzzIQJEzzPSKFd8TQUR48erZXHQf3EngIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYL4qHei46O9jyTkJDgeaZ3796eZ7Kzsz3PJCUleZ6pTdOmTavrJaAOsacAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIDhgnioNSkpKSHNvfDCC55nrrnmGs8zjRo18jxT37399tueZ5YuXVoDK8HPBXsKAABDFAAAhigAAAxRAAAYogAAMEQBAGCIAgDAEAUAgCEKAABDFAAAhigAAAxRAAAYLoiHWjNlypSQ5kK9kB6ksrIyzzN+v78GVoKfC/YUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYLhKKmpNfHx8XS/hktO9e3fPM+3bt/c8U1RU5HkG9RN7CgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGJ9zzlVpQ5+vpteCi9yQIUNCmrv55ps9z3Tu3NnzTIsWLTzPJCUleZ5p2DC061DW1v/BdevWeZ7p1atXDawE1a0q3+7ZUwAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwHBBPKCWZWdnhzQ3e/bsal5J5QoKCjzPJCYm1sBKUN24IB4AwBOiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMA0rOsFXAoaNPDe3rCwsJAe69SpUyHNofbk5+fX9RKAs2JPAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAwwXxasHVV1/teWbs2LEhPda9994b0hxqT1paWl0vATgr9hQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBguEqqR7179/Y8s2LFCs8zn3/+uecZSQoLC/M8U15eHtJj1Wc+n8/zzDXXXON55qGHHvI8M3jwYM8zten111+v6yWgDrGnAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCA4YJ4HoWHh3ueadasmeeZtLQ0zzOSFB8f73lm7969IT1WbWnevLnnmezsbM8zjz/+uOeZi1Fubm5dLwF1iD0FAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMF8S7yGzZssXzTEFBgeeZI0eOeJ5JTk72PCNJDRt6f5mGchHC+m737t2eZyZPnux5ZuPGjZ5ncPFgTwEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAMMF8S4y0dHRnmdSUlJqYCU4m08++SSkuXvuucfzzObNm0N6LFy62FMAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMD4nHOuShv6fDW9lp+FsLAwzzP9+/f3PLNy5UrPM7gwx44d8zyzZMkSzzPjxo3zPCNJ5eXlIc0BFary7Z49BQCAIQoAAEMUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABiukloLGjVq5Hnm+eefD+mx+vbt63mmU6dOIT1WbZk3b57nmaVLl3qe2bVrl+eZrVu3ep4B6gpXSQUAeEIUAACGKAAADFEAABiiAAAwRAEAYIgCAMAQBQCAIQoAAEMUAACGKAAADFEAABguiAcAlwguiAcA8IQoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAAQxQAAKZhVTd0ztXkOgAA9QB7CgAAQxQAAIYoAAAMUQAAGKIAADBEAQBgiAIAwBAFAIAhCgAA83+nDzQN41U4oAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Протестуйте навчену нейромережу на 10 зображеннях. З тестової вибірки візьміть 5\n",
        "# випадкових зображень і передайте їх у нейронну мережу.\n",
        "# Виведіть зображення та випишіть  поруч відповідь нейромережі.\n",
        "# Зробіть висновок про те, чи помиляється твоя нейронна мережа, і якщо так, то як часто?\n",
        "import random\n",
        "\n",
        "random.seed(123)\n",
        "indices = random.sample(range(len(x_test)), 10)\n",
        "\n",
        "for index in indices:\n",
        "    test_image = x_test[index]\n",
        "    test_label = y_test[index]\n",
        "\n",
        "    pred_probs = neural_net(tf.expand_dims(test_image, axis=0))\n",
        "    pred_label = tf.argmax(pred_probs, axis=1).numpy()[0]\n",
        "\n",
        "    plt.imshow(test_image.reshape(28, 28), cmap='gray')\n",
        "    plt.title(f\"True Label: {test_label}, Predicted Label: {pred_label}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Висновок**\n",
        "Точність тестування нейромережі становить приблизно 87.29%. Це означає, що мережа вірно класифікує близько 8729.74% тестових зображень, що є хорошим показником.\n",
        "\n",
        "У цьому тесті було використано 10 рукописних зображень цифр для перевірки роботи нейромережі. З цих 10 зображень усі 10 були правильно визначені. Це говорить про те, що в цьому конкретному тестовому наборі нейромережа показала високу ефективність і точність.\n",
        "\n",
        "Однак варто зазначити, що результати тестування не завжди повністю відображають роботу нейромережі в реальних умовах. Застосування нейромережі до нових, раніше не бачених даних може призвести до інших результатів, і точність може відрізнятися залежно від складності та різноманітності тестового набору.\n",
        "\n",
        "Матриця помилок дає змогу наочно побачити, які класи (цифри) було правильно визначено, а які було неправильно класифіковано. Основна діагональ матриці являє собою правильні передбачення, а значення поза діагоналлю вказують на помилки класифікації. Що темніший колір клітинки на тепловій карті, то менше помилок класифікації між відповідними класами. Що світліший колір клітинки, то більше помилок.\n",
        "\n",
        "Нейромережа найгірше визначає цифру 0, а найкраще цифру 2. Найчастіше плутає 0 з 5 та 0 з 6.\n",
        "\n",
        "Загалом нейромережа, здатна правильно визначати рукописні цифри може бути корисною в різних додатках, як-от системи оптичного розпізнавання символів (OCR), системи класифікації почерку та інші завдання, пов'язані з опрацюванням рукописних даних."
      ],
      "metadata": {
        "id": "OrczLhYXd9Qr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}